<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jan Kirenz</title>
    <link>https://kirenz.com/</link>
    <description>Recent content on Jan Kirenz</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Jan Kirenz, {year}</copyright>
    <lastBuildDate>Tue, 16 Feb 2021 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://kirenz.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Classification with Tidymodels, Workflows and Recipes</title>
      <link>https://kirenz.com/post/2021-02-17-r-classification-tidymodels/</link>
      <pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/post/2021-02-17-r-classification-tidymodels/</guid>
      <description>
&lt;script src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#business-understanding&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Business understanding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-understanding&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Data understanding&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#import-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Import Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clean-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Clean data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#format-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.3&lt;/span&gt; Format data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#missing-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.4&lt;/span&gt; Missing data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#create-new-variables&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.5&lt;/span&gt; Create new variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-overview&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.6&lt;/span&gt; Data overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-splitting&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.7&lt;/span&gt; Data splitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-exploration&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.8&lt;/span&gt; Data exploration&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#create-data-copy&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.8.1&lt;/span&gt; Create data copy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#geographical-overview&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.8.2&lt;/span&gt; Geographical overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#numerical-variables&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.8.3&lt;/span&gt; Numerical variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#categorical-variables&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.8.4&lt;/span&gt; Categorical variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-preparation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Data preparation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-preparation-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.1&lt;/span&gt; Data preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-prepropecessing-recipe&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.2&lt;/span&gt; Data prepropecessing recipe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#validation-set&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.3&lt;/span&gt; Validation set&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-building&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Model building&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#specify-models&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1&lt;/span&gt; Specify models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1.1&lt;/span&gt; Logistic regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-forest&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1.2&lt;/span&gt; Random forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boosted-tree-xgboost&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1.3&lt;/span&gt; Boosted tree (XGBoost)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-nearest-neighbor&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1.4&lt;/span&gt; K-nearest neighbor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-network&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1.5&lt;/span&gt; Neural network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#create-workflows&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2&lt;/span&gt; Create workflows&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2.1&lt;/span&gt; Logistic regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-forest-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2.2&lt;/span&gt; Random forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#xgboost&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2.3&lt;/span&gt; XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-nearest-neighbor-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2.4&lt;/span&gt; K-nearest neighbor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-network-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2.5&lt;/span&gt; Neural network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluate-models&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3&lt;/span&gt; Evaluate models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression-2&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3.1&lt;/span&gt; Logistic regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-forest-2&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3.2&lt;/span&gt; Random forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#xgboost-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3.3&lt;/span&gt; XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-nearest-neighbor-2&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3.4&lt;/span&gt; K-nearest neighbor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-network-2&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3.5&lt;/span&gt; Neural network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compare-models&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3.6&lt;/span&gt; Compare models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#last-evaluation-on-test-set&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.4&lt;/span&gt; Last evaluation on test set&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;The content of this tutorial is mainly based on the excellent books “Hands-on machine learning with scikit-learn, keras and tensorflow” from Aurélien Géron (2019) and “Tidy Modeling with R” from Max Kuhn and Julia Silge (2021)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial, we’ll build the following classification models using the tidymodels framework, which is a collection of R packages for modeling and machine learning using &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; principles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Logistic Regression&lt;/li&gt;
&lt;li&gt;Random Forest,&lt;/li&gt;
&lt;li&gt;XGBoost (extreme gradient boosted trees),&lt;/li&gt;
&lt;li&gt;K-nearest neighbor&lt;/li&gt;
&lt;li&gt;Neural network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that due to performance reasons, I only show the code for the neural net but don’t actually run it.&lt;/p&gt;
&lt;p&gt;Furthermore, we follow this data science lifecycle process:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:crisp1clas&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://data-science-tidymodels.netlify.app/css/CRISP-DM.png&#34; alt=&#34;Cross Industry Standard Process for Data Mining (Wirth &amp;amp; Hipp, 2000)&#34; width=&#34;70%&#34; height=&#34;70%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 0.1: Cross Industry Standard Process for Data Mining (Wirth &amp;amp; Hipp, 2000)
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;business-understanding&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Business understanding&lt;/h1&gt;
&lt;p&gt;In business understanding, you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Define your (business) goal&lt;/li&gt;
&lt;li&gt;Frame the problem (regression, classification,…)&lt;/li&gt;
&lt;li&gt;Choose a performance measure&lt;/li&gt;
&lt;li&gt;Show the data processing components&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First of all, we take a look at the big picture and define the objective of our data science project in business terms.&lt;/p&gt;
&lt;p&gt;In our example, the goal is to build a classification model to predict the type of median housing prices in districts in California. In particular, the model should learn from California census data and be able to predict wether the median house price in a district (population of 600 to 3000 people) is below or above a certain threshold, given some predictor variables. Hence, we face a &lt;strong&gt;supervised learning&lt;/strong&gt; situation and should use a &lt;strong&gt;classification model&lt;/strong&gt; to predict the categorical outcomes (below or above the preice). Furthermore, we use the &lt;strong&gt;F1-Score&lt;/strong&gt; as a performance measure for our classification problem.&lt;/p&gt;
&lt;p&gt;Note that in our classification example we again use the dataset from the previous regession tutorial. Therefore, we first need to create our categorical dependent variable from the numeric variable median house value. We will do this in the phase data understanding during the creation of new variables. Afterwards, we will remove the numeric variable median house value from our data.&lt;/p&gt;
&lt;p&gt;Let’s assume that the model’s output will be fed to another analytics system, along with other data. This downstream system will determine whether it is worth investing in a given area or not. The &lt;strong&gt;data processing components&lt;/strong&gt; (also called data pipeline) are shown in the figure below (you can use &lt;a href=&#34;https://docs.google.com/presentation/d/1vjm5YdmOH5LrubFhHf1vlqW2O9Z2UqdWA8biN3e8K5U/edit#slide=id.g19b41f69d7_2_265&#34;&gt;Google’s architectural templates&lt;/a&gt; to draw a data pipeline).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:datapipeline-class2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://data-science-tidymodels.netlify.app/css/data-pipeline.png&#34; alt=&#34;Data processing components&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Data processing components
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-understanding&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Data understanding&lt;/h1&gt;
&lt;p&gt;In Data Understanding, you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Import data&lt;/li&gt;
&lt;li&gt;Clean data&lt;/li&gt;
&lt;li&gt;Format data properly&lt;/li&gt;
&lt;li&gt;Create new variables&lt;/li&gt;
&lt;li&gt;Get an overview about the complete data&lt;/li&gt;
&lt;li&gt;Split data into training and test set using stratified sampling&lt;/li&gt;
&lt;li&gt;Discover and visualize the data to gain insights&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;import-data&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Import Data&lt;/h2&gt;
&lt;p&gt;First of all, let’s import the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

LINK &amp;lt;- &amp;quot;https://raw.githubusercontent.com/kirenz/datasets/master/housing_unclean.csv&amp;quot;
housing_df &amp;lt;- read_csv(LINK)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-data&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Clean data&lt;/h2&gt;
&lt;p&gt;To get a first impression of the data we take a look at the top 4 rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gt)

housing_df %&amp;gt;% 
  slice_head(n = 4) %&amp;gt;% 
  gt() # print output using gt&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#lhrgurynmx .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#lhrgurynmx .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#lhrgurynmx .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#lhrgurynmx .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#lhrgurynmx .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#lhrgurynmx .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#lhrgurynmx .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#lhrgurynmx .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#lhrgurynmx .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#lhrgurynmx .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#lhrgurynmx .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#lhrgurynmx .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#lhrgurynmx .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#lhrgurynmx .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#lhrgurynmx .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#lhrgurynmx .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#lhrgurynmx .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#lhrgurynmx .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#lhrgurynmx .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#lhrgurynmx .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#lhrgurynmx .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#lhrgurynmx .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#lhrgurynmx .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#lhrgurynmx .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#lhrgurynmx .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#lhrgurynmx .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#lhrgurynmx .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#lhrgurynmx .gt_left {
  text-align: left;
}

#lhrgurynmx .gt_center {
  text-align: center;
}

#lhrgurynmx .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#lhrgurynmx .gt_font_normal {
  font-weight: normal;
}

#lhrgurynmx .gt_font_bold {
  font-weight: bold;
}

#lhrgurynmx .gt_font_italic {
  font-style: italic;
}

#lhrgurynmx .gt_super {
  font-size: 65%;
}

#lhrgurynmx .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;lhrgurynmx&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;longitude&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;latitude&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;housing_median_age&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;total_rooms&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;total_bedrooms&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;population&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;households&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;median_income&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;median_house_value&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;ocean_proximity&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;-122.23&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;37.88&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;41.0years&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;880&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;129&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;322&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;126&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;8.3252&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;452600.0$&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;NEAR BAY&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;-122.22&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;37.86&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;21.0&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;7099&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;1106&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;2401&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;1138&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;8.3014&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;358500.0&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;NEAR BAY&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;-122.24&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;37.85&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;52.0&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;1467&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;190&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;496&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;177&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;7.2574&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;352100.0&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;NEAR BAY&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;-122.25&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;37.85&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;52.0&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;1274&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;235&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;558&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;219&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;5.6431&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;341300.0&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;NEAR BAY&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;Notice the values in the first row of the variables &lt;code&gt;housing_median_age&lt;/code&gt;and &lt;code&gt;median_house_value&lt;/code&gt;. We need to remove the strings “years” and “$”. Therefore, we use the function &lt;code&gt;str_remove_all&lt;/code&gt; from the &lt;code&gt;stringr&lt;/code&gt; package. Since there could be multiple wrong entries of the same type, we apply our corrections to all of the rows of the corresponding variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stringr)

housing_df &amp;lt;- 
  housing_df %&amp;gt;% 
  mutate(
    housing_median_age = str_remove_all(housing_median_age, &amp;quot;[years]&amp;quot;),
    median_house_value = str_remove_all(median_house_value, &amp;quot;[$]&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We don’t cover the phase of data cleaning in detail in this tutorial. However, in a real data science project, data cleaning is usually a very time consuming process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;format-data&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Format data&lt;/h2&gt;
&lt;p&gt;Next, we take a look at the data structure and check wether all data formats are correct:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Numeric variables should be formatted as integers (&lt;code&gt;int&lt;/code&gt;) or double precision floating point numbers (&lt;code&gt;dbl&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Categorical (nominal and ordinal) variables should usually be formatted as factors (&lt;code&gt;fct&lt;/code&gt;) and not characters (&lt;code&gt;chr&lt;/code&gt;). Especially, if they don’t have many levels.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(housing_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,640
## Columns: 10
## $ longitude          &amp;lt;dbl&amp;gt; -122.23, -122.22, -122.24, -122.25, -122.25, -122.2…
## $ latitude           &amp;lt;dbl&amp;gt; 37.88, 37.86, 37.85, 37.85, 37.85, 37.85, 37.84, 37…
## $ housing_median_age &amp;lt;chr&amp;gt; &amp;quot;41.0&amp;quot;, &amp;quot;21.0&amp;quot;, &amp;quot;52.0&amp;quot;, &amp;quot;52.0&amp;quot;, &amp;quot;52.0&amp;quot;, &amp;quot;52.0&amp;quot;, &amp;quot;52…
## $ total_rooms        &amp;lt;dbl&amp;gt; 880, 7099, 1467, 1274, 1627, 919, 2535, 3104, 2555,…
## $ total_bedrooms     &amp;lt;dbl&amp;gt; 129, 1106, 190, 235, 280, 213, 489, 687, 665, 707, …
## $ population         &amp;lt;dbl&amp;gt; 322, 2401, 496, 558, 565, 413, 1094, 1157, 1206, 15…
## $ households         &amp;lt;dbl&amp;gt; 126, 1138, 177, 219, 259, 193, 514, 647, 595, 714, …
## $ median_income      &amp;lt;dbl&amp;gt; 8.3252, 8.3014, 7.2574, 5.6431, 3.8462, 4.0368, 3.6…
## $ median_house_value &amp;lt;chr&amp;gt; &amp;quot;452600.0&amp;quot;, &amp;quot;358500.0&amp;quot;, &amp;quot;352100.0&amp;quot;, &amp;quot;341300.0&amp;quot;, &amp;quot;34…
## $ ocean_proximity    &amp;lt;chr&amp;gt; &amp;quot;NEAR BAY&amp;quot;, &amp;quot;NEAR BAY&amp;quot;, &amp;quot;NEAR BAY&amp;quot;, &amp;quot;NEAR BAY&amp;quot;, &amp;quot;NE…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The package &lt;code&gt;visdat&lt;/code&gt; helps us to explore the data class structure visually:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(visdat)

vis_dat(housing_df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can observe that the numeric variables &lt;code&gt;housing_media_age&lt;/code&gt; and &lt;code&gt;median_house_value&lt;/code&gt; are declared as characters (&lt;code&gt;chr&lt;/code&gt;) instead of numeric. We choose to format the variables as &lt;code&gt;dbl&lt;/code&gt;, since the values could be floating-point numbers.&lt;/p&gt;
&lt;p&gt;Furthermore, the categorical variable &lt;code&gt;ocean_proximity&lt;/code&gt; is formatted as character instead of factor. Let’s take a look at the levels of the variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_df %&amp;gt;% 
  count(ocean_proximity,
        sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   ocean_proximity     n
##   &amp;lt;chr&amp;gt;           &amp;lt;int&amp;gt;
## 1 &amp;lt;1H OCEAN        9136
## 2 INLAND           6551
## 3 NEAR OCEAN       2658
## 4 NEAR BAY         2290
## 5 ISLAND              5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variable has only 5 levels and therefore should be formatted as a factor.&lt;/p&gt;
&lt;p&gt;Note that it is usually a good idea to first take care of the numerical variables. Afterwards, we can easily convert all remaining character variables to factors using the function &lt;code&gt;across&lt;/code&gt; from the dplyr package (which is part of the tidyverse).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# convert to numeric
housing_df &amp;lt;- 
  housing_df %&amp;gt;% 
  mutate(
    housing_median_age = as.numeric(housing_median_age),
    median_house_value = as.numeric(median_house_value)
  )

# convert all remaining character variables to factors 
housing_df &amp;lt;- 
  housing_df %&amp;gt;% 
  mutate(across(where(is.character), as.factor))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;missing-data&#34; class=&#34;section level2&#34; number=&#34;2.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Missing data&lt;/h2&gt;
&lt;p&gt;Now let’s turn our attention to missing data. Missing data can be viewed with the function &lt;code&gt;vis_miss&lt;/code&gt; from the package &lt;code&gt;visdat&lt;/code&gt;. We arrange the data by columns with most missingness:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vis_miss(housing_df, sort_miss = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here an alternative method to obtain missing data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.na(housing_df) %&amp;gt;% colSums()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          longitude           latitude housing_median_age        total_rooms 
##                  0                  0                  0                  0 
##     total_bedrooms         population         households      median_income 
##                207                  0                  0                  0 
## median_house_value    ocean_proximity 
##                  0                  0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a missing rate of 0.1% (207 cases) in our variable &lt;code&gt;total_bedroms&lt;/code&gt;. This can cause problems for some algorithms. We will take care of this issue during our data preparation phase.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-new-variables&#34; class=&#34;section level2&#34; number=&#34;2.5&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.5&lt;/span&gt; Create new variables&lt;/h2&gt;
&lt;p&gt;One very important thing you may want to do at the beginning of your data science project is to create new variable combinations. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the &lt;em&gt;total number of rooms&lt;/em&gt; in a district is not very useful if you don’t know how many households there are. What you really want is the &lt;em&gt;number of rooms per household&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;And the &lt;em&gt;population per household&lt;/em&gt; also seems like an interesting attribute combination to look at.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s create these new attributes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_df &amp;lt;- 
  housing_df %&amp;gt;% 
  mutate(rooms_per_household = total_rooms/households,
        bedrooms_per_room = total_bedrooms/total_rooms,
        population_per_household = population/households)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Furthermore, in our example we need to create our dependent variable and drop the original numeric variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_df &amp;lt;- 
  housing_df %&amp;gt;% 
  mutate(price_category = case_when( 
    median_house_value &amp;lt; 150000 ~ &amp;quot;below&amp;quot;,
    median_house_value &amp;gt;= 150000 ~ &amp;quot;above&amp;quot;,
    )) %&amp;gt;% 
  mutate(price_category = as.factor(price_category)) %&amp;gt;% 
  select(-median_house_value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we created the new label &lt;code&gt;price_category&lt;/code&gt; from the variable &lt;code&gt;median_house_value&lt;/code&gt; it is crucial that we never use the variable &lt;code&gt;median_house_value&lt;/code&gt; as a predictor in our models. Therefore we drop it.&lt;/p&gt;
&lt;p&gt;Take a look at our dependent variable and create a table with the package &lt;a href=&#34;https://gt.rstudio.com/&#34;&gt;&lt;code&gt;gt&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gt)

housing_df %&amp;gt;% 
  count(price_category, # count observations
        name =&amp;quot;districts_total&amp;quot;) %&amp;gt;%  # name the new variable 
  mutate(percent = districts_total/sum(districts_total)) %&amp;gt;%  # calculate percentages
  gt() # create table&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#dopkjztufg .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#dopkjztufg .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#dopkjztufg .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#dopkjztufg .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#dopkjztufg .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#dopkjztufg .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#dopkjztufg .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#dopkjztufg .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#dopkjztufg .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#dopkjztufg .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#dopkjztufg .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#dopkjztufg .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#dopkjztufg .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#dopkjztufg .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#dopkjztufg .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#dopkjztufg .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#dopkjztufg .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#dopkjztufg .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#dopkjztufg .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#dopkjztufg .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#dopkjztufg .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#dopkjztufg .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#dopkjztufg .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#dopkjztufg .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#dopkjztufg .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#dopkjztufg .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#dopkjztufg .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#dopkjztufg .gt_left {
  text-align: left;
}

#dopkjztufg .gt_center {
  text-align: center;
}

#dopkjztufg .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#dopkjztufg .gt_font_normal {
  font-weight: normal;
}

#dopkjztufg .gt_font_bold {
  font-weight: bold;
}

#dopkjztufg .gt_font_italic {
  font-style: italic;
}

#dopkjztufg .gt_super {
  font-size: 65%;
}

#dopkjztufg .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;dopkjztufg&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;price_category&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;districts_total&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;percent&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;above&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;13084&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.6339147&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;below&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;7556&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.3660853&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;Let’s make a nice looking table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_df %&amp;gt;% 
  count(price_category, 
        name =&amp;quot;districts_total&amp;quot;) %&amp;gt;%
  mutate(percent = districts_total/sum(districts_total)*100,
         percent = round(percent, 2)) %&amp;gt;%
 gt() %&amp;gt;%
  tab_header(
    title = &amp;quot;California median house prices&amp;quot;,
    subtitle = &amp;quot;Districts above and below 150.000$&amp;quot;
  ) %&amp;gt;%
  cols_label(
    price_category = &amp;quot;Price&amp;quot;,
    districts_total = &amp;quot;Districts&amp;quot;,
    percent = &amp;quot;Percent&amp;quot;
  ) %&amp;gt;% 
  fmt_number(
    columns = vars(districts_total),
    suffixing = TRUE
  ) &lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#lzwlxuzllo .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#lzwlxuzllo .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#lzwlxuzllo .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#lzwlxuzllo .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#lzwlxuzllo .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#lzwlxuzllo .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#lzwlxuzllo .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#lzwlxuzllo .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#lzwlxuzllo .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#lzwlxuzllo .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#lzwlxuzllo .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#lzwlxuzllo .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#lzwlxuzllo .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#lzwlxuzllo .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#lzwlxuzllo .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#lzwlxuzllo .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#lzwlxuzllo .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#lzwlxuzllo .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#lzwlxuzllo .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#lzwlxuzllo .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#lzwlxuzllo .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#lzwlxuzllo .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#lzwlxuzllo .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#lzwlxuzllo .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#lzwlxuzllo .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#lzwlxuzllo .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#lzwlxuzllo .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#lzwlxuzllo .gt_left {
  text-align: left;
}

#lzwlxuzllo .gt_center {
  text-align: center;
}

#lzwlxuzllo .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#lzwlxuzllo .gt_font_normal {
  font-weight: normal;
}

#lzwlxuzllo .gt_font_bold {
  font-weight: bold;
}

#lzwlxuzllo .gt_font_italic {
  font-style: italic;
}

#lzwlxuzllo .gt_super {
  font-size: 65%;
}

#lzwlxuzllo .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;lzwlxuzllo&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  &lt;thead class=&#34;gt_header&#34;&gt;
    &lt;tr&gt;
      &lt;th colspan=&#34;3&#34; class=&#34;gt_heading gt_title gt_font_normal&#34; style&gt;California median house prices&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th colspan=&#34;3&#34; class=&#34;gt_heading gt_subtitle gt_font_normal gt_bottom_border&#34; style&gt;Districts above and below 150.000$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Price&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Districts&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Percent&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;above&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;13.08K&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;63.39&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;below&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;7.56K&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;36.61&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-overview&#34; class=&#34;section level2&#34; number=&#34;2.6&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.6&lt;/span&gt; Data overview&lt;/h2&gt;
&lt;p&gt;After we took care of our data issues, we can obtain a data summary of all numerical and categorical attributes using a function from the package &lt;code&gt;skimr&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;skim(housing_df)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-14&#34;&gt;Table 2.1: &lt;/span&gt;Data summary&lt;/caption&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;housing_df&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of rows&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20640&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of columns&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;_______________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Column type frequency:&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;factor&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;numeric&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;________________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Group variables&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;None&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: factor&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;ordered&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_unique&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;top_counts&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ocean_proximity&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;lt;1H: 9136, INL: 6551, NEA: 2658, NEA: 2290&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;price_category&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;abo: 13084, bel: 7556&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: numeric&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p25&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p50&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p75&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p100&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;hist&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;longitude&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-119.57&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-124.35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-121.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-118.49&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-118.01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-114.31&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▂▆▃▇▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;latitude&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.63&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32.54&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33.93&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.26&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37.71&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41.95&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▅▂▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;housing_median_age&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28.64&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.59&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;52.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▃▇▇▇▅&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;total_rooms&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2635.76&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2181.62&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1447.75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2127.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3148.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39320.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;total_bedrooms&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;207&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.99&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;537.87&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;421.39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;296.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;435.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;647.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6445.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;population&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1425.48&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1132.46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;787.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1166.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1725.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35682.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;households&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;499.54&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;382.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;280.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;409.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;605.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6082.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;median_income&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.87&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.90&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.53&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.74&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▇▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rooms_per_household&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.47&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.85&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.44&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;141.91&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bedrooms_per_room&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;207&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.99&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.06&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.24&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;population_per_household&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.82&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1243.33&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We have 20640 observations and 13 columns in our data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;sd&lt;/strong&gt; column shows the standard deviation, which measures how dispersed the values are.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;p0, p25, p50, p75 and p100&lt;/strong&gt; columns show the corresponding percentiles: a percentile indicates the value below which a given percentage of observations in a group of observations fall. For example, 25% of the districts have a &lt;code&gt;housing_median_age&lt;/code&gt; lower than 18, while 50% are lower than 29 and 75% are lower than 37. These are often called the 25th percentile (or first quartile), the median, and the 75th percentile.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Further note that the &lt;strong&gt;median income&lt;/strong&gt; attribute does not look like it is expressed in US dollars (USD). Actually the data has been scaled and capped at 15 (actually, 15.0001) for higher median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another quick way to get an overview of the type of data you are dealing with is to plot a &lt;strong&gt;histogram&lt;/strong&gt; for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis). You can either plot this one attribute at a time, or you can use &lt;code&gt;ggscatmat&lt;/code&gt; from the package &lt;code&gt;GGally&lt;/code&gt; on the whole dataset (as shown in the following code example), and it will plot a histogram for each numerical attribute as well as correlation coefficients (Pearson is the default). We just select the most promising variabels for our plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(GGally)

housing_df %&amp;gt;% 
  select(
    housing_median_age, 
    median_income, bedrooms_per_room, rooms_per_household, 
    population_per_household) %&amp;gt;% 
  ggscatmat(alpha = 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another option is to use &lt;code&gt;ggpairs&lt;/code&gt;, where we even can integrate categorical variables like our dependent variable &lt;code&gt;price_category&lt;/code&gt; and ocean proximity in the output:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(GGally)

housing_df %&amp;gt;% 
  select(
    housing_median_age, 
    median_income, bedrooms_per_room, rooms_per_household, 
    population_per_household, ocean_proximity,
    price_category) %&amp;gt;% 
  ggpairs()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are a few things you might notice in these histograms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The variables &lt;em&gt;median income&lt;/em&gt;, &lt;em&gt;housing median age&lt;/em&gt; were capped.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note that our attributes have very different scales. We will take care of this issue later in data preparation, when we use feature scaling (data normalization).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, many histograms are tail-heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will transform these attributes later on to have more bell-shaped distributions. For our right-skewed data (i.e., tail is on the right, also called positive skew), common transformations include square root and log (we will use the log).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-splitting&#34; class=&#34;section level2&#34; number=&#34;2.7&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.7&lt;/span&gt; Data splitting&lt;/h2&gt;
&lt;p&gt;Before we get started with our in-depth data exploration, let’s split our single dataset into two: a training set and a testing set. The training data will be used to fit models, and the testing set will be used to measure model performance. We perform data exploration only on the training data.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;training dataset&lt;/strong&gt; is a dataset of examples used during the learning process and is used to fit the models. A &lt;strong&gt;test dataset&lt;/strong&gt; is a dataset that is independent of the training dataset and is used to evaluate the performance of the final model. If a model fit to the training dataset also fits the test dataset well, minimal &lt;em&gt;overfitting&lt;/em&gt; has taken place. A better fitting of the training dataset as opposed to the test dataset usually points to overfitting.&lt;/p&gt;
&lt;p&gt;In our data split, we want to ensure that the training and test set is representative of the categories of our dependent variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_df %&amp;gt;% 
  ggplot(aes(price_category)) +
  geom_bar() &lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:hist-med-value-class2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/hist-med-value-class2-1.png&#34; alt=&#34;Histogram of Median Proces&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.1: Histogram of Median Proces
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In general, we would like to have instances for each &lt;em&gt;stratum&lt;/em&gt;, or else the estimate of a stratum’s importance may be biased. A &lt;em&gt;stratum&lt;/em&gt; (plural strata) refers to a subset (part) of the whole data from which is being sampled. We only have two categories in our data.&lt;/p&gt;
&lt;p&gt;To actually split the data, we can use the &lt;code&gt;rsample&lt;/code&gt; package (included in &lt;code&gt;tidymodels&lt;/code&gt;) to create an object that contains the information on how to split the data (which we call &lt;code&gt;data_split&lt;/code&gt;), and then two more &lt;code&gt;rsample&lt;/code&gt; functions to create data frames for the training and testing sets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible 
set.seed(123)

# Put 3/4 of the data into the training set 
data_split &amp;lt;- initial_split(housing_df, 
                           prop = 3/4, 
                           strata = price_category)

# Create dataframes for the two sets:
train_data &amp;lt;- training(data_split) 
test_data &amp;lt;- testing(data_split)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-exploration&#34; class=&#34;section level2&#34; number=&#34;2.8&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.8&lt;/span&gt; Data exploration&lt;/h2&gt;
&lt;p&gt;The point of data exploration is to gain insights that will help you select important variables for your model and to get ideas for feature engineering in the data preparation phase. Ususally, data exploration is an iterative process: once you get a prototype model up and running, you can analyze its output to gain more insights and come back to this exploration step. It is important to note that we perform data exploration only with our training data.&lt;/p&gt;
&lt;div id=&#34;create-data-copy&#34; class=&#34;section level3&#34; number=&#34;2.8.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.8.1&lt;/span&gt; Create data copy&lt;/h3&gt;
&lt;p&gt;We first make a copy of the training data since we don’t want to alter our data during data exploration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_explore &amp;lt;- train_data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we take a closer look at the relationships between our variables. In particular, we are interested in the relationships between ur &lt;em&gt;dependent&lt;/em&gt; variable &lt;code&gt;price_category&lt;/code&gt; and all other variables. The goal is to identify possible &lt;em&gt;predictor variables&lt;/em&gt; which we could use in our models to predict the &lt;code&gt;price_category&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;geographical-overview&#34; class=&#34;section level3&#34; number=&#34;2.8.2&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.8.2&lt;/span&gt; Geographical overview&lt;/h3&gt;
&lt;p&gt;Since our data includes information about &lt;code&gt;longitude&lt;/code&gt; and &lt;code&gt;latitude&lt;/code&gt;, we start our data exploration with the creation of a geographical scatterplot of the data to get some first insights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_explore %&amp;gt;% 
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(color = &amp;quot;cornflowerblue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:point-long-lat-class2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/point-long-lat-class2-1.png&#34; alt=&#34;Scatterplot of longitude and latitude&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.2: Scatterplot of longitude and latitude
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A better visualization that highlights high-density areas (with parameter &lt;code&gt;alpha = 0.1&lt;/code&gt; ):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_explore %&amp;gt;% 
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(color = &amp;quot;cornflowerblue&amp;quot;, alpha = 0.1) &lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:point-long-lat-a-class2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/point-long-lat-a-class2-1.png&#34; alt=&#34;Scatterplot of longitude and latitude that highlights high-density areas&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.3: Scatterplot of longitude and latitude that highlights high-density areas
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Overview about California housing prices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;red is expensive,&lt;/li&gt;
&lt;li&gt;purple is cheap and&lt;/li&gt;
&lt;li&gt;larger circles indicate areas with a larger population.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_explore %&amp;gt;% 
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(aes(size = population, color = price_category), 
             alpha = 0.4)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:plot-ca-prices-class2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/plot-ca-prices-class2-1.png&#34; alt=&#34;California housing_df prices&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.4: California housing_df prices
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Lastly, we add a map to our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmap)

qmplot(x = longitude, 
       y = latitude, 
       data = data_explore, 
       geom = &amp;quot;point&amp;quot;, 
       color = price_category, 
       size = population,
       alpha = 0.4) +
  scale_alpha(guide = &amp;#39;none&amp;#39;) # don&amp;#39;t show legend for alpha&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This image tells you that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density. Hence our &lt;code&gt;ocean_proximity&lt;/code&gt; variable may be a useful predictor of our categorical price variable median housing prices, although in Northern California the housing prices in coastal districts are not too high, so it is not a simple rule.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;numerical-variables&#34; class=&#34;section level3&#34; number=&#34;2.8.3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.8.3&lt;/span&gt; Numerical variables&lt;/h3&gt;
&lt;p&gt;We can use boxplots to check, if we actually find differences in our numeric variables for the different levels of our dependent &lt;em&gt;categorical variable&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_explore %&amp;gt;% 
  ggplot(aes(x = price_category, y = median_income, 
             fill = price_category, color = price_category)) +
  geom_boxplot(alpha=0.4) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let`s define a function for this task that accepts strings as inputs so we don’t have to copy and paste our code for every plot. Note that we only have to change the “y-variable” in every plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print_boxplot &amp;lt;- function(.y_var){
  
  # convert strings to variable
  y_var &amp;lt;- sym(.y_var) 
 
  # unquote variables using {{}}
  data_explore %&amp;gt;% 
  ggplot(aes(x = price_category, y = {{y_var}},
             fill = price_category, color = price_category)) +
  geom_boxplot(alpha=0.4) 
  
}  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obtain all of the names of the y-variables we want to use for our plots:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y_var &amp;lt;- 
  data_explore %&amp;gt;% 
  select(where(is.numeric), -longitude, - latitude) %&amp;gt;% 
  variable.names() # obtain name&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The map function applys the function &lt;code&gt;print_boxplot&lt;/code&gt; to each element of our atomic vector &lt;code&gt;y_var&lt;/code&gt; and returns the according plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)

map(y_var, print_boxplot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-23-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-23-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[4]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-23-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[5]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-23-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[6]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-23-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[7]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-23-7.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[8]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-23-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[9]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-23-9.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can observe a difference in the price_category:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The differences between our two groups are quite small for &lt;code&gt;housing_median_age&lt;/code&gt;, &lt;code&gt;total_room&lt;/code&gt;, &lt;code&gt;total_bedrooms&lt;/code&gt;, &lt;code&gt;population&lt;/code&gt; and &lt;code&gt;households&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can observe a noticeable difference for our variables &lt;code&gt;median_income&lt;/code&gt; and &lt;code&gt;bedrooms_per_room&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;population_per_household&lt;/code&gt; and &lt;code&gt;rooms_per_household&lt;/code&gt; include some extreme values We first need to fix this before we can proceed with our interpretations for this variabels.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again, let’s write a short function for this task and filter some of the extreme cases. We call the new function &lt;code&gt;print_boxplot_out&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print_boxplot_out &amp;lt;- function(.y_var_out){
  
  y_var &amp;lt;- sym(.y_var_out) 
 
  data_explore %&amp;gt;% 
  filter(rooms_per_household &amp;lt; 50, population_per_household &amp;lt; 20) %&amp;gt;% 
  ggplot(aes(x = price_category, y = {{y_var}},
             fill = price_category, color = price_category)) +
  geom_boxplot(alpha=0.4) 
  
} 

y_var_out &amp;lt;- 
  data_explore %&amp;gt;% 
  select(rooms_per_household, population_per_household) %&amp;gt;% 
  variable.names() 

map(y_var_out, print_boxplot_out)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-24-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we are able to recognize a small difference for &lt;code&gt;population_per_household&lt;/code&gt;. &lt;code&gt;rooms_per_household&lt;/code&gt; on the other hand is quite similar for both groups.&lt;/p&gt;
&lt;p&gt;Additionally, we can use the function &lt;code&gt;ggscatmat&lt;/code&gt; to create plots with our dependent variable as color column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(GGally)

data_explore %&amp;gt;% 
  select(price_category, median_income, bedrooms_per_room, rooms_per_household, 
         population_per_household) %&amp;gt;% 
  ggscatmat(color=&amp;quot;price_category&amp;quot;, 
            corMethod = &amp;quot;spearman&amp;quot;,
            alpha=0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are a few things you might notice in these histograms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Note that our attributes have very different scales. We will take care of this issue later in data preparation, when we use feature scaling (data normalization).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The histograms are tail-heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will transform these attributes later on to have more bell-shaped distributions. For our right-skewed data (i.e., tail is on the right, also called positive skew), common transformations include square root and log (we will use the log).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a result of our data exploration, we will include the numerical variables&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;median_income&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bedrooms_per_room&lt;/code&gt; and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;population_per_household&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;as predictors in our model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;categorical-variables&#34; class=&#34;section level3&#34; number=&#34;2.8.4&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.8.4&lt;/span&gt; Categorical variables&lt;/h3&gt;
&lt;p&gt;Now let’s analyze the relationship between our categorical variables &lt;code&gt;ocean proximity&lt;/code&gt; and &lt;code&gt;price_category&lt;/code&gt;. We start with a simple count.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gt)

data_explore %&amp;gt;% 
  count(price_category, ocean_proximity) %&amp;gt;% 
  group_by(price_category) %&amp;gt;% 
  mutate(percent = n / sum(n) *100,
         percent = round(percent, 2)) %&amp;gt;% 
  gt() %&amp;gt;% 
    tab_header(
    title = &amp;quot;California median house prices&amp;quot;,
    subtitle = &amp;quot;Districts above and below 150.000$&amp;quot;
  ) %&amp;gt;% 
  cols_label(
    ocean_proximity = &amp;quot;Ocean Proximity&amp;quot;,
    n = &amp;quot;Districts&amp;quot;,
    percent = &amp;quot;Percent&amp;quot;
  ) %&amp;gt;% 
  fmt_number(
    columns = vars(n),
    suffixing = TRUE
  ) &lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#xmjcfzubex .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#xmjcfzubex .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#xmjcfzubex .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#xmjcfzubex .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#xmjcfzubex .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#xmjcfzubex .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#xmjcfzubex .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#xmjcfzubex .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#xmjcfzubex .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#xmjcfzubex .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#xmjcfzubex .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#xmjcfzubex .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#xmjcfzubex .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#xmjcfzubex .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#xmjcfzubex .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#xmjcfzubex .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#xmjcfzubex .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#xmjcfzubex .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#xmjcfzubex .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#xmjcfzubex .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#xmjcfzubex .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#xmjcfzubex .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#xmjcfzubex .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#xmjcfzubex .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#xmjcfzubex .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#xmjcfzubex .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#xmjcfzubex .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#xmjcfzubex .gt_left {
  text-align: left;
}

#xmjcfzubex .gt_center {
  text-align: center;
}

#xmjcfzubex .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#xmjcfzubex .gt_font_normal {
  font-weight: normal;
}

#xmjcfzubex .gt_font_bold {
  font-weight: bold;
}

#xmjcfzubex .gt_font_italic {
  font-style: italic;
}

#xmjcfzubex .gt_super {
  font-size: 65%;
}

#xmjcfzubex .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;xmjcfzubex&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  &lt;thead class=&#34;gt_header&#34;&gt;
    &lt;tr&gt;
      &lt;th colspan=&#34;3&#34; class=&#34;gt_heading gt_title gt_font_normal&#34; style&gt;California median house prices&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th colspan=&#34;3&#34; class=&#34;gt_heading gt_subtitle gt_font_normal gt_bottom_border&#34; style&gt;Districts above and below 150.000$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Ocean Proximity&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_center&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Districts&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Percent&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr class=&#34;gt_group_heading_row&#34;&gt;
      &lt;td colspan=&#34;3&#34; class=&#34;gt_group_heading&#34;&gt;above&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;&amp;lt;1H OCEAN&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;5.69K&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;57.96&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;INLAND&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;1.19K&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;12.15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;ISLAND&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;4.00&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;0.04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;NEAR BAY&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;1.42K&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;14.42&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;NEAR OCEAN&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;1.51K&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;15.43&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr class=&#34;gt_group_heading_row&#34;&gt;
      &lt;td colspan=&#34;3&#34; class=&#34;gt_group_heading&#34;&gt;below&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;&amp;lt;1H OCEAN&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;1.17K&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;20.72&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;INLAND&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;3.66K&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;64.53&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;NEAR BAY&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;332.00&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;5.86&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;NEAR OCEAN&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_center&#34;&gt;504.00&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_right&#34;&gt;8.89&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;geom_bin2d()&lt;/code&gt; creats a heatmap by counting the number of cases in each group, and then mapping the number of cases to each subgroub’s fill.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_explore %&amp;gt;%
  ggplot(aes(price_category, ocean_proximity)) +
  geom_bin2d() +
  scale_fill_continuous(type = &amp;quot;viridis&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can observe that most districts with a median house price above 150,000 have an ocean proximity below 1 hour. On the other hand, districts below that threshold are typically inland. Hence, ocean proximity is indeed a good predictor for our two different median house value categories.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-preparation&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Data preparation&lt;/h1&gt;
&lt;p&gt;Data preparation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Handle missing values&lt;/li&gt;
&lt;li&gt;Fix or remove outliers&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Feature selection&lt;/li&gt;
&lt;li&gt;Feature engineering&lt;/li&gt;
&lt;li&gt;Feature scaling&lt;/li&gt;
&lt;li&gt;Create a validation set&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next, we’ll preprocess our data before training the models. We mainly use the tidymodels packages &lt;code&gt;recipes&lt;/code&gt; and &lt;code&gt;workflows&lt;/code&gt; for this steps. &lt;code&gt;Recipes&lt;/code&gt; are built as a series of optional data preparation steps, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Data cleaning&lt;/em&gt;: Fix or remove outliers, fill in missing values (e.g., with zero, mean, median…) or drop their rows (or columns).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Feature selection&lt;/em&gt;: Drop the attributes that provide no useful information for the task.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Feature engineering&lt;/em&gt;: Discretize continuous features, decompose features (e.g., the weekday from a date variable, etc.), add promising transformations of features (e.g., log(x), sqrt(x), x2 , etc.) or aggregate features into promising new features (like we already did).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Feature scaling&lt;/em&gt;: Standardize or normalize features.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will want to use our recipe across several steps as we train and test our models. To simplify this process, we can use a &lt;em&gt;model workflow&lt;/em&gt;, which pairs a model and recipe together.&lt;/p&gt;
&lt;div id=&#34;data-preparation-1&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Data preparation&lt;/h2&gt;
&lt;p&gt;Before we create our &lt;code&gt;recipes&lt;/code&gt;, we first select the variables which we will use in the model. Note that we keep &lt;code&gt;longitude&lt;/code&gt; and &lt;code&gt;latitude&lt;/code&gt; to be able to map the data in a later stage but we will not use the variables in our model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_df_new &amp;lt;-
  housing_df %&amp;gt;% 
  select( # select our predictors
    longitude, latitude, 
    price_category, 
    median_income, 
    ocean_proximity, 
    bedrooms_per_room, 
    rooms_per_household, 
    population_per_household
         )

glimpse(housing_df_new)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,640
## Columns: 8
## $ longitude                &amp;lt;dbl&amp;gt; -122.23, -122.22, -122.24, -122.25, -122.25, …
## $ latitude                 &amp;lt;dbl&amp;gt; 37.88, 37.86, 37.85, 37.85, 37.85, 37.85, 37.…
## $ price_category           &amp;lt;fct&amp;gt; above, above, above, above, above, above, abo…
## $ median_income            &amp;lt;dbl&amp;gt; 8.3252, 8.3014, 7.2574, 5.6431, 3.8462, 4.036…
## $ ocean_proximity          &amp;lt;fct&amp;gt; NEAR BAY, NEAR BAY, NEAR BAY, NEAR BAY, NEAR …
## $ bedrooms_per_room        &amp;lt;dbl&amp;gt; 0.1465909, 0.1557966, 0.1295160, 0.1844584, 0…
## $ rooms_per_household      &amp;lt;dbl&amp;gt; 6.984127, 6.238137, 8.288136, 5.817352, 6.281…
## $ population_per_household &amp;lt;dbl&amp;gt; 2.555556, 2.109842, 2.802260, 2.547945, 2.181…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Furthermore, we need to make a new data split since we updated the original data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)

data_split &amp;lt;- initial_split(housing_df_new, # updated data
                           prop = 3/4, 
                           strata = price_category)

train_data &amp;lt;- training(data_split) 
test_data &amp;lt;- testing(data_split)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-prepropecessing-recipe&#34; class=&#34;section level2&#34; number=&#34;3.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Data prepropecessing recipe&lt;/h2&gt;
&lt;p&gt;The type of data preprocessing is dependent on the data and the type of model being fit. The excellent book “Tidy Modeling with R” provides an &lt;a href=&#34;https://www.tmwr.org/pre-proc-table.html&#34;&gt;appendix with recommendations for baseline levels of preprocessing&lt;/a&gt; that are needed for various model functions.&lt;/p&gt;
&lt;p&gt;Let’s create a base &lt;code&gt;recipe&lt;/code&gt; for all of our classification models. Note that the sequence of steps matter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;recipe()&lt;/code&gt; function has two arguments:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A formula. Any variable on the left-hand side of the tilde (&lt;code&gt;~&lt;/code&gt;) is considered the model outcome (here, &lt;code&gt;price_category&lt;/code&gt;). On the right-hand side of the tilde are the predictors. Variables may be listed by name (separated by a &lt;code&gt;+&lt;/code&gt;), or you can use the dot (&lt;code&gt;.&lt;/code&gt;) to indicate all other variables as predictors.&lt;/li&gt;
&lt;li&gt;The data. A recipe is associated with the data set used to create the model. This will typically be the training set, so &lt;code&gt;data = train_data&lt;/code&gt; here.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;update_role()&lt;/code&gt;: This step of adding roles to a recipe is optional; the purpose of using it here is that those two variables can be retained in the data but not included in the model. This can be convenient when, after the model is fit, we want to investigate some poorly predicted value. These ID columns will be available and can be used to try to understand what went wrong.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;step_naomit()&lt;/code&gt; removes observations (rows of data) if they contain NA or NaN values. We use &lt;code&gt;skip = TRUE&lt;/code&gt; because we don’t want to perform this part to new data so that the number of samples in the assessment set is the same as the number of predicted values (even if they are NA).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that instead of deleting missing values we could also easily substitute (i.e., &lt;em&gt;impute&lt;/em&gt;) missing values of variables by one of the following methods (using the training set):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://recipes.tidymodels.org/reference/step_medianimpute.html&#34;&gt;median&lt;/a&gt;,&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://recipes.tidymodels.org/reference/step_meanimpute.html&#34;&gt;mean&lt;/a&gt;,&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://recipes.tidymodels.org/reference/step_modeimpute.html&#34;&gt;mode&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://recipes.tidymodels.org/reference/step_knnimpute.html&#34;&gt;k-nearest neighbors&lt;/a&gt;,&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://recipes.tidymodels.org/reference/step_impute_linear.html&#34;&gt;linear model&lt;/a&gt;,&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://recipes.tidymodels.org/reference/step_bagimpute.html&#34;&gt;bagged tree models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Take a look at the &lt;a href=&#34;https://recipes.tidymodels.org/reference/index.html&#34;&gt;recipes reference&lt;/a&gt; for an overview about all possible imputation methods.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;step_novel()&lt;/code&gt; converts all nominal variables to factors and takes care of other issues related to categorical variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;step_log()&lt;/code&gt; will log transform data (since some of our numerical variables are right-skewed). Note that this step can not be performed on negative numbers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;step_normalize()&lt;/code&gt; normalizes (center and scales) the numeric variables to have a standard deviation of one and a mean of zero. (i.e., z-standardization).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;step_dummy()&lt;/code&gt; converts our factor column &lt;code&gt;ocean_proximity&lt;/code&gt; into numeric binary (0 and 1) variables.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that this step may cause problems if your categorical variable has too many levels - especially if some of the levels are very infrequent. In this case you should either drop the variable or pool infrequently occurring values into an “other” category with &lt;a href=&#34;https://recipes.tidymodels.org/reference/step_other.html&#34;&gt;&lt;code&gt;step_other&lt;/code&gt;&lt;/a&gt;. This steps has to be performed befor &lt;code&gt;step_dummy&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;step_zv()&lt;/code&gt;: removes any numeric variables that have zero variance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;step_corr()&lt;/code&gt;: will remove predictor variables that have large correlations with other predictor variables.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that the package &lt;a href=&#34;https://themis.tidymodels.org/index.html&#34;&gt;&lt;code&gt;themis&lt;/code&gt;&lt;/a&gt; contains extra steps for the &lt;code&gt;recipes&lt;/code&gt; package for dealing with &lt;strong&gt;imbalanced data&lt;/strong&gt;. A classification data set with skewed class proportions is called imbalanced. Classes that make up a large proportion of the data set are called majority classes. Those that make up a smaller proportion are minority classes (see &lt;a href=&#34;https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data&#34;&gt;Google Developers&lt;/a&gt; for more details). &lt;code&gt;Themis&lt;/code&gt; provides various methods for over-sampling (e.g. SMOTE) and under-sampling. However, we don’t have to use this methods since our data is not imbalanced.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_rec &amp;lt;-
  recipe(price_category ~ .,
         data = train_data) %&amp;gt;%
  update_role(longitude, latitude, 
              new_role = &amp;quot;ID&amp;quot;) %&amp;gt;% 
  step_log(
    median_income,
    bedrooms_per_room, rooms_per_household, 
    population_per_household
    ) %&amp;gt;% 
  step_naomit(everything(), skip = TRUE) %&amp;gt;% 
  step_novel(all_nominal(), -all_outcomes()) %&amp;gt;%
  step_normalize(all_numeric(), -all_outcomes(), 
                 -longitude, -latitude) %&amp;gt;% 
  step_dummy(all_nominal(), -all_outcomes()) %&amp;gt;%
  step_zv(all_numeric(), -all_outcomes()) %&amp;gt;%
  step_corr(all_predictors(), threshold = 0.7, method = &amp;quot;spearman&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To view the current set of variables and roles, use the &lt;code&gt;summary()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(housing_rec)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 4
##   variable                 type    role      source  
##   &amp;lt;chr&amp;gt;                    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;   
## 1 longitude                numeric ID        original
## 2 latitude                 numeric ID        original
## 3 median_income            numeric predictor original
## 4 ocean_proximity          nominal predictor original
## 5 bedrooms_per_room        numeric predictor original
## 6 rooms_per_household      numeric predictor original
## 7 population_per_household numeric predictor original
## 8 price_category           nominal outcome   original&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we would like to check if all of our preprocessing steps from above actually worked, we can proceed as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prepped_data &amp;lt;- 
  housing_rec %&amp;gt;% # use the recipe object
  prep() %&amp;gt;% # perform the recipe on training data
  juice() # extract only the preprocessed dataframe &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take a look at the data structure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(prepped_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 15,317
## Columns: 10
## $ longitude                  &amp;lt;dbl&amp;gt; -122.22, -122.24, -122.25, -122.25, -122.25…
## $ latitude                   &amp;lt;dbl&amp;gt; 37.86, 37.85, 37.85, 37.85, 37.84, 37.84, 3…
## $ median_income              &amp;lt;dbl&amp;gt; 1.8483941, 1.5639024, 1.0313625, 0.3223031,…
## $ rooms_per_household        &amp;lt;dbl&amp;gt; 0.676801762, 1.719249869, 0.420589933, -0.3…
## $ population_per_household   &amp;lt;dbl&amp;gt; -1.07327656, -0.04202316, -0.38771664, -1.0…
## $ price_category             &amp;lt;fct&amp;gt; above, above, above, above, above, above, a…
## $ ocean_proximity_INLAND     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ ocean_proximity_ISLAND     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ ocean_proximity_NEAR.BAY   &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ ocean_proximity_NEAR.OCEAN &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualize the numerical data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prepped_data %&amp;gt;% 
  select(price_category, 
         median_income, 
         rooms_per_household, 
         population_per_household) %&amp;gt;% 
  ggscatmat(corMethod = &amp;quot;spearman&amp;quot;,
            alpha=0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You should notice that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the variables &lt;code&gt;longitude&lt;/code&gt; and &lt;code&gt;latitude&lt;/code&gt; did not change.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;median_income&lt;/code&gt;, &lt;code&gt;rooms_per_household&lt;/code&gt; and &lt;code&gt;population_per_household&lt;/code&gt; are now z-standardized and the distributions are a bit less right skewed (due to our log transformation)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;ocean_proximity&lt;/code&gt; was replaced by dummy variables.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;validation-set&#34; class=&#34;section level2&#34; number=&#34;3.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3&lt;/span&gt; Validation set&lt;/h2&gt;
&lt;p&gt;Remember that we already partitioned our data set into a &lt;em&gt;training set&lt;/em&gt; and &lt;em&gt;test set&lt;/em&gt;. This lets us judge whether a given model will generalize well to new data. However, using only two partitions may be insufficient when doing many rounds of hyperparameter tuning (which we don’t perform in this tutorial but it is always recommended to use a validation set).&lt;/p&gt;
&lt;p&gt;Therefore, it is usually a good idea to create a so called &lt;code&gt;validation set&lt;/code&gt;. Watch this short &lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/validation/video-lecture&#34;&gt;video from Google’s Machine Learning crash course&lt;/a&gt; to learn more about the value of a validation set.&lt;/p&gt;
&lt;p&gt;We use k-fold crossvalidation to build a set of 5 validation folds with the function &lt;code&gt;vfold_cv&lt;/code&gt;. We also use stratified sampling:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)

cv_folds &amp;lt;-
 vfold_cv(train_data, 
          v = 5, 
          strata = price_category) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will come back to the &lt;em&gt;validation set&lt;/em&gt; after we specified our models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-building&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Model building&lt;/h1&gt;
&lt;div id=&#34;specify-models&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Specify models&lt;/h2&gt;
&lt;p&gt;The process of specifying our models is always as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Pick a &lt;code&gt;model type&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;set the &lt;code&gt;engine&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Set the &lt;code&gt;mode&lt;/code&gt;: regression or classification&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can choose the &lt;code&gt;model type&lt;/code&gt; and &lt;code&gt;engine&lt;/code&gt; from this &lt;a href=&#34;https://www.tidymodels.org/find/parsnip/&#34;&gt;list&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level3&#34; number=&#34;4.1.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1.1&lt;/span&gt; Logistic regression&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_spec &amp;lt;- # your model specification
  logistic_reg() %&amp;gt;%  # model type
  set_engine(engine = &amp;quot;glm&amp;quot;) %&amp;gt;%  # model engine
  set_mode(&amp;quot;classification&amp;quot;) # model mode

# Show your model specification
log_spec&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Logistic Regression Model Specification (classification)
## 
## Computational engine: glm&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level3&#34; number=&#34;4.1.2&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1.2&lt;/span&gt; Random forest&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ranger)

rf_spec &amp;lt;- 
  rand_forest() %&amp;gt;% 
  set_engine(&amp;quot;ranger&amp;quot;, importance = &amp;quot;impurity&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;classification&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we set the engine, we add &lt;code&gt;importance = &#34;impurity&#34;&lt;/code&gt;. This will provide variable importance scores for this model, which gives some insight into which predictors drive model performance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;boosted-tree-xgboost&#34; class=&#34;section level3&#34; number=&#34;4.1.3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1.3&lt;/span&gt; Boosted tree (XGBoost)&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(xgboost)

xgb_spec &amp;lt;- 
  boost_tree() %&amp;gt;% 
  set_engine(&amp;quot;xgboost&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;classification&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;k-nearest-neighbor&#34; class=&#34;section level3&#34; number=&#34;4.1.4&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1.4&lt;/span&gt; K-nearest neighbor&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knn_spec &amp;lt;- 
  nearest_neighbor(neighbors = 4) %&amp;gt;% # we can adjust the number of neighbors 
  set_engine(&amp;quot;kknn&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;classification&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-network&#34; class=&#34;section level3&#34; number=&#34;4.1.5&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1.5&lt;/span&gt; Neural network&lt;/h3&gt;
&lt;p&gt;To use the neural network model, you will need to install the following packages: &lt;code&gt;keras&lt;/code&gt;. You will also need the python keras library installed (see &lt;code&gt;?keras::install_keras()&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(keras)

nnet_spec &amp;lt;-
  mlp() %&amp;gt;%
  set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;% 
  set_engine(&amp;quot;keras&amp;quot;, verbose = 0) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We set the engine-specific &lt;code&gt;verbose&lt;/code&gt; argument to prevent logging the results.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;create-workflows&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Create workflows&lt;/h2&gt;
&lt;p&gt;To combine the data preparation recipe with the model building, we use the package &lt;a href=&#34;https://workflows.tidymodels.org&#34;&gt;workflows&lt;/a&gt;. A workflow is an object that can bundle together your pre-processing recipe, modeling, and even post-processing requests (like calculating the RMSE).&lt;/p&gt;
&lt;div id=&#34;logistic-regression-1&#34; class=&#34;section level3&#34; number=&#34;4.2.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2.1&lt;/span&gt; Logistic regression&lt;/h3&gt;
&lt;p&gt;Bundle recipe and model with &lt;code&gt;workflows&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_wflow &amp;lt;- # new workflow object
 workflow() %&amp;gt;% # use workflow function
 add_recipe(housing_rec) %&amp;gt;%   # use the new recipe
 add_model(log_spec)   # add your model spec

# show object
log_wflow&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ══ Workflow ════════════════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: logistic_reg()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 7 Recipe Steps
## 
## • step_log()
## • step_naomit()
## • step_novel()
## • step_normalize()
## • step_dummy()
## • step_zv()
## • step_corr()
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## Logistic Regression Model Specification (classification)
## 
## Computational engine: glm&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest-1&#34; class=&#34;section level3&#34; number=&#34;4.2.2&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2.2&lt;/span&gt; Random forest&lt;/h3&gt;
&lt;p&gt;Bundle recipe and model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_wflow &amp;lt;-
 workflow() %&amp;gt;%
 add_recipe(housing_rec) %&amp;gt;% 
 add_model(rf_spec) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;xgboost&#34; class=&#34;section level3&#34; number=&#34;4.2.3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2.3&lt;/span&gt; XGBoost&lt;/h3&gt;
&lt;p&gt;Bundle recipe and model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xgb_wflow &amp;lt;-
 workflow() %&amp;gt;%
 add_recipe(housing_rec) %&amp;gt;% 
 add_model(xgb_spec)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;k-nearest-neighbor-1&#34; class=&#34;section level3&#34; number=&#34;4.2.4&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2.4&lt;/span&gt; K-nearest neighbor&lt;/h3&gt;
&lt;p&gt;Bundle recipe and model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knn_wflow &amp;lt;-
 workflow() %&amp;gt;%
 add_recipe(housing_rec) %&amp;gt;% 
 add_model(knn_spec)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-network-1&#34; class=&#34;section level3&#34; number=&#34;4.2.5&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2.5&lt;/span&gt; Neural network&lt;/h3&gt;
&lt;p&gt;Bundle recipe and model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nnet_wflow &amp;lt;-
 workflow() %&amp;gt;%
 add_recipe(housing_rec) %&amp;gt;% 
 add_model(nnet_spec)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-models&#34; class=&#34;section level2&#34; number=&#34;4.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; Evaluate models&lt;/h2&gt;
&lt;p&gt;Now we can use our validation set (&lt;code&gt;cv_folds&lt;/code&gt;) to estimate the performance of our models using the &lt;code&gt;fit_resamples()&lt;/code&gt; function to fit the models on each of the folds and store the results.&lt;/p&gt;
&lt;p&gt;Note that &lt;code&gt;fit_resamples()&lt;/code&gt; will fit our model to each resample and evaluate on the heldout set from each resample. The function is usually only used for computing performance metrics across some set of resamples to evaluate our models (like accuracy) - the models are not even stored. However, in our example we save the predictions in order to visualize the model fit and residuals with &lt;code&gt;control_resamples(save_pred = TRUE)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we collect the performance metrics with &lt;code&gt;collect_metrics()&lt;/code&gt; and pick the model that does best on the validation set.&lt;/p&gt;
&lt;div id=&#34;logistic-regression-2&#34; class=&#34;section level3&#34; number=&#34;4.3.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.1&lt;/span&gt; Logistic regression&lt;/h3&gt;
&lt;p&gt;We use our workflow object to perform resampling. Furthermore, we use
&lt;code&gt;metric_set()&lt;/code&gt;to choose some common classification performance metrics provided by the &lt;code&gt;yardstick&lt;/code&gt; package. Visit &lt;a href=&#34;https://yardstick.tidymodels.org/reference/index.html&#34;&gt;&lt;code&gt;yardsticks&lt;/code&gt; reference&lt;/a&gt; to see the complete list of all possible metrics.&lt;/p&gt;
&lt;p&gt;Note that Cohen’s &lt;em&gt;kappa&lt;/em&gt; coefficient (κ) is a similar measure to accuracy, but is normalized by the accuracy that would be expected by chance alone and is very useful when one or more classes have large frequency distributions. The higher the value, the better.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_res &amp;lt;- 
  log_wflow %&amp;gt;% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(
      save_pred = TRUE)
    ) &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;model-coefficients&#34; class=&#34;section level4&#34; number=&#34;4.3.1.1&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.1.1&lt;/span&gt; Model coefficients&lt;/h4&gt;
&lt;p&gt;The above described method to obtain &lt;code&gt;log_res&lt;/code&gt; is fine if we are not interested in model coefficients. However, if we would like to extract the model coeffcients from &lt;code&gt;fit_resamples&lt;/code&gt;, we need to proceed as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# save model coefficients for a fitted model object from a workflow

get_model &amp;lt;- function(x) {
  pull_workflow_fit(x) %&amp;gt;% tidy()
}

# same as before with one exception
log_res_2 &amp;lt;- 
  log_wflow %&amp;gt;% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(
      save_pred = TRUE,
      extract = get_model) # use extract and our new function
    ) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now there is a &lt;code&gt;.extracts&lt;/code&gt; column with nested tibbles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_res_2$.extracts[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   .extracts            .config             
##   &amp;lt;list&amp;gt;               &amp;lt;chr&amp;gt;               
## 1 &amp;lt;tibble[,5] [8 × 5]&amp;gt; Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get the results use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_res_2$.extracts[[1]][[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## # A tibble: 8 x 5
##   term                       estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;                         &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)                  -2.02     0.0510  -39.6    0       
## 2 median_income                -1.91     0.0470  -40.6    0       
## 3 rooms_per_household           0.252    0.0376    6.70   2.14e-11
## 4 population_per_household      0.485    0.0297   16.3    7.36e-60
## 5 ocean_proximity_INLAND        2.92     0.0729   40.0    0       
## 6 ocean_proximity_ISLAND      -11.4    160.       -0.0714 9.43e- 1
## 7 ocean_proximity_NEAR.BAY      0.386    0.0991    3.90   9.60e- 5
## 8 ocean_proximity_NEAR.OCEAN    0.631    0.0871    7.24   4.34e-13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of the results can be flattened and collected using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_coef &amp;lt;- map_dfr(log_res_2$.extracts, ~ .x[[1]][[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Show all of the resample coefficients for a single predictor:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(all_coef, term == &amp;quot;median_income&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 5
##   term          estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 median_income    -1.91    0.0470     -40.6       0
## 2 median_income    -1.91    0.0469     -40.6       0
## 3 median_income    -1.89    0.0469     -40.4       0
## 4 median_income    -1.96    0.0480     -40.8       0
## 5 median_income    -1.90    0.0468     -40.5       0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-metrics&#34; class=&#34;section level4&#34; number=&#34;4.3.1.2&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.1.2&lt;/span&gt; Performance metrics&lt;/h4&gt;
&lt;p&gt;Show average performance over all folds (note that we use &lt;code&gt;log_res&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_res %&amp;gt;%  collect_metrics(summarize = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 6
##   .metric   .estimator  mean     n  std_err .config             
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 accuracy  binary     0.849     5 0.00283  Preprocessor1_Model1
## 2 f_meas    binary     0.883     5 0.00243  Preprocessor1_Model1
## 3 kap       binary     0.672     5 0.00569  Preprocessor1_Model1
## 4 precision binary     0.871     5 0.000987 Preprocessor1_Model1
## 5 recall    binary     0.896     5 0.00430  Preprocessor1_Model1
## 6 roc_auc   binary     0.918     5 0.00117  Preprocessor1_Model1
## 7 sens      binary     0.896     5 0.00430  Preprocessor1_Model1
## 8 spec      binary     0.770     5 0.00162  Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Show performance for every single fold:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_res %&amp;gt;%  collect_metrics(summarize = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 40 x 5
##    id    .metric   .estimator .estimate .config             
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
##  1 Fold1 recall    binary         0.886 Preprocessor1_Model1
##  2 Fold1 precision binary         0.868 Preprocessor1_Model1
##  3 Fold1 f_meas    binary         0.877 Preprocessor1_Model1
##  4 Fold1 accuracy  binary         0.842 Preprocessor1_Model1
##  5 Fold1 kap       binary         0.658 Preprocessor1_Model1
##  6 Fold1 sens      binary         0.886 Preprocessor1_Model1
##  7 Fold1 spec      binary         0.766 Preprocessor1_Model1
##  8 Fold1 roc_auc   binary         0.914 Preprocessor1_Model1
##  9 Fold2 recall    binary         0.907 Preprocessor1_Model1
## 10 Fold2 precision binary         0.874 Preprocessor1_Model1
## # … with 30 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;collect-predictions&#34; class=&#34;section level4&#34; number=&#34;4.3.1.3&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.1.3&lt;/span&gt; Collect predictions&lt;/h4&gt;
&lt;p&gt;To obtain the actual model predictions, we use the function &lt;code&gt;collect_predictions&lt;/code&gt; and save the result as &lt;code&gt;log_pred&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_pred &amp;lt;- 
  log_res %&amp;gt;%
  collect_predictions()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;confusion-matrix&#34; class=&#34;section level4&#34; number=&#34;4.3.1.4&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.1.4&lt;/span&gt; Confusion matrix&lt;/h4&gt;
&lt;p&gt;Now we can use the predictions to create a &lt;em&gt;confusion matrix&lt;/em&gt; with &lt;code&gt;conf_mat()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_pred %&amp;gt;% 
  conf_mat(price_category, .pred_class) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Truth
## Prediction above below
##      above  8788  1306
##      below  1025  4361&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, the confusion matrix can quickly be visualized in different formats using &lt;code&gt;autoplot()&lt;/code&gt;. Type mosaic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_pred %&amp;gt;% 
  conf_mat(price_category, .pred_class) %&amp;gt;% 
  autoplot(type = &amp;quot;mosaic&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-56-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Or type heatmap:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_pred %&amp;gt;% 
  conf_mat(price_category, .pred_class) %&amp;gt;% 
  autoplot(type = &amp;quot;heatmap&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-57-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-curve&#34; class=&#34;section level4&#34; number=&#34;4.3.1.5&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.1.5&lt;/span&gt; ROC-Curve&lt;/h4&gt;
&lt;p&gt;We can also make an ROC curve for our 5 folds. Since the category we are predicting is the first level in the &lt;code&gt;price_category&lt;/code&gt; factor (“above”), we provide &lt;code&gt;roc_curve()&lt;/code&gt; with the relevant class probability &lt;code&gt;.pred_above&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_pred %&amp;gt;% 
  group_by(id) %&amp;gt;% # id contains our folds
  roc_curve(price_category, .pred_above) %&amp;gt;% 
  autoplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-58-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Visit &lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc&#34;&gt;Google developer’s Machine Learning Crashcourse&lt;/a&gt; to learn more about the ROC-Curve.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;probability-distributions&#34; class=&#34;section level4&#34; number=&#34;4.3.1.6&#34;&gt;
&lt;h4&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.1.6&lt;/span&gt; Probability distributions&lt;/h4&gt;
&lt;p&gt;Plot predicted probability distributions for our two classes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_pred %&amp;gt;% 
  ggplot() +
  geom_density(aes(x = .pred_above, 
                   fill = price_category), 
               alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-59-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest-2&#34; class=&#34;section level3&#34; number=&#34;4.3.2&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.2&lt;/span&gt; Random forest&lt;/h3&gt;
&lt;p&gt;We don’t repeat all of the steps shown in logistic regression and just focus on the performance metrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_res &amp;lt;-
  rf_wflow %&amp;gt;% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

rf_res %&amp;gt;%  collect_metrics(summarize = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 6
##   .metric   .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 accuracy  binary     0.858     5 0.00318 Preprocessor1_Model1
## 2 f_meas    binary     0.889     5 0.00264 Preprocessor1_Model1
## 3 kap       binary     0.690     5 0.00663 Preprocessor1_Model1
## 4 precision binary     0.876     5 0.00208 Preprocessor1_Model1
## 5 recall    binary     0.903     5 0.00442 Preprocessor1_Model1
## 6 roc_auc   binary     0.926     5 0.00179 Preprocessor1_Model1
## 7 sens      binary     0.903     5 0.00442 Preprocessor1_Model1
## 8 spec      binary     0.779     5 0.00409 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;xgboost-1&#34; class=&#34;section level3&#34; number=&#34;4.3.3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.3&lt;/span&gt; XGBoost&lt;/h3&gt;
&lt;p&gt;We don’t repeat all of the steps shown in logistic regression and just focus on the performance metrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xgb_res &amp;lt;- 
  xgb_wflow %&amp;gt;% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

xgb_res %&amp;gt;% collect_metrics(summarize = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 6
##   .metric   .estimator  mean     n  std_err .config             
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 accuracy  binary     0.859     5 0.00262  Preprocessor1_Model1
## 2 f_meas    binary     0.890     5 0.00235  Preprocessor1_Model1
## 3 kap       binary     0.694     5 0.00508  Preprocessor1_Model1
## 4 precision binary     0.881     5 0.000480 Preprocessor1_Model1
## 5 recall    binary     0.898     5 0.00485  Preprocessor1_Model1
## 6 roc_auc   binary     0.928     5 0.00121  Preprocessor1_Model1
## 7 sens      binary     0.898     5 0.00485  Preprocessor1_Model1
## 8 spec      binary     0.791     5 0.00158  Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;k-nearest-neighbor-2&#34; class=&#34;section level3&#34; number=&#34;4.3.4&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.4&lt;/span&gt; K-nearest neighbor&lt;/h3&gt;
&lt;p&gt;We don’t repeat all of the steps shown in logistic regression and just focus on the performance metrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knn_res &amp;lt;- 
  knn_wflow %&amp;gt;% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

knn_res %&amp;gt;% collect_metrics(summarize = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 6
##   .metric   .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 accuracy  binary     0.803     5 0.00437 Preprocessor1_Model1
## 2 f_meas    binary     0.845     5 0.00361 Preprocessor1_Model1
## 3 kap       binary     0.575     5 0.00926 Preprocessor1_Model1
## 4 precision binary     0.844     5 0.00402 Preprocessor1_Model1
## 5 recall    binary     0.846     5 0.00565 Preprocessor1_Model1
## 6 roc_auc   binary     0.883     5 0.00289 Preprocessor1_Model1
## 7 sens      binary     0.846     5 0.00565 Preprocessor1_Model1
## 8 spec      binary     0.729     5 0.00832 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-network-2&#34; class=&#34;section level3&#34; number=&#34;4.3.5&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.5&lt;/span&gt; Neural network&lt;/h3&gt;
&lt;p&gt;We don’t repeat all of the steps shown in logistic regression and just focus on the performance metrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nnet_res &amp;lt;- 
  nnet_wflow %&amp;gt;% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-models&#34; class=&#34;section level3&#34; number=&#34;4.3.6&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3.6&lt;/span&gt; Compare models&lt;/h3&gt;
&lt;p&gt;Extract metrics from our models to compare them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_metrics &amp;lt;- 
  log_res %&amp;gt;% 
  collect_metrics(summarise = TRUE) %&amp;gt;%
  mutate(model = &amp;quot;Logistic Regression&amp;quot;) # add the name of the model to every row

rf_metrics &amp;lt;- 
  rf_res %&amp;gt;% 
  collect_metrics(summarise = TRUE) %&amp;gt;%
  mutate(model = &amp;quot;Random Forest&amp;quot;)

xgb_metrics &amp;lt;- 
  xgb_res %&amp;gt;% 
  collect_metrics(summarise = TRUE) %&amp;gt;%
  mutate(model = &amp;quot;XGBoost&amp;quot;)

knn_metrics &amp;lt;- 
  knn_res %&amp;gt;% 
  collect_metrics(summarise = TRUE) %&amp;gt;%
  mutate(model = &amp;quot;Knn&amp;quot;)

# nnet_metrics &amp;lt;- 
#   nnet_res %&amp;gt;% 
#   collect_metrics(summarise = TRUE) %&amp;gt;%
#   mutate(model = &amp;quot;Neural Net&amp;quot;)

# create dataframe with all models
model_compare &amp;lt;- bind_rows(
                          log_metrics,
                           rf_metrics,
                           xgb_metrics,
                           knn_metrics,
                         # nnet_metrics
                           ) 

# change data structure
model_comp &amp;lt;- 
  model_compare %&amp;gt;% 
  select(model, .metric, mean, std_err) %&amp;gt;% 
  pivot_wider(names_from = .metric, values_from = c(mean, std_err)) 

# show mean F1-Score for every model
model_comp %&amp;gt;% 
  arrange(mean_f_meas) %&amp;gt;% 
  mutate(model = fct_reorder(model, mean_f_meas)) %&amp;gt;% # order results
  ggplot(aes(model, mean_f_meas, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = &amp;quot;Blues&amp;quot;) +
   geom_text(
     size = 3,
     aes(label = round(mean_f_meas, 2), y = mean_f_meas + 0.08),
     vjust = 1
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-64-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# show mean area under the curve (auc) per model
model_comp %&amp;gt;% 
  arrange(mean_roc_auc) %&amp;gt;% 
  mutate(model = fct_reorder(model, mean_roc_auc)) %&amp;gt;%
  ggplot(aes(model, mean_roc_auc, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = &amp;quot;Blues&amp;quot;) + 
     geom_text(
     size = 3,
     aes(label = round(mean_roc_auc, 2), y = mean_roc_auc + 0.08),
     vjust = 1
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-64-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that the model results are all quite similar. In our example we choose the F1-Score as performance measure to select the best model. Let’s find the maximum mean F1-Score:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_comp %&amp;gt;% slice_max(mean_f_meas)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 17
##   model   mean_accuracy mean_f_meas mean_kap mean_precision mean_recall
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 XGBoost         0.859       0.890    0.694          0.881       0.898
## # … with 11 more variables: mean_roc_auc &amp;lt;dbl&amp;gt;, mean_sens &amp;lt;dbl&amp;gt;,
## #   mean_spec &amp;lt;dbl&amp;gt;, std_err_accuracy &amp;lt;dbl&amp;gt;, std_err_f_meas &amp;lt;dbl&amp;gt;,
## #   std_err_kap &amp;lt;dbl&amp;gt;, std_err_precision &amp;lt;dbl&amp;gt;, std_err_recall &amp;lt;dbl&amp;gt;,
## #   std_err_roc_auc &amp;lt;dbl&amp;gt;, std_err_sens &amp;lt;dbl&amp;gt;, std_err_spec &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it’s time to fit the best model one last time to the full &lt;em&gt;training set&lt;/em&gt; and evaluate the resulting final model on the &lt;em&gt;test set&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;last-evaluation-on-test-set&#34; class=&#34;section level2&#34; number=&#34;4.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.4&lt;/span&gt; Last evaluation on test set&lt;/h2&gt;
&lt;p&gt;Tidymodels provides the function &lt;a href=&#34;https://tune.tidymodels.org/reference/last_fit.html&#34;&gt;&lt;code&gt;last_fit()&lt;/code&gt;&lt;/a&gt; which fits a model to the whole &lt;em&gt;training data&lt;/em&gt; and evaluates it on the &lt;em&gt;test set&lt;/em&gt;. We just need to provide the workflow object of the best model as well as the &lt;strong&gt;data split&lt;/strong&gt; object (not the training data).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;last_fit_rf &amp;lt;- last_fit(rf_wflow, 
                        split = data_split,
                        metrics = metric_set(
                          recall, precision, f_meas, 
                          accuracy, kap,
                          roc_auc, sens, spec)
                        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Show performance metrics&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;last_fit_rf %&amp;gt;% 
  collect_metrics()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 4
##   .metric   .estimator .estimate .config             
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 recall    binary         0.895 Preprocessor1_Model1
## 2 precision binary         0.877 Preprocessor1_Model1
## 3 f_meas    binary         0.886 Preprocessor1_Model1
## 4 accuracy  binary         0.854 Preprocessor1_Model1
## 5 kap       binary         0.683 Preprocessor1_Model1
## 6 sens      binary         0.895 Preprocessor1_Model1
## 7 spec      binary         0.783 Preprocessor1_Model1
## 8 roc_auc   binary         0.925 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And these are our final performance metrics. Remember that if a model fit to the training dataset also fits the test dataset well, minimal &lt;em&gt;overfitting&lt;/em&gt; has taken place. This seems to be also the case in our example.&lt;/p&gt;
&lt;p&gt;To learn more about the model we can access the variable importance scores via the &lt;code&gt;.workflow&lt;/code&gt; column. We first need to pluck out the first element in the workflow column, then pull out the fit from the workflow object. Finally, the &lt;code&gt;vip&lt;/code&gt; package helps us visualize the variable importance scores for the top features. Note that we can’t create this type of plot for every model engine.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(vip)

last_fit_rf %&amp;gt;% 
  pluck(&amp;quot;.workflow&amp;quot;, 1) %&amp;gt;%   
  pull_workflow_fit() %&amp;gt;% 
  vip(num_features = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-68-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The two most important predictors in whether a district has a median house value above or below 150000 dollars were the ocean proximity inland and the median income.&lt;/p&gt;
&lt;p&gt;Take a look at the confusion matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;last_fit_rf %&amp;gt;%
  collect_predictions() %&amp;gt;% 
  conf_mat(price_category, .pred_class) %&amp;gt;% 
  autoplot(type = &amp;quot;heatmap&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-69-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s create the ROC curve. Again, since the event we are predicting is the first level in the &lt;code&gt;price_category&lt;/code&gt; factor (“above”), we provide &lt;code&gt;roc_curve()&lt;/code&gt; with the relevant class probability &lt;code&gt;.pred_above&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;last_fit_rf %&amp;gt;% 
  collect_predictions() %&amp;gt;% 
  roc_curve(price_category, .pred_above) %&amp;gt;% 
  autoplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2021-02-17-r-classification-tidymodels/index_files/figure-html/unnamed-chunk-70-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on all of the results, the validation set and test set performance statistics are very close, so we would have pretty high confidence that our random forest model with the selected hyperparameters would perform well when predicting new data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Learn the Basics of AI</title>
      <link>https://kirenz.com/project/intro-machine-learning/</link>
      <pubDate>Thu, 07 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/intro-machine-learning/</guid>
      <description>&lt;p&gt;&lt;strong&gt;General introduction to machine learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A good place to start your journey into the basics of machine learning &amp;ndash;the science of getting computers to act without being explicitly programmed&amp;ndash; are the non-technical and easy to follow introductions to machine learning by Yufeng Guo from &lt;em&gt;Google&amp;rsquo;s AI Adventures&lt;/em&gt; video-series:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=HcqpanDadyQ&#34; title=&#34;What is Machine Learning?&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/HcqpanDadyQ/0.jpg&#34; alt=&#34;What is Machine Learning?&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nKW8Ndu7Mjw&#34; title=&#34;The 7 Steps of Machine Learning?&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/nKW8Ndu7Mjw/0.jpg&#34; alt=&#34;The 7 Steps of Machine Learning&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Overview about the different machine learning algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Vasily Zubarev provides a very casual introduction to machine learning using easy to understand language and real-world examples in his blog vas3k.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://vas3k.com/blog/machine_learning/?ref=hn&#34; target=&#34;_blank&#34;&gt;Machine Learning for Everyone&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Interactive visual example of a decision tree algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;R2D3 is an experiment in expressing statistical thinking with interactive design and they provide an excellent visual introduction to machine learning:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.r2d3.us/visual-intro-to-machine-learning-part-1/&#34; target=&#34;_blank&#34;&gt;Visual introduction to machine learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;In this video, Google&amp;rsquo;s Laurence Moroney will guide you through the basic principles of machine learning (especially deep learning):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VwVg9jCtqaU&amp;quot;Machine Learning Zero to Hero (Google I/O&#39;19&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/VwVg9jCtqaU/0.jpg&#34; alt=&#34;Machine Learning Zero to Hero (Google I/O&#39;19)&#34; /&gt;&lt;/a&gt;, Laurence Moroney&amp;rdquo;)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Introduction to Recommender Systems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CS50 is Harvard University&amp;rsquo;s introduction to the intellectual enterprises of computer science and the art of programming. They provide an easy to follow introduction to recommender systems:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Eeg1DEeWUjA&#34; title=&#34;Recommender Systems&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/Eeg1DEeWUjA/0.jpg&#34; alt=&#34;Recommender Systems&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Overview about crucial machine learning content&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ai.google/static/images/share.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/&#34; target=&#34;_blank&#34;&gt;Google&amp;rsquo;s Machine Learning Crash Course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Google recently published a series of internal AI training resources originally developed for its engineers. The &lt;em&gt;crash course&lt;/em&gt; provides a fast-paced and practical overview about the fundamental concepts of machine learning. Here you can learn and apply fundamental machine learning concepts, get real-world examples with the companion &lt;a href=&#34;https://www.kaggle.com&#34; target=&#34;_blank&#34;&gt;Kaggle competition&lt;/a&gt;, or visit &lt;a href=&#34;https://ai.google/education/&#34; target=&#34;_blank&#34;&gt;Learn with Google AI&lt;/a&gt; to explore the full library of training resources.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Introduction to Neural Networks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Excellent introduction to neural networks created by Grant Sanderson.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aircAruvnKk&#34; title=&#34;What is Machine Learning?&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/aircAruvnKk/0.jpg&#34; alt=&#34;Neural networks&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Advanced material:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Detailed machine learning course with code development&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.andrewng.org/content/uploads/2013/04/photo_about_w211.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34; target=&#34;_blank&#34;&gt;Introduction to Machine Learning&lt;/a&gt; from Andrew Ng&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this excellent course from Andrew Ng, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself.&lt;/p&gt;

&lt;p&gt;This course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks).&lt;/li&gt;
&lt;li&gt;(ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning).&lt;/li&gt;
&lt;li&gt;(iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI).&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Data Science with Tidymodels, Workflows and Recipes</title>
      <link>https://kirenz.com/post/2020-12-19-r-tidymodels-housing/</link>
      <pubDate>Sat, 19 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/post/2020-12-19-r-tidymodels-housing/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-understanding&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Data understanding&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#imort-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1&lt;/span&gt; Imort Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-overview&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2&lt;/span&gt; Data overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-exploration&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3&lt;/span&gt; Data exploration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-preparation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Data preparation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-splitting&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Data splitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#recipes&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Recipes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-building&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Model building&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#model-specification&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.1&lt;/span&gt; Model specification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#create-workflow&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.2&lt;/span&gt; Create workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluate-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.3&lt;/span&gt; Evaluate model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#last-fit-and-evaluation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.4&lt;/span&gt; Last fit and evaluation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;The following examples are adapted from the excellent book “Hands-on machine learning with scikit-learn, keras and tensorflow” from A. Geron and the &lt;a href=&#34;https://www.tidymodels.org/start/recipes/&#34;&gt;tidymodels documentation&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this tutorial you will learn how to specify a simple regression model with the tidymodels package using recipes, which is designed to help you preprocess your data before training your model.&lt;/p&gt;
&lt;p&gt;To use the code in this article, you will need to install the following packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;tidymodels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html&#34;&gt;skimr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ggobi.github.io/ggally/index.html&#34;&gt;GGally&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/dkahle/ggmap&#34;&gt;ggmap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidymodels)
library(skimr)
library(GGally)
library(ggmap)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, our goal is to build a model of housing prices in California. In particular, the model should learn from California census data and be able to predict the median house price in any district (population of 600 to 3000 people), given some predictor variables. We use the root mean square error (RMSE) as a performance measure for our regression problem.&lt;/p&gt;
&lt;div id=&#34;data-understanding&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Data understanding&lt;/h1&gt;
&lt;p&gt;In Data Understanding, we first&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Import data&lt;/li&gt;
&lt;li&gt;Get an overview about the data structure&lt;/li&gt;
&lt;li&gt;Discover and visualize the data to gain insights&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;imort-data&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Imort Data&lt;/h2&gt;
&lt;p&gt;First of all, let’s import the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LINK &amp;lt;- &amp;quot;https://raw.githubusercontent.com/kirenz/datasets/master/housing.csv&amp;quot;
housing_df &amp;lt;- read_csv(LINK)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-overview&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Data overview&lt;/h2&gt;
&lt;p&gt;Next, we take a look at the data structure:&lt;/p&gt;
&lt;p&gt;California census top 4 rows of the DataFrame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(housing_df, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 10
##   longitude latitude housing_median_… total_rooms total_bedrooms population
##       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1     -122.     37.9               41         880            129        322
## 2     -122.     37.9               21        7099           1106       2401
## 3     -122.     37.8               52        1467            190        496
## 4     -122.     37.8               52        1274            235        558
## # … with 4 more variables: households &amp;lt;dbl&amp;gt;, median_income &amp;lt;dbl&amp;gt;,
## #   median_house_value &amp;lt;dbl&amp;gt;, ocean_proximity &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Data info:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(housing_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 20,640
## Columns: 10
## $ longitude          &amp;lt;dbl&amp;gt; -122.23, -122.22, -122.24, -122.25, -122.25, -122.…
## $ latitude           &amp;lt;dbl&amp;gt; 37.88, 37.86, 37.85, 37.85, 37.85, 37.85, 37.84, 3…
## $ housing_median_age &amp;lt;dbl&amp;gt; 41, 21, 52, 52, 52, 52, 52, 52, 42, 52, 52, 52, 52…
## $ total_rooms        &amp;lt;dbl&amp;gt; 880, 7099, 1467, 1274, 1627, 919, 2535, 3104, 2555…
## $ total_bedrooms     &amp;lt;dbl&amp;gt; 129, 1106, 190, 235, 280, 213, 489, 687, 665, 707,…
## $ population         &amp;lt;dbl&amp;gt; 322, 2401, 496, 558, 565, 413, 1094, 1157, 1206, 1…
## $ households         &amp;lt;dbl&amp;gt; 126, 1138, 177, 219, 259, 193, 514, 647, 595, 714,…
## $ median_income      &amp;lt;dbl&amp;gt; 8.3252, 8.3014, 7.2574, 5.6431, 3.8462, 4.0368, 3.…
## $ median_house_value &amp;lt;dbl&amp;gt; 452600, 358500, 352100, 341300, 342200, 269700, 29…
## $ ocean_proximity    &amp;lt;chr&amp;gt; &amp;quot;NEAR BAY&amp;quot;, &amp;quot;NEAR BAY&amp;quot;, &amp;quot;NEAR BAY&amp;quot;, &amp;quot;NEAR BAY&amp;quot;, &amp;quot;N…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Data summary of numerical and categorical attributes using a function from the package &lt;code&gt;skimr&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;skim(housing_df)&lt;/code&gt;&lt;/pre&gt;
&lt;table style=&#34;width: auto;&#34; class=&#34;table table-condensed&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-5&#34;&gt;Table 1.1: &lt;/span&gt;Data summary
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Name
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
housing_df
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Number of rows
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
20640
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Number of columns
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
_______________________
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Column type frequency:
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
character
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
numeric
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
________________________
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Group variables
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
None
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: character&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
skim_variable
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n_missing
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
complete_rate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
min
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
max
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
empty
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n_unique
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
whitespace
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ocean_proximity
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: numeric&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
skim_variable
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n_missing
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
complete_rate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
mean
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sd
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
p0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
p25
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
p50
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
p75
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
p100
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
hist
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
longitude
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-119.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-124.35
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-121.80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-118.49
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-118.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-114.31
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
▂▆▃▇▁
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
latitude
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35.63
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
32.54
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.93
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34.26
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
41.95
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
▇▁▅▂▁
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
housing_median_age
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
28.64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
52.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
▃▇▇▇▅
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
total_rooms
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2635.76
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2181.62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1447.75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2127.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3148.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39320.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
▇▁▁▁▁
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
total_bedrooms
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
207
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
537.87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
421.39
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
296.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
435.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
647.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6445.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
▇▁▁▁▁
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
population
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1425.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1132.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
787.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1166.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1725.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35682.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
▇▁▁▁▁
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
households
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
499.54
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
382.33
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
280.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
409.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
605.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6082.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
▇▁▁▁▁
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
median_income
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.53
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.74
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
▇▇▁▁▁
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
median_house_value
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
206855.82
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
115395.62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14999.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
119600.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
179700.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
264725.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500001.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
▅▇▅▂▂
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Count levels of our categorical variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_df %&amp;gt;% 
  count(ocean_proximity,
        sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   ocean_proximity     n
##   &amp;lt;chr&amp;gt;           &amp;lt;int&amp;gt;
## 1 &amp;lt;1H OCEAN        9136
## 2 INLAND           6551
## 3 NEAR OCEAN       2658
## 4 NEAR BAY         2290
## 5 ISLAND              5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;ggscatmat&lt;/code&gt; from the package &lt;code&gt;GGally&lt;/code&gt; creates a matrix with scatterplots, densities and correlations for numeric columns. In our code, we enter the dataset &lt;code&gt;housing_df&lt;/code&gt;, choose columns 6 to 9, a color column for our categorical variable &lt;code&gt;ocean_proximity&lt;/code&gt;, and an alpha level of 0.8 (for transparency).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggscatmat(housing_df, columns = 6:9, color=&amp;quot;ocean_proximity&amp;quot;, alpha=0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To obtain an overview of even more visualizations, we can use the function &lt;code&gt;ggpairs&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggpairs(housing_df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-exploration&#34; class=&#34;section level2&#34; number=&#34;1.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; Data exploration&lt;/h2&gt;
&lt;p&gt;A Geographical scatterplot of the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_df %&amp;gt;% 
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(color = &amp;quot;cornflowerblue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:point-long-lat&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;index_files/figure-html/point-long-lat-1.png&#34; alt=&#34;Scatterplot of longitude and latitude&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Scatterplot of longitude and latitude
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A better visualization that highlights high-density areas:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_df %&amp;gt;% 
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(color = &amp;quot;cornflowerblue&amp;quot;, alpha = 0.1) &lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:point-long-lat-a&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;index_files/figure-html/point-long-lat-a-1.png&#34; alt=&#34;Scatterplot of longitude and latitude that highlights high-density areas&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.2: Scatterplot of longitude and latitude that highlights high-density areas
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;California housing prices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;red is expensive,&lt;/li&gt;
&lt;li&gt;purple is cheap and&lt;/li&gt;
&lt;li&gt;larger circles indicate areas with a larger population.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_df %&amp;gt;% 
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(aes(size = population, color = median_house_value), 
             alpha = 0.4) +
  scale_colour_gradientn(colours=rev(rainbow(4)))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:plot-ca-prices&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;index_files/figure-html/plot-ca-prices-1.png&#34; alt=&#34;California housing_df prices&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.3: California housing_df prices
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmap)

qmplot(x = longitude, 
       y = latitude, 
       data = housing_df, 
       geom = &amp;quot;point&amp;quot;, 
       color = median_house_value, 
       size = population,
       alpha = 0.4) +
  scale_colour_gradientn(colours=rev(rainbow(4)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-preparation&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Data preparation&lt;/h1&gt;
&lt;div id=&#34;data-splitting&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Data splitting&lt;/h2&gt;
&lt;p&gt;Before we build our model, we first split data into training and test set using stratified sampling.&lt;/p&gt;
&lt;p&gt;Let’s assume we would know that the median income is a very important attribute to predict median housing prices. Therefore, we would want to create a training and test set using stratified sampling.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;stratum&lt;/em&gt; (plural strata) refers to a subset (part) of the population (entire collection of items under consideration) which is being sampled:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_df %&amp;gt;% 
  ggplot(aes(median_income)) +
  geom_histogram(bins = 30)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:hist-med-income&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;index_files/figure-html/hist-med-income-1.png&#34; alt=&#34;Histogram of Median Income&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.1: Histogram of Median Income
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We want to ensure that the test set is representative of the various categories of incomes in the whole dataset. In other words, we would like to have instances for each &lt;em&gt;stratum&lt;/em&gt;, or else the estimate of a stratum’s importance may be biased. This means that you should not have too many strata, and each stratum should be large enough. We use 5 strata in our example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)

new_split &amp;lt;- initial_split(housing_df, 
                           prop = 3/4, 
                           strata = median_income, 
                           breaks = 5)

new_train &amp;lt;- training(new_split) 
new_test &amp;lt;- testing(new_split)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;recipes&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Recipes&lt;/h2&gt;
&lt;p&gt;Next, we use a &lt;code&gt;recipe()&lt;/code&gt; to build a set of steps for data preprocessing and feature engineering.&lt;/p&gt;
&lt;p&gt;Recipes are built as a series of preprocessing steps, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;converting qualitative predictors to indicator variables (also known as dummy variables),&lt;/li&gt;
&lt;li&gt;transforming data to be on a different scale (e.g., taking the logarithm of a variable),&lt;/li&gt;
&lt;li&gt;transforming whole groups of predictors together,&lt;/li&gt;
&lt;li&gt;extracting key features from raw variables (e.g., getting the day of the week out of a date variable),&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, the idea of the &lt;a href=&#34;https://recipes.tidymodels.org&#34;&gt;recipes package&lt;/a&gt; is to define a recipe or blueprint that can be used to sequentially define the encodings and preprocessing of the data (i.e. “feature engineering”) before we build our models.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;First, we must tell the &lt;code&gt;recipe()&lt;/code&gt; what our model is going to be (using a formula here) and what our training data is.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;step_novel()&lt;/code&gt; will convert all nominal variables to factors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We then convert the factor columns into (one or more) numeric binary (0 and 1) variables for the levels of the training data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We remove any numeric variables that have zero variance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We normalize (center and scale) the numeric variables.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;housing_rec &amp;lt;-
  recipe(median_house_value ~ ., data = new_train) %&amp;gt;%
  step_novel(all_nominal(), -all_outcomes()) %&amp;gt;%
  step_dummy(all_nominal()) %&amp;gt;%
  step_zv(all_predictors()) %&amp;gt;%
  step_normalize(all_predictors())

# Show the content of our recipe
housing_rec&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          9
## 
## Operations:
## 
## Novel factor level assignment for all_nominal(), -all_outcomes()
## Dummy variables from all_nominal()
## Zero variance filter on all_predictors()
## Centering and scaling for all_predictors()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it’s time to &lt;strong&gt;specify&lt;/strong&gt; and then &lt;strong&gt;fit&lt;/strong&gt; our models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-building&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Model building&lt;/h1&gt;
&lt;div id=&#34;model-specification&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Model specification&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Pick a &lt;code&gt;model type&lt;/code&gt;: choose from this &lt;a href=&#34;https://www.tidymodels.org/find/parsnip/&#34;&gt;list&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Set the &lt;code&gt;engine&lt;/code&gt;: choose from this &lt;a href=&#34;https://www.tidymodels.org/find/parsnip/&#34;&gt;list&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Set the &lt;code&gt;mode&lt;/code&gt;: regression or classification&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)

lm_spec &amp;lt;- # your model specification
  linear_reg() %&amp;gt;%  # model type
  set_engine(engine = &amp;quot;lm&amp;quot;) %&amp;gt;%  # model engine
  set_mode(&amp;quot;regression&amp;quot;) # model mode

# Show your model specification
lm_spec&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear Regression Model Specification (regression)
## 
## Computational engine: lm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To combine the data preparation with the model building, we use the package &lt;a href=&#34;https://workflows.tidymodels.org&#34;&gt;workflows&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A workflow is an object that can bundle together your pre-processing, modeling, and post-processing requests&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-workflow&#34; class=&#34;section level2&#34; number=&#34;3.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Create workflow&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm_wflow &amp;lt;-
 workflow() %&amp;gt;%
 add_model(lm_spec) %&amp;gt;% 
 add_recipe(housing_rec)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-model&#34; class=&#34;section level2&#34; number=&#34;3.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3&lt;/span&gt; Evaluate model&lt;/h2&gt;
&lt;p&gt;We build a validation set with K-fold crossvalidation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)

cv_folds &amp;lt;-
 vfold_cv(new_train, 
          v = 5, 
          strata = median_income,
          breaks = 5) 

cv_folds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #  5-fold cross-validation using stratification 
## # A tibble: 5 x 2
##   splits               id   
##   &amp;lt;list&amp;gt;               &amp;lt;chr&amp;gt;
## 1 &amp;lt;split [12.4K/3.1K]&amp;gt; Fold1
## 2 &amp;lt;split [12.4K/3.1K]&amp;gt; Fold2
## 3 &amp;lt;split [12.4K/3.1K]&amp;gt; Fold3
## 4 &amp;lt;split [12.4K/3.1K]&amp;gt; Fold4
## 5 &amp;lt;split [12.4K/3.1K]&amp;gt; Fold5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can fit the model and collect the performance metrics with &lt;code&gt;collect_metrics()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm_wflow_eval &amp;lt;- 
  lm_wflow %&amp;gt;% 
  fit_resamples(
    median_house_value ~ ., 
    resamples = cv_folds
    ) 

lm_wflow_eval%&amp;gt;% 
    collect_metrics()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator      mean     n   std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   69040.        5 787.      Preprocessor1_Model1
## 2 rsq     standard       0.644     5   0.00983 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Usually, we would fit multiple models and select the one with the smallest RMSE. In this example, we only demonstrate the process with one model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;last-fit-and-evaluation&#34; class=&#34;section level2&#34; number=&#34;3.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.4&lt;/span&gt; Last fit and evaluation&lt;/h2&gt;
&lt;p&gt;Fit the best model to the training set and evaluate the test set with the function &lt;a href=&#34;https://tune.tidymodels.org/reference/last_fit.html&#34;&gt;&lt;code&gt;last_fit()&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;last_fit_lm &amp;lt;- last_fit(lm_wflow, split = new_split)

# Show RMSE and RSQ
last_fit_lm %&amp;gt;% 
  collect_metrics()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
##   .metric .estimator .estimate .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   68182.    Preprocessor1_Model1
## 2 rsq     standard       0.650 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applied Statistics</title>
      <link>https://kirenz.com/talk/2019-web-analytics-social-media/</link>
      <pubDate>Fri, 04 Dec 2020 09:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/talk/2019-web-analytics-social-media/</guid>
      <description>&lt;p&gt;In dem Kurs Angewandte Statistik (Applied Statistics) werden zentrale statistische Methoden und Modelle für die fundierte Analyse quantitativer Daten behandelt. Der Schwerpunkt der Veranstaltung liegt in der Vermittlung moderner statistischer Verfahren (bspw. Lasso Regression, Generalized Additive Models, Cross Validation, Bootstrapping) anhand praxisbezogener Fallstudien. In den Fallstudien werden die statistischen Berechnungen und Modellierungen anhand der populären Programmiersprachen R und Python vorgestellt.&lt;/p&gt;

&lt;p&gt;Inhaltliche Schwerpunkte:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Deskriptive Statistik&lt;/li&gt;
&lt;li&gt;Explorative Datenanalyse&lt;/li&gt;
&lt;li&gt;Wahrscheinlichkeitstheorie und Bayes-Theorem&lt;/li&gt;
&lt;li&gt;Hypothesentests&lt;/li&gt;
&lt;li&gt;Regression&lt;/li&gt;
&lt;li&gt;Klassifikation&lt;/li&gt;
&lt;li&gt;Cross Validation und Bootstrapping&lt;/li&gt;
&lt;li&gt;Feature-Subset-Selection&lt;/li&gt;
&lt;li&gt;Regularization&lt;/li&gt;
&lt;li&gt;Nicht lineare Modelle&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nach Abschluss des Moduls verfügen die Teilnehmenden über:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Umfangreiches Wissen zu modernen statistischen Methoden und Modellen für die fundierte Analyse unterschiedlicher Datentypen.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Die Fähigkeit, Ergebnisse statistischer Analysen zu interpretieren.&lt;/li&gt;
&lt;li&gt;Kenntnisse für eine verständliche und visuell ansprechende Ergebnisdarstellung.&lt;/li&gt;
&lt;li&gt;Anwendungsbezogene Kenntnisse statistischer Programmierung in Python und R.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hierarchische Clusteranalyse mit Ward in R</title>
      <link>https://kirenz.com/post/2020-05-21-r-hierarchische-clusteranalyse/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/post/2020-05-21-r-hierarchische-clusteranalyse/</guid>
      <description>
&lt;script src=&#34;https://kirenz.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#grundlagen&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Grundlagen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#proximitätsmaß&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Proximitätsmaß&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#euklidische-distanz&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Euklidische Distanz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quadrierte-euklidische-distanz&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Quadrierte euklidische Distanz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#l_1-distanz&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.3&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt;-Distanz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clustering-algorithmus&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Clustering-Algorithmus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementierung-in-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Implementierung in R&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#datenvorbereitung&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1&lt;/span&gt; Datenvorbereitung&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#variablenauswahl&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1.1&lt;/span&gt; Variablenauswahl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fehlende-werte&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1.2&lt;/span&gt; Fehlende Werte&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standardisierung&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1.3&lt;/span&gt; Standardisierung&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#proximitätsmaß-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2&lt;/span&gt; Proximitätsmaß&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchische-clusteranalyse&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3&lt;/span&gt; Hierarchische Clusteranalyse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dendrogramm&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.4&lt;/span&gt; Dendrogramm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;In diesem Tutorial werden die Grundlagen der Clusteranalyse beschrieben und die hierarchische Clusteranalyse mit der Ward-Methode in R umgesetzt.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;grundlagen&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Grundlagen&lt;/h1&gt;
&lt;p&gt;Die Clusteranalyse ist ein exploratives Verfahren um Ähnlichkeitsstrukturen in Daten zu erkennen. Bei den Untersuchungsobjekten einer Clusteranalyse kann es sich sowohl um Personen, Produkte oder um beliebige andere Einheiten wie Filme, Länder oder Unternehmen handeln. Durch die Anwendung der Clusteranalyse können diese Objekte anhand ihrer Eigenschaftsausprägungen zu Clustern zusammengefasst werden. Dabei soll jedes Cluster in sich möglichst gleichartig (homogen) sein und sich gleichzeitig von den anderen Clustern möglichst stark unterscheiden (heterogen).
Beispielsweise erfasst der Streaminganbieter Netflix die Sehgewohnheiten seiner Abonnenten und hat auf dieser Grundlage über 2000 Mikro-Cluster, sogenannte “Taste Communities”, gebildet (&lt;a href=&#34;https://www.vulture.com/2018/06/how-netflix-swallowed-tv-industry.html&#34;&gt;New York Magazine, 2018&lt;/a&gt;). Den Mitgliedern der jeweiligen Clustern sollen anhand der jeweiligen Clusterzugehörigkeit möglichst passende Inhalte vorgeschlagen werden. Die Filme können dabei ebenfalls anhand unterschiedlicher Merkmale geclustert und im Anschluss mit aussagekräftigen Bezeichnungen versehen werden:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;netflix.png&#34; alt=&#34;Zusammenhänge zwischen verschiedenen Serien, dargestellt in Clustern (Quelle: [Netflix, 2017](https://media.netflix.com/de/press-releases/decoding-the-defenders-netflix-unveils-the-gateway-shows-that-lead-to-a-heroic-binge))&#34; width=&#34;70%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Zusammenhänge zwischen verschiedenen Serien, dargestellt in Clustern (Quelle: &lt;a href=&#34;https://media.netflix.com/de/press-releases/decoding-the-defenders-netflix-unveils-the-gateway-shows-that-lead-to-a-heroic-binge&#34;&gt;Netflix, 2017&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Wichtige Voraussetzungen, die bei der Durchführung der Analyse beachtet werden sollten &lt;a href=&#34;https://www.methodenberatung.uzh.ch/de/datenanalyse_spss/interdependenz/gruppierung/cluster.html&#34;&gt;(Universität Zürich, 2018)&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Die Analyse kann für unterschiedliche Datentypen (kategoriale und metrische Daten) genutzt werden.&lt;/li&gt;
&lt;li&gt;Fehlende Werte und Ausreißerwerte sollten vorab beseitigt werden.&lt;/li&gt;
&lt;li&gt;Weisen die verwendeten Variablen große Unterschiede bezüglich ihres Wertebereichs auf (bspw. wenn eine Variable in cm und die andere in km gemessen wurde), so sollten diese auf ein einheitliches Niveau transformiert werden. Üblicherweise wird dafür die z-Transformation genutzt.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bei der Berechnung der Cluster wird nach bestimmten Regeln entschieden, wie die Objekte zu Clustern zusammengefasst werden. Das Ergebnis dieses Prozesses hängt nicht nur von der Wahl des Clustering-Algorithmus ab, sondern auch davon, wie die Distanz oder Ähnlichkeit zwischen den Objekten bestimmt wird.&lt;/p&gt;
&lt;p&gt;Zu Beginn der Clusteranalyse wird daher in Abhängigkeit von den vorliegenden Datentypen ein sogenanntes &lt;em&gt;Proximitätsmaß&lt;/em&gt; gewählt.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;proximitätsmaß&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Proximitätsmaß&lt;/h1&gt;
&lt;p&gt;Mit Hilfe des Proximitätsmaßes wird die Distanz zwischen den Objekten berechnet. In Abhängigkeit von dem Skalenniveau der Variablen wird eine Distanzfunktion zur Bestimmung des Abstandes (Distanz) zweier Elemente oder eine Ähnlichkeitsfunktion zur Bestimmung der Ähnlichkeit verwendet:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Bei kategorialen (nominalen und ordinalen) Variablen werden Ähnlichkeitsmaße benutzt.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bei metrischen Variablen werden Distanzmaße genutzt.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In diesem Tutorial behandeln wir die Distanzmaße “euklidische Distanz” (auch &lt;span class=&#34;math inline&#34;&gt;\(L_2\)&lt;/span&gt; genannt), “quadrierte euklidische Distanz” und die “L1-Distanz” (auch Manhattan-Metrik, Manhattan-Distanz, Mannheimer Metrik, Taxi- oder Cityblock-Metrik geannt).&lt;/p&gt;
&lt;div id=&#34;euklidische-distanz&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Euklidische Distanz&lt;/h2&gt;
&lt;p&gt;Mit Hilfe der euklidischen Distanz kann der Abstand zwischen zwei Punkten als gerade Linie in einem Raum berechnet werden (“Luftliniendistanz”). Anders formuliert ist der euklidische Abstand zweier Punkte die mit einem Lineal gemessene Länge einer Strecke, die diese zwei Punkte verbindet. Ein Distanzwert von Null bedeutet dabei, dass die Objekte einen Abstand von Null aufweisen, also identisch sind.&lt;/p&gt;
&lt;p&gt;Die Formel für die Berechnung der euklidischen Distanz für &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; verschiedenen Variablen lautet:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = \sqrt{\sum_{i=1}^{n}(A_i - B_i)^2}\]&lt;/span&gt;
Die Formel kann in einem zweidimensionalen Koordinatensystem mit den beiden Variablen &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; und &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (d.h. n = 2) wie folgt visualisiert werden:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://miro.medium.com/max/1524/1*J2bK-UKhrW1Ill5EyAxXOQ.png&#34; alt=&#34;Die euklidische Distanz von Punkt A zu Punkt B  (Quelle: [Korstanje, 2019](https://towardsdatascience.com/3-distances-that-every-data-scientist-should-know-59d864e5030a))&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.1: Die euklidische Distanz von Punkt A zu Punkt B (Quelle: &lt;a href=&#34;https://towardsdatascience.com/3-distances-that-every-data-scientist-should-know-59d864e5030a&#34;&gt;Korstanje, 2019&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Wie aus dem Punktediagramm entnommen werden kann, gelten für die Punkte A und B:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_A\)&lt;/span&gt; = 70&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_B\)&lt;/span&gt; = 330&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_A\)&lt;/span&gt; = 40&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(y_B\)&lt;/span&gt; = 228&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Da wir in diesem Beispiel nur 2 Variablen vorliegen haben (n = 2), gilt hier ein bekannter Spezialfall der Berechnung des euklidischen Abstandes: der Satz des Pythagoras. Für die Berechnung der euklidischen Distanz werden daher lediglich die (X,Y)-Koordinaten benötigt um mit Hilfe der Formel von Pythagoras die Distanz zu berechnen:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = \sqrt{(x_A-x_B)^2 + (y_A-y_B)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = \sqrt{(70-330)^2 + (40-228)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = \sqrt{(-260)^2 + (-188)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = \sqrt{(76600 + 35344) }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = \sqrt{(112225) }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = 335\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quadrierte-euklidische-distanz&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Quadrierte euklidische Distanz&lt;/h2&gt;
&lt;p&gt;Anstelle der einfachen euklidischen Distanz kann auch die quadrierte euklidische Distanz als Distanzmaß genutzt werden. Dadurch werden größere Abweichungen stärker gewichtet. Die Formel der quadrierten euklidischen Distanz lautet:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d^2(A,B) = \sum_{i=1}^{n}(A_i - B_i)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Für unser Datenbeispiel gilt daher:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d^2(A,B) = (x_A-x_B)^2 + (y_A-y_B)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d^2(A,B) = (70-330)^2 + (40-228)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d^2(A,B) = 112225\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;l_1-distanz&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt;-Distanz&lt;/h2&gt;
&lt;p&gt;Die &lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt;-Distanz (auch Manhattan-Metrik, Manhattan-Distanz, Mannheimer Metrik, Taxi- oder Cityblock-Metrik) ist eine Metrik, in der die Distanz &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; zwischen zwei Punkten &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; und &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; als die Summe der absoluten Differenzen ihrer Einzelkoordinaten definiert wird. Dies ist insbesondere bei der Berechnung von geografischen Abständen relevant, bei welchen der Abstand zwischen zwei Punkten über vordefinierte Wege (bspw. Straßen in einer Stadt mit einer blockartigen Struktur wie in Manhattan oder Mannheim) zurückgelegt werden muss.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:manhattan-distance&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://miro.medium.com/max/1400/1*88uZae0Utf7kavhQFvMqaw.png&#34; alt=&#34;Die L1 Distanz von Punkt A zu Punkt B als Manhatten-Metrik. [(Quelle: Korstanje, 2019](https://towardsdatascience.com/3-distances-that-every-data-scientist-should-know-59d864e5030a))&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.2: Die L1 Distanz von Punkt A zu Punkt B als Manhatten-Metrik. &lt;a href=&#34;https://towardsdatascience.com/3-distances-that-every-data-scientist-should-know-59d864e5030a&#34;&gt;(Quelle: Korstanje, 2019&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Wie aus der Abbildung ersichtlich wird, existieren mehrere Möglichkeiten, den Abstand zwischen den Punkten A und B zu berechnen. Wichtig ist jedoch, dass die “Straßen” nicht verlassen werden dürfen. D.h. es können bspw. zwei Blöcke nach oben (Norden) und dann drei Blöcke nach rechts (Osten) auf der Fahrbahn zurückgelegt werden, um von Punkt A aus Punkt B zu erreichen. Unabhängig von dem gewählten Pfad ist die Distanz aufgrund der blockartigen Struktur immer die gleiche.&lt;/p&gt;
&lt;p&gt;Allgemein lautet die Formel für die Berechnung des L1-Abstands wie folgt:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = \sum_{i} |A_i - B_i|\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In unserem Fall gilt:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = |x_A - x_B| + |y_A - y_B |\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = |70 - 330| + |40 - 228 |\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = |-260 | + |-188|\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = 260 + 188\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d(A,B) = 448 \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;clustering-algorithmus&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Clustering-Algorithmus&lt;/h1&gt;
&lt;p&gt;Ist das Proximitätsmaß berechnet, so wird anhand eines Clustering-Algorithmus die eigentliche Gruppierung der Daten vorgenommen. In dieser Abbildung sind beispielhaft einige Clustering-Algorithmen aufgeführt:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://www.methodenberatung.uzh.ch/dam/jcr:ffffffff-81eb-fc79-0000-000008e2c10d/Clus_Abb_04.jpg&#34; alt=&#34;Überblick über Clustering-Algorithmen (Quelle: [Universität Zürich,  2018](https://www.methodenberatung.uzh.ch/de/datenanalyse_spss/interdependenz/gruppierung/cluster.html))&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3.1: Überblick über Clustering-Algorithmen (Quelle: &lt;a href=&#34;https://www.methodenberatung.uzh.ch/de/datenanalyse_spss/interdependenz/gruppierung/cluster.html&#34;&gt;Universität Zürich, 2018&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Bei den hier dargestellten Algorithmen wird zwischen &lt;strong&gt;hierarchischen&lt;/strong&gt; und &lt;strong&gt;nicht-hierarchischen&lt;/strong&gt; Algorithmen unterschieden. Im Rahmen dieses Tutorials werden ausschließlich hierarchische Algorithmen behandelt. Diese werden weiter in agglomerative und divisive Verfahren unterteilt.&lt;/p&gt;
&lt;p&gt;Bei &lt;strong&gt;divisiven&lt;/strong&gt; Verfahren wird zunächst ein Cluster gebildet, welches alle Datenpunkte enthält. Dieses wird dann schrittweise in kleinere Cluster zerteilt, bis jeder Fall ein eigenes Cluster bildet. Bei &lt;strong&gt;agglomerativen&lt;/strong&gt; Verfahren werden die Datenpunkte zuerst einzeln betrachtet (d.h. jeder Fall ist ein eigenes Cluster) und dann schrittweise zu größeren Clustern zusammengefasst. Die agglomerativen Verfahren werden in Linkage-Methoden und Varianz-Methoden unterteilt.&lt;/p&gt;
&lt;p&gt;Bei den &lt;strong&gt;Linkage-Methoden&lt;/strong&gt; wird in jedem Schritt nach einer bestimmten Logik geprüft, welche Cluster den geringsten Abstand zueinander aufweisen. Diese Cluster werden dann zu einem neuen Cluster fusioniert. Je nach Linkage-Methode wird diese Distanz zwischen den Clustern unterschiedlich bestimmt &lt;a href=&#34;https://www.methodenberatung.uzh.ch/de/datenanalyse_spss/interdependenz/gruppierung/cluster.html&#34;&gt;(Universität Zürich, 2018)&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nächstgelegener Nachbar (engl. “&lt;em&gt;single linkage&lt;/em&gt;”): Das Minimum aller möglichen Distanzen zwischen den Datenpunkten in Cluster 1 und jenen in Cluster 2 wird betrachtet:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;SingleLinkage.svg&#34; alt=&#34;Single Linkage (Quelle: [Sigbert,  2011](https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#/media/Datei:SingleLinkage.svg))&#34; width=&#34;40%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3.2: Single Linkage (Quelle: &lt;a href=&#34;https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#/media/Datei:SingleLinkage.svg&#34;&gt;Sigbert, 2011&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Entferntester Nachbar (engl. “&lt;em&gt;complete linkage&lt;/em&gt;”): Das Maximum aller möglichen Distanzen zwischen den Datenpunkten in Cluster 1 und jenen in Cluster 2 wird betrachtet:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;CompleteLinkage.svg&#34; alt=&#34;Complete Linkage (Quelle: [Sigbert, 2011](https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#/media/Datei:CompleteLinkage.svg))&#34; width=&#34;40%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3.3: Complete Linkage (Quelle: &lt;a href=&#34;https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#/media/Datei:CompleteLinkage.svg&#34;&gt;Sigbert, 2011&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Linkage zwischen Gruppen (engl. “&lt;em&gt;average linkage&lt;/em&gt;”): Der Mittelwert aller möglichen Distanzen zwischen den Datenpunkten in Cluster 1 und jenen in Cluster 2 wird betrachtet.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-6&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;AverageLinkage.svg&#34; alt=&#34;Average Linkage (Quelle: [Sigbert , 2011](https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#/media/Datei:AverageLinkage.svg))&#34; width=&#34;40%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3.4: Average Linkage (Quelle: &lt;a href=&#34;https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#/media/Datei:AverageLinkage.svg&#34;&gt;Sigbert , 2011&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Other Linkage: Dies umfasst verschiedene Methoden. Beispielsweise wird die Distanz zwischen dem Median von Cluster 1 und dem Median von Cluster 2 betrachtet (&lt;em&gt;Median-Clustering&lt;/em&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-7&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;MedianLinkage.svg&#34; alt=&#34;Median Linkage (Quelle: [Sigbert,  2011](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/MedianLinkage.svg/300px-MedianLinkage.svg.png))&#34; width=&#34;40%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3.5: Median Linkage (Quelle: &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/MedianLinkage.svg/300px-MedianLinkage.svg.png&#34;&gt;Sigbert, 2011&lt;/a&gt;)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Neben den Linkage-Methoden exisiteren noch weitere Methoden. Die &lt;strong&gt;Ward-Methode&lt;/strong&gt; ist eine beliebte &lt;strong&gt;Varianz-basierte-Methode&lt;/strong&gt;. Dabei werden die Cluster, die den kleinsten Zuwachs der totalen Varianz aufweisen, fusioniert. Die Methode ist daher eine Erweiterung der empirischen Varianz einer Variablen auf den multivariaten Fall.&lt;/p&gt;
&lt;p&gt;Formel der empirischen Varianz:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[s^2 = \frac{1}{n} \sum_{i=1}^{n}(x_i - \bar{x})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Formel der totalen Varianz (Streuung eines multivariaten Datensatzes mit &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; Variablen &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[T = \frac{1}{n}\sum_{j=1}^{p}  \sum_{i=1}^{n}(x_{ij} - \bar{x_j})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In den Formeln wird ersichtlich, dass &lt;span class=&#34;math inline&#34;&gt;\((x_i-\bar{x})^2\)&lt;/span&gt; mit der bereits bekannten quadrierten euklidischen Distanz &lt;span class=&#34;math inline&#34;&gt;\(d^2(x_i,\bar{x})\)&lt;/span&gt; übereinstimmt. Es wird also für jedes Cluster die Summe der quadrierten Distanzen der Einzelfälle vom jeweiligen Cluster-Mittelwert berechnet. Diese Werte werden dann über alle Variablen &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; aufsummiert. Im nächsten Schritt werden jeweils jene zwei Cluster fusioniert, deren Zusammenfügen die geringste Erhöhung der Gesamtsumme der quadrierten Distanzen zur Folge hat.&lt;/p&gt;
&lt;p&gt;In dieser Abbildung sind die Ergebnisse der verschiedenen Clustering-Algorithmen für unterschiedliche Datensätze exemplarisch dargestellt (Quelle: &lt;a href=&#34;https://scikit-learn.org/stable/modules/clustering.html&#34;&gt;scikit-learn&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;linkage_comparison.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bei den agglomerativen Verfahren führt das single linkage Verfahren in einigen Fällen zu einer sehr einseitigen Verteilung der Cluster. Die Ward Methode führt dagegen in den meisten Fällen zu einer relativ ausgeglichenen Aufteilung. Im folgenden Beispiel wird ebenfalls die Ward-Methode genutzt.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementierung-in-r&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Implementierung in R&lt;/h1&gt;
&lt;p&gt;Für die Durchführung der hierarchischen Clusteranalyse mit der Ward-Methode nutzen wir die Daten des World Happiness Reports aus dem Jahr 2020. Der World Happiness Report ist ein jährlich vom Sustainable Development Solutions Network der Vereinten Nationen veröffentlichter Bericht. Der Bericht enthält Ranglisten zur Lebenszufriedenheit in verschiedenen Ländern der Welt und Datenanalysen aus verschiedenen Perspektiven (siehe &lt;a href=&#34;https://worldhappiness.report&#34;&gt;Helliwell et al., 2020&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Import der Daten:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

df &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/kirenz/datasets/master/whr_20.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In dieser Analyse nutzen wir die landesspezifischen Informationen zu der Lebenserwartung in Jahren (&lt;code&gt;healthy_life_expectancy&lt;/code&gt;) und das logarithmierte Bruttoinlandsprodukt pro Einwohner (&lt;code&gt;logged_gdp_per_capita&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;% 
  ggplot(aes(logged_gdp_per_capita, 
             healthy_life_expectancy, 
             label = country_name )) +
  geom_point() +
  geom_text(check_overlap = TRUE,
            vjust = 0, nudge_y = 0.5) +
  theme_classic() +
  ylab(&amp;quot;Lebenserwartung&amp;quot;) +
  xlab(&amp;quot;Bruttoinlandsprodukt pro Einwohner (logarithmiert)&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Damit die Vorgehensweise des hierarchischen Clustering-Algorithmus besser nachvollzogen werden kann, ziehen wir zufällig 20 Länder aus dem Datensatz:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)

df &amp;lt;- df %&amp;gt;% 
  sample_n(20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Darstellung der Länder in einem Punktediagramm:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;% 
  ggplot(aes(logged_gdp_per_capita, 
             healthy_life_expectancy, 
             label = country_name )) +
  geom_point() +
  geom_text(size = 3,
            check_overlap = FALSE,
            vjust = 0, nudge_y = 0.5) +
  theme_classic() +
  ylab(&amp;quot;Lebenserwartung&amp;quot;) +
  xlab(&amp;quot;Bruttoinlandsprodukt pro Einwohner (logarithmiert)&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;datenvorbereitung&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Datenvorbereitung&lt;/h2&gt;
&lt;div id=&#34;variablenauswahl&#34; class=&#34;section level3&#34; number=&#34;4.1.1&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1.1&lt;/span&gt; Variablenauswahl&lt;/h3&gt;
&lt;p&gt;Wir erzeugen einen neuen Datensatz &lt;code&gt;df_cl&lt;/code&gt;, in welchem nur die Variablen enthalten sind, die für die Clusteranalyse genutzt werden sollen. Zusätzlich nutzen wir die Variable &lt;code&gt;country_name&lt;/code&gt;, um in einem späteren Schritt die Daten sinnvoll beschriften zu können.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_cl &amp;lt;- select(df, c(&amp;quot;country_name&amp;quot;, 
                      &amp;quot;logged_gdp_per_capita&amp;quot;, 
                      &amp;quot;healthy_life_expectancy&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fehlende-werte&#34; class=&#34;section level3&#34; number=&#34;4.1.2&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1.2&lt;/span&gt; Fehlende Werte&lt;/h3&gt;
&lt;p&gt;Wir prüfen, ob in den Daten fehlende Werte vorliegen:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(is.na(df_cl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In diesem Datensatz liegen keine fehlenden Werte vor. Falls dies in einem anderen Projekt jedoch der Fall sein sollte, könnten wir diese fehlenden Werte mit dem Befehl &lt;code&gt;drop_na()&lt;/code&gt; entfernen:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_cl &amp;lt;- drop_na(df_cl)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;standardisierung&#34; class=&#34;section level3&#34; number=&#34;4.1.3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1.3&lt;/span&gt; Standardisierung&lt;/h3&gt;
&lt;p&gt;Damit die Werte der Variablen in einem einheitlichen Werteintervall vorliegen, nutzen wir für die Standardisierung der Daten die z-Transformation. Mit Hilfe dieser Standardisierung wird der Mittelwert auf 0 und die Standardabweichung der Variablen auf 1 gesetzt. Die Formel dafür lautet:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[z = \frac{x - \bar{x}}{s}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar{x}\)&lt;/span&gt;: Mittelwert der Daten&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;: Standardabweichung der Daten&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Wir führen die Standardisierung mit Hilfe des Befehls &lt;code&gt;scale()&lt;/code&gt; durch und speichern die neuen Variablen in dem Datensatz ab.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_cl$healthy_life_expectancy_sc &amp;lt;-  scale(df_cl$healthy_life_expectancy, 
                                           center = TRUE, 
                                           scale = TRUE)

df_cl$logged_gdp_per_capita_sc &amp;lt;-  scale(df_cl$logged_gdp_per_capita, 
                                         center = TRUE, 
                                         scale = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wie in der Abbildung nachvollzogen werden kann, ändert sich nicht die Position der Länder, sondern lediglich die Einheiten auf der X- und Y-Achse:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_cl %&amp;gt;% 
  ggplot(aes(logged_gdp_per_capita_sc, 
             healthy_life_expectancy_sc, 
             label = country_name)) +
  geom_point() +
  geom_text(size = 3,
            check_overlap = FALSE,
            vjust = 0, nudge_y = 0.1) +
  theme_classic() +
  ylab(&amp;quot;Lebenserwartung (z-Werte)&amp;quot;) +
  xlab(&amp;quot;Bruttoinlandsprodukt pro Einwohner (z-Werte)&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;proximitätsmaß-1&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Proximitätsmaß&lt;/h2&gt;
&lt;p&gt;Wir nutzen als Proximitätsmaß die euklidische Distanz und speichern das Ergebnis der Funktion &lt;code&gt;dist()&lt;/code&gt;, welche die Distanz zwischen allen Ländern berechnet, mit der Bezeichnung &lt;code&gt;d&lt;/code&gt; ab. Da wir die Variable &lt;code&gt;country_name&lt;/code&gt; nicht mit in die Berechnung einbeziehen möchten, entfernen wir diese in dem &lt;code&gt;select()&lt;/code&gt;-Befehl.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d &amp;lt;- 
  df_cl %&amp;gt;% 
  select(-country_name) %&amp;gt;% 
  dist(method = &amp;quot;euclidean&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchische-clusteranalyse&#34; class=&#34;section level2&#34; number=&#34;4.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; Hierarchische Clusteranalyse&lt;/h2&gt;
&lt;p&gt;Im nächsten Schritt wird die hierarchische Clusteranalyse mit dem Befehl &lt;code&gt;hclust()&lt;/code&gt; angewendet. Dafür übergeben wir der Funktion das Datenobjekt &lt;code&gt;d&lt;/code&gt;, welches die euklidischen Distanzen zwischen den Ländern enthält (für weitere Hinweise zu der Funktion, siehe diesen Beitrag auf &lt;a href=&#34;https://stats.stackexchange.com/a/109962&#34;&gt;stackoverflow&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc &amp;lt;- hclust(d, method = &amp;quot;ward.D2&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Zu Beginn der agglomerativen Cluster-Bildung ist jedes Land in einem eigenen Cluster. Am Ende sind alle Länder in einem gemeinsamen Cluster. Die optimale Clusteranzahl wird dabei nicht von dem Algorithmus bestimmt, sondern muss auf Grundlage weiterer Überlegungen ermittelt werden. Bei der Bestimmung der optimalen Clusteranzahl ist die sogenannte “Cophenetic Distance” und das “Dendogramm” hilfreich.&lt;/p&gt;
&lt;p&gt;Zu Beginn der agglomerativen Clusterbildung werden diejenigen Länder fusioniert, welche die geringste Distanz zueinander aufweisen. Diese “geringste Distanz” zwischen zwei Clustern, bei welcher die Zusammeführung stattfindet, kann mit der “Cophenetic Distance” bestimmt werden:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sort(unique(cophenetic(hc)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  0.4446962  0.4991792  0.6219964  0.8162091  0.9668424  1.2776699
##  [7]  1.5519296  1.8267467  2.2893469  2.3881172  2.7432910  3.0358361
## [13]  3.5850849  4.3343418  4.7705415  4.9156397 14.3947213 14.9659808
## [19] 41.2573679&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Die geringste Distanz zwischen zwei Clustern beträgt zu Beginn (wenn jedes Land sein eigenes Cluster darstellt) 0.44. Dies war also der geringste Abstand zwischen zwei Ländern. Danach steigt der Abstand monoton steigend an, da immer unähnlichere Cluster (d.h. mit einem größeren Abstand zueinander) fusioniert werden. Bei der letzten Zusammenführung der Cluster in ein einziges gemeinsames Cluster nimmt die Distanz den Maximalwert von 41 an. Damit die Werte leichter interpretierbar sind, wird der Prozess üblicherweise in einem sogenannten Dendrogramm dargestellt.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dendrogramm&#34; class=&#34;section level2&#34; number=&#34;4.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.4&lt;/span&gt; Dendrogramm&lt;/h2&gt;
&lt;p&gt;Mit Hilfe des Dendrogramms kann das Ergebnis des Clustering-Algorithmus dargestellt werden. Das Dendrogramm liest sich dabei von unten nach oben und beschreibt in diese Richtung den Prozess des Clusterings. Die vertikale Achse beschreibt die Heterogenität der Cluster mit der bereits erwähnten “Cophenetic Distance” (die in der Abbildung als &lt;code&gt;Height&lt;/code&gt; bezeichnet wird). Auf der unteren Seite des Dendrogramms sind alle Fälle einzeln aufgelistet. Zunächst entspricht also jedes Land einem Cluster, was sich daran zeigt, dass jeder Fall eine eigene horizontale Linie aufweist. Diese Cluster werden von unten nach oben sukzessive zu größeren Clustern zusammengefügt. Die vertikalen Linien zeigen an, dass zwei Cluster fusioniert werden.&lt;/p&gt;
&lt;p&gt;Darstellung des Dendrogramms:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hc) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nutzung der Ländernamen als Labels in dem Dendrogramm:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc$labels &amp;lt;- df$country_name

plot(hc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Die “optimale” Anzahl der Cluster sollte insbesondere anhand inhalticher Interpretationen in Hinblick einer größtmöglichen Plausibilität der gebildeten Cluster geschehen. Zusätzlich kann der größte (bzw. ein großer) Zuwachs der Heterogenität in dem Dendrogramm als Entscheidungskriterium genutzt werden. Bei unseren Daten entsteht der größte Heterogenitätszuwachs zwischen einer 2-Cluster und 1-Cluster-Lösung. Der Heterogenitätszuwachs zwischen einer 4-Cluster und 2-Cluster-Lösung ist ebenfalls relativ groß. Wir entscheiden uns hier für eine Clusteranzahl von 4, hätten jedoch auch die 2-Cluster-Lösung wählen können. Wie bereits erwähnt existiert bei diesem Verfahren oftmals keine eindeutige “optimale” Lösung, da jeweils auch die Interpretiertbarkeit der Cluster auf Grundlage inhaltlicher Überlegungen eine wichtige Rolle spielt.&lt;/p&gt;
&lt;p&gt;Darstellung des Dendrogramms mit roten Grenzen bei einer Größe von 4 Clustern:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc$labels &amp;lt;- df$country_name

plot(hc)

rect.hclust(hc, k = 4, border = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ermittlung der Gruppenzugehörigkeit (Cluster 1 bis Cluster 4) der jeweiligen Länder bei einer Clustergröße von k = 4. Dafür nutzen wir die Funktion &lt;code&gt;cutree()&lt;/code&gt;, die einen “Schnitt” bei der entsprechenden Clustergröße vornimmt und die Daten in die entsprechenden Gruppen (Nummer des Clusters) einteilt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gruppen &amp;lt;- cutree(hc, k = 4) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hinzufügung der Nummer des Clusters zu dem Datensatz:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_cl$cluster &amp;lt;- gruppen&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Darstellung der Cluster in einem Punktediagramm:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_cl %&amp;gt;% 
  ggplot(aes(logged_gdp_per_capita, 
             healthy_life_expectancy, 
             label = country_name, 
             color = factor(cluster))) +
  geom_point() +
  geom_text(size = 3,
            check_overlap = FALSE,
            vjust = 0, nudge_y = 0.5,
            show.legend = FALSE) +
  theme_classic() +
  ylab(&amp;quot;Lebenserwartung&amp;quot;) +
  xlab(&amp;quot;Bruttoinlandsprodukt pro Einwohner (logarithmiert)&amp;quot;) +
  theme(legend.title=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Zum Vergleich, hier noch die Aufteilung der Daten bei einer Wahl von 2 Clustern:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hc)

rect.hclust(hc, k = 2, border = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gruppen_2 &amp;lt;- cutree(hc, k = 2) 

df_cl$cluster_2 &amp;lt;- gruppen_2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Darstellung der 2-Cluster-Lösung in einem Punktediagramm:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_cl %&amp;gt;% 
  ggplot(aes(logged_gdp_per_capita, 
             healthy_life_expectancy, 
             label = country_name, 
             color = factor(cluster_2))) +
  geom_point() +
  geom_text(size = 3,
            check_overlap = FALSE,
            vjust = 0, nudge_y = 0.5,
            show.legend = FALSE) +
  theme_classic() +
  ylab(&amp;quot;Lebenserwartung&amp;quot;) +
  xlab(&amp;quot;Bruttoinlandsprodukt pro Einwohner (logarithmiert)&amp;quot;) +
  theme(legend.title=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Association Rule Mining in R</title>
      <link>https://kirenz.com/post/2020-05-14-r-association-rule-mining/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/post/2020-05-14-r-association-rule-mining/</guid>
      <description>
&lt;script src=&#34;https://kirenz.com/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://kirenz.com/rmarkdown-libs/plotly-binding/plotly.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://kirenz.com/rmarkdown-libs/typedarray/typedarray.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://kirenz.com/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://kirenz.com/rmarkdown-libs/crosstalk/css/crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://kirenz.com/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://kirenz.com/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://kirenz.com/rmarkdown-libs/plotly-main/plotly-latest.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://kirenz.com/rmarkdown-libs/vis/vis.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://kirenz.com/rmarkdown-libs/vis/vis.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://kirenz.com/rmarkdown-libs/visNetwork-binding/visNetwork.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-market-basket-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1&lt;/span&gt; The market-basket model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#association-rules&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2&lt;/span&gt; Association rules&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#association-measures&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Association measures&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#support&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#confidence&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Confidence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lift&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.3&lt;/span&gt; Lift&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-priori-algorithm&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; A-Priori Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-in-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Implementation in R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#transform-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1&lt;/span&gt; Transform data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inspect-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2&lt;/span&gt; Inspect data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-priori-algorithm-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3&lt;/span&gt; A-Priori Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#set-lhs-and-rhs&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.4&lt;/span&gt; Set LHS and RHS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visualizing-association-rules&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.5&lt;/span&gt; Visualizing association rules&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scatter-plot&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.6&lt;/span&gt; Scatter-Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interactive-scatter-plot&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.7&lt;/span&gt; Interactive scatter-plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#graph-based-visualization&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.8&lt;/span&gt; Graph-based visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parallel-coordinate-plot&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.9&lt;/span&gt; Parallel coordinate plot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Association rule mining is one of the most popular data mining methods. This kind of analysis is also called &lt;em&gt;frequent itemset analysis&lt;/em&gt;, &lt;em&gt;association analysis&lt;/em&gt; or &lt;em&gt;association rule learning&lt;/em&gt;. To perform the analysis in R, we use the &lt;code&gt;arules&lt;/code&gt; and &lt;code&gt;arulesViz&lt;/code&gt; packages.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;In association analysis, we are usually interested in the absolute number of customer transactions (also called baskets) that contain a particular set of items (usually products). A typical application of association analysis is the analysis of consumer buying behavior in supermarkets and chain stores where they record the contents of shopping carts brought to the register for checkout. These transaction data are normally recorded by point-of-sale scanners and often consist of &lt;a href=&#34;https://en.wikipedia.org/wiki/Tuple&#34;&gt;tuples&lt;/a&gt; of the form: &lt;code&gt;{transaction ID, item ID, item ID, ...}&lt;/code&gt;. By finding frequent itemsets, a retailer can learn what is commonly bought together and use this information to increase sales in several ways.&lt;/p&gt;
&lt;p&gt;Imagine there is a pair of different products (which we call &lt;em&gt;items&lt;/em&gt;), &lt;em&gt;X&lt;/em&gt; and &lt;em&gt;Y&lt;/em&gt;, that are frequently bought together in a store (Ng &amp;amp; Soo, 2017):&lt;/p&gt;
&lt;style&gt;
div.blue { background-color:#e6f0ff; border-radius: 2px; padding: 10px;}
&lt;/style&gt;
&lt;div class=&#34;blue&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Both X and Y can be placed on the same shelf, so that buyers of one item would be prompted to buy the other.&lt;/li&gt;
&lt;li&gt;Promotional discounts could be applied to just one out of the two items.&lt;/li&gt;
&lt;li&gt;Advertisements on X could be targeted at buyers who purchase Y.&lt;/li&gt;
&lt;li&gt;X and Y could be combined into a new product, such as having Y in flavors of X.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Note that online retailers like Amazon.com or online platforms like Spotify have little need for this kind of analysis, since it is designed to search for itemsets that appear frequently. If the online retailer was limited to frequent itemsets, they would miss all the opportunities that are present in the “long tail” to select advertisements for each customer individually (for example to recommend certain products or songs). Instead of searching for &lt;em&gt;frequent&lt;/em&gt; itemsets, they use &lt;em&gt;similarity&lt;/em&gt; search algorithms (like &lt;em&gt;collaborative filtering&lt;/em&gt;) to detect similar customers that have a large fraction of their baskets in common, even if the absolute number of baskets is small. (Leskovec, Rajaraman, &amp;amp; Ullman, 2020)&lt;/p&gt;
&lt;div id=&#34;the-market-basket-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; The market-basket model&lt;/h2&gt;
&lt;p&gt;Association rule mining is based on the so called “market-basket” model of data. This is essentially a many-many relationship between two kinds of elements, called &lt;strong&gt;items&lt;/strong&gt; and &lt;strong&gt;baskets&lt;/strong&gt; (also called &lt;strong&gt;transactions&lt;/strong&gt;) with some assumptions about the shape of the data (Leskovec, Rajaraman, &amp;amp; Ullman, 2020):&lt;/p&gt;
&lt;div class=&#34;blue&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Each basket (i.e. transaction) consists of a set of items (usually products).&lt;/li&gt;
&lt;li&gt;Usually we assume that the number of items in a basket is small (much smaller than the total number of all items).&lt;/li&gt;
&lt;li&gt;The number of all baskets (transactions) is usually assumed to be very large.&lt;/li&gt;
&lt;li&gt;The data is assumed to be represented in a file consisting of a sequence of baskets (transactions).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;To illustrate the logic of association rule mining, let’s create a sequence of baskets (transactions) with a small number of items from different customers in a grocery store. Note that because we use a very simple example with only a few baskets and items, the results of the analysis will differ from the results we may obtain from a real world example. We save the data as a sequence of transactions with the name &lt;code&gt;market_basket&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a list of baskets
market_basket &amp;lt;-  
  list(  
  c(&amp;quot;apple&amp;quot;, &amp;quot;beer&amp;quot;, &amp;quot;rice&amp;quot;, &amp;quot;meat&amp;quot;),
  c(&amp;quot;apple&amp;quot;, &amp;quot;beer&amp;quot;, &amp;quot;rice&amp;quot;),
  c(&amp;quot;apple&amp;quot;, &amp;quot;beer&amp;quot;), 
  c(&amp;quot;apple&amp;quot;, &amp;quot;pear&amp;quot;),
  c(&amp;quot;milk&amp;quot;, &amp;quot;beer&amp;quot;, &amp;quot;rice&amp;quot;, &amp;quot;meat&amp;quot;), 
  c(&amp;quot;milk&amp;quot;, &amp;quot;beer&amp;quot;, &amp;quot;rice&amp;quot;), 
  c(&amp;quot;milk&amp;quot;, &amp;quot;beer&amp;quot;),
  c(&amp;quot;milk&amp;quot;, &amp;quot;pear&amp;quot;)
  )

# set transaction names (T1 to T8)
names(market_basket) &amp;lt;- paste(&amp;quot;T&amp;quot;, c(1:8), sep = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each basket includes so called &lt;strong&gt;itemsets&lt;/strong&gt; (like {apple, beer, etc.}). You can observe that “apple” is bought together with “beer” in three transactions:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://annalyzin.files.wordpress.com/2016/04/association-rule-support-table.png?w=652&amp;h=578&#34; alt=&#34;Market basket example (Ng &amp;amp; Soo, 2017)&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Market basket example (Ng &amp;amp; Soo, 2017)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The frequent-itemsets problem is that of finding sets of items that appear in many of the baskets. Hence, a set of items that appears in many baskets is said to be “frequent”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;association-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Association rules&lt;/h2&gt;
&lt;p&gt;While we are interested in extracting frequent sets of items, this information is often presented as a collection of &lt;em&gt;if–then rules&lt;/em&gt;, called &lt;strong&gt;association rules&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The form of an association rule is &lt;code&gt;{X -&amp;gt; Y}&lt;/code&gt;, where &lt;code&gt;{X}&lt;/code&gt; is a set of items and &lt;code&gt;{Y}&lt;/code&gt; is an item. The implication of this association rule is that if all of the items in &lt;code&gt;{X}&lt;/code&gt; appear in some basket, then &lt;code&gt;{Y}&lt;/code&gt; is “likely” to appear in that basket as well.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;{X} is also called &lt;strong&gt;antecedent&lt;/strong&gt; or &lt;strong&gt;left-hand-side (LHS)&lt;/strong&gt; and&lt;/li&gt;
&lt;li&gt;{Y} is called &lt;strong&gt;consequent&lt;/strong&gt; or &lt;strong&gt;right-hand-side (RHS)&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An example association rule for products from Apple could be &lt;code&gt;{Apple iPad, Apple iPad Cover} -&amp;gt; {Apple Pencil}&lt;/code&gt;, meaning that if Apple’s iPad and iPad Cover &lt;code&gt;{X}&lt;/code&gt; are bought, customers are also likely to buy Apple’s Pencil &lt;code&gt;{Y}&lt;/code&gt;. Notice that the logical implication symbol “-&amp;gt;” does not indicate a causal relationship between {X} and {Y}. It is merely an estimate of the conditional probability of {Y} given {X}.&lt;/p&gt;
&lt;p&gt;Now imagine a grocery store with tens of thousands of different products. We wouldn’t want to calculate all associations between every possible combination of products. Instead, we would want to select only potentially “relevant” rules from the set of all possible rules. Therefore, we use the measures &lt;strong&gt;support&lt;/strong&gt;, &lt;strong&gt;confidence&lt;/strong&gt; and &lt;strong&gt;lift&lt;/strong&gt; to reduce the number of relationships we need to analyze:&lt;/p&gt;
&lt;div class=&#34;blue&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Support is an indication of how frequently a set of items appear in baskets.&lt;/li&gt;
&lt;li&gt;Confidence is an indication of how often the support-rule has been found to be true.&lt;/li&gt;
&lt;li&gt;Lift is a measure of association using both support and confidence.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;If we are looking for association rules {X -&amp;gt; Y} that apply to a reasonable fraction of the baskets, then the support of X must be reasonably high. In practice, such as for marketing in brick-and-mortar stores, “reasonably high” is often around 1% to 10% of the baskets. We also want the conﬁdence of the rule to be reasonably high, perhaps 50%, or else the rule has little practical effect. (Leskovec, Rajaraman, &amp;amp; Ullman, 2020)&lt;/p&gt;
&lt;p&gt;Furthermore, it must be assumed that there are not too many frequent itemsets and thus not too many candidates for high-support, high-conﬁdence association rules. The reason for this is that if we give companies to many association rules that meet our thresholds for support and conﬁdence, they cannot even read them, let alone act on them. Thus, it is normal to adjust the support and confidence thresholds so that we do not get too many frequent itemsets. (Leskovec, Rajaraman, &amp;amp; Ullman, 2020)&lt;/p&gt;
&lt;p&gt;Next, we take a closer look at the measures support, confidence and lift.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;association-measures&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Association measures&lt;/h1&gt;
&lt;div id=&#34;support&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Support&lt;/h2&gt;
&lt;p&gt;The metric support tells us how popular a set of items is, as measured by the proportion of transactions in which the itemset appears.&lt;/p&gt;
&lt;p&gt;In our data, the support of {apple} is 4 out of 8, or 50%. The support of {apple, beer, rice} is 2 out of 8, or 25%.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Support(apple) = \frac{4}{8} = 0.5\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Or in general, for a set of items X:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Support(X) = \frac{frequency(X)}{n} \]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;with n = number of all transactions (baskets).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Usually, a specific support-threshold is used to reduce the number of itemsets we need to analyze. At the beginning of the analysis, we could set our support-threshold to 10%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Confidence&lt;/h2&gt;
&lt;p&gt;Confidence tells us how likely an item Y is purchased given that item X is purchased, expressed as {X -&amp;gt; Y}. It is measured by the proportion of transactions with item X, in which item Y also appears. The confidence of a rule is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Confidence(X -&amp;gt; Y) = \frac{support(X \cup Y)}{support(X)} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence, the confidence can be interpreted as an estimate of the probability &lt;code&gt;P(Y|X)&lt;/code&gt;. In other words, this is the probability of finding the RHS (Y) of the rule in transactions under the condition that these transactions also contain the LHS (X) (Hornik, Grün, &amp;amp; Hahsler, 2005). Confidence is directed and therefore usually gives different values for the rules &lt;code&gt;X -&amp;gt; Y&lt;/code&gt; and &lt;code&gt;Y -&amp;gt; X&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(support(X ∪ Y)\)&lt;/span&gt; means the support of the union of the items in X and Y. Since we usually state probabilities of events and not sets of items, we can rewrite &lt;span class=&#34;math inline&#34;&gt;\(support(X \cup Y)\)&lt;/span&gt; as the probability &lt;span class=&#34;math inline&#34;&gt;\(P(E_X \cap E_Y)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(E_{X}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(E_{Y}\)&lt;/span&gt; are the events that a transaction contains itemset X and Y, respectively (review &lt;a href=&#34;https://michael.hahsler.net/research/association_rules/measures.html&#34;&gt;this site&lt;/a&gt; from Michael Hahsler for a detailed explanation).&lt;/p&gt;
&lt;p&gt;In our example, the confidence that beer is purchased given that apple is purchased ({apple -&amp;gt; beer}) is 3 out of 4, or 75%. This means the conditional probability P(beer|apple) = 75%. Apple is the antecedent or left-hand-side (LHS) and beer is the consequent or right-hand-side (RHS).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Confidence(apple -&amp;gt; beer ) = \frac{support(apple ∪ beer)}{support(apple)} = \frac{\frac{3}{8}{}{}}{\frac{4}{8}{}} = \frac{3}{4} = 0.75\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that the confidence measure might misrepresent the importance of an association. This is because it only accounts for how popular item X is (in our case apple) but not Y (in our case beer).&lt;/p&gt;
&lt;p&gt;If beer is also very popular in general, there will be a higher chance that a transaction containing apple will also contain beer, thus inflating the confidence measure. To account for the base popularity of both items, we use a third measure called lift.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lift&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Lift&lt;/h2&gt;
&lt;p&gt;Lift tells us how likely item Y is purchased when item X is purchased, while controlling for how popular items Y and X are. It measures how many times more often X and Y occur together than expected if they were statistically independent.&lt;/p&gt;
&lt;p&gt;In our example, lift is calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Lift(apple -&amp;gt; beer ) = \frac{support(apple ∪ beer)}{support(apple) \times support(beer)}  = \frac{\frac{3}{8}{}{}}{\frac{4}{8}{\times \frac{6}{8}}} = \frac{\frac{3}{8}{}{}}{\frac{24}{64}} = \frac{\frac{3}{8}{}{}}{\frac{3}{8}} = 1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A lift value of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;lift = 1&lt;/strong&gt;: implies no association between items.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;lift &amp;gt; 1&lt;/strong&gt;: greater than 1 means that item Y is likely to be bought if item X is bought,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;lift &amp;lt; 1&lt;/strong&gt;: less than 1 means that item Y is unlikely to be bought if item X is bought.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The lift of &lt;code&gt;{apple -&amp;gt; beer}&lt;/code&gt; is 1, which implies no association between the two items.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-priori-algorithm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; A-Priori Algorithm&lt;/h1&gt;
&lt;p&gt;There are different algorithms for finding frequent item-sets. In this tutorial we cover the main idea behind the &lt;code&gt;A-Priori Algorithm&lt;/code&gt;, which reduces the number of itemsets we need to examine. It works by eliminating itemsets by looking ﬁrst at smaller sets and recognizing that a large set cannot be frequent unless all its subsets are. Put simply, the algorithm states that if an itemset is infrequent, then all its subsets must also be infrequent.&lt;/p&gt;
&lt;p&gt;This means that if item {beer} was found to be infrequent, we can expect the itemset {beer, pizza} to be equally or even more infrequent. So in consolidating the list of popular itemsets, we need not consider {beer, pizza}, nor any other itemset configuration that contains {beer}.&lt;/p&gt;
&lt;p&gt;The A-Priori Algorithm uses a so called &lt;em&gt;breadth-first&lt;/em&gt; search strategy, which can be viewed in this decision tree:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/4/46/Animated_BFS.gif&#34; alt=&#34;Example of breadth-first search (source: [Matheny, 2007)](https://en.wikipedia.org/wiki/Breadth-first_search#/media/File:Animated_BFS.gif)&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3.1: Example of breadth-first search (source: &lt;a href=&#34;https://en.wikipedia.org/wiki/Breadth-first_search#/media/File:Animated_BFS.gif&#34;&gt;Matheny, 2007)&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Using this principle, the number of itemsets that have to be examined can be &lt;em&gt;pruned&lt;/em&gt; (i.e. removing sections of the decision tree).&lt;/p&gt;
&lt;p&gt;The list of popular itemsets can be obtained in these steps (Ng &amp;amp; Soo, 2017):&lt;/p&gt;
&lt;div class=&#34;blue&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step 0. Start with itemsets containing just a single item, such as {apple} and {pear}.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 1. Determine the support-threshold for itemsets. Keep the itemsets that meet your minimum support threshold, and remove itemsets that do not.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 2. Using the itemsets you have kept from Step 1, generate all the possible itemset configurations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Step 3. Repeat Steps 1 &amp;amp; 2 until there are no more new itemsets.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;This iterative process is illustrated in the animation below:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://annalyzin.files.wordpress.com/2016/04/association-rules-apriori-tutorial-explanation.gif&#34; alt=&#34;A-Priori Algorithm (Ng &amp;amp; Soo, 2017)&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3.2: A-Priori Algorithm (Ng &amp;amp; Soo, 2017)
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As seen in the animation, {apple} was determine to have low support, hence it was removed and all other itemset configurations that contain apple need not be considered. This reduced the number of itemsets to consider by more than half.&lt;/p&gt;
&lt;p&gt;Note that the &lt;strong&gt;support threshold&lt;/strong&gt; that you pick in Step 1 could be based on a formal analysis or past experience. If you discover that sales of items beyond a certain proportion tend to have a significant impact on your profits, you might consider using that proportion as your support threshold (otherwise you may use 1%-10% as a starting value).&lt;/p&gt;
&lt;p&gt;We have seen how the A-Priori Algorithm can be used to identify itemsets with high support. The same principle can also be used to identify item associations with high &lt;strong&gt;confidence&lt;/strong&gt; or &lt;strong&gt;lift&lt;/strong&gt;. Finding rules with high confidence or lift is less computationally taxing once high-support itemsets have been identified, because confidence and lift values are calculated using support values (Ng &amp;amp; Soo, 2017).&lt;/p&gt;
&lt;p&gt;Take for example the task of finding high-confidence rules. If the rule&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;{beer, chips -&amp;gt; apple}&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;has low confidence, all other rules with the same left hand side (LHS) items and with apple on the right hand side (RHS) would have low confidence too. Specifically, the rules&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;{beer -&amp;gt; apple, chips}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{chips -&amp;gt; apple, beer}&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;would have low confidence as well. As before, lower level candidate item rules can be pruned using the A-Priori Algorithm, so that fewer candidate rules need to be examined (Ng &amp;amp; Soo, 2017).&lt;/p&gt;
&lt;p&gt;In summary, when you apply the A-Priori Algorithm on a given set of transactions, your goal will be to find all rules with support greater than or equal to your support threshold and confidence greater than or equal to your confidence threshold.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Implementation in R&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;arules&amp;quot;)
install.packages(&amp;quot;arulesViz&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To perform the association analysis in R, we use the &lt;code&gt;arules&lt;/code&gt; and &lt;code&gt;arulesViz&lt;/code&gt; packages. Review Hornik et al. (2005) for a detailed description of the packages or visit the &lt;a href=&#34;http://mhahsler.github.io/arules/&#34;&gt;arules documentation site&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;transform-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Transform data&lt;/h2&gt;
&lt;p&gt;First of all, you have to load the transaction data into an object of the “transaction class” to be able to analyze the data. This is done by using the following function of the &lt;code&gt;arules&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(arules)

trans &amp;lt;- as(market_basket, &amp;quot;transactions&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;inspect-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Inspect data&lt;/h2&gt;
&lt;p&gt;Take a look at the dimensions of this object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means we have 8 transactions and 6 distinct items.&lt;/p&gt;
&lt;p&gt;Obtain a list of the distinct items in the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;itemLabels(trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;apple&amp;quot; &amp;quot;beer&amp;quot;  &amp;quot;meat&amp;quot;  &amp;quot;milk&amp;quot;  &amp;quot;pear&amp;quot;  &amp;quot;rice&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;View the summary of the transaction data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(trans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## transactions as itemMatrix in sparse format with
##  8 rows (elements/itemsets/transactions) and
##  6 columns (items) and a density of 0.4583333 
## 
## most frequent items:
##    beer   apple    milk    rice    meat (Other) 
##       6       4       4       4       2       2 
## 
## element (itemset/transaction) length distribution:
## sizes
## 2 3 4 
## 4 2 2 
## 
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    2.00    2.00    2.50    2.75    3.25    4.00 
## 
## includes extended item information - examples:
##   labels
## 1  apple
## 2   beer
## 3   meat
## 
## includes extended transaction information - examples:
##   transactionID
## 1            T1
## 2            T2
## 3            T3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;summary()&lt;/code&gt; gives us information about our transaction object:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There are 8 transactions (rows) and 6 items (columns) and we can view the most frequent items.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Density tells us the percentage of non-zero cells in this 8x6-matrix.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Element length distribution: a set of 2 items in 4 transactions; 3 items in 2 of the transactions and 4 items in 2 transactions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that a matrix is called a &lt;strong&gt;sparse matrix&lt;/strong&gt; if most of the elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. The number of zero-valued elements divided by the total number of elements is called the &lt;em&gt;sparsity&lt;/em&gt; of the matrix (which is equal to 1 minus the density of the matrix).&lt;/p&gt;
&lt;p&gt;Take a look at all transactions and items in a matrix like fashion:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;image(trans)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2020-05-14-r-association-rule-mining/index_files/figure-html/Matrix%20of%20transactions%20and%20items-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can observe that almost half of the “cells” (45,83 %) are non zero values.&lt;/p&gt;
&lt;p&gt;Display the relative item frequency:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;itemFrequencyPlot(trans, topN=10,  cex.names=1)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-10&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://kirenz.com/post/2020-05-14-r-association-rule-mining/index_files/figure-html/unnamed-chunk-10-1.png&#34; alt=&#34;Relative item frequency&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.1: Relative item frequency
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The items {apple}, {milk} and {rice} all have a relative item frequency (i.e. support) of 50%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-priori-algorithm-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; A-Priori Algorithm&lt;/h2&gt;
&lt;p&gt;The next step is to analyze the rules using the A-Priori Algorithm with the function &lt;code&gt;apriori()&lt;/code&gt;. This function requires both a minimum support and a minimum confidence constraint at the same time. The option &lt;code&gt;parameter&lt;/code&gt; will allow you to set the &lt;em&gt;support-threshold&lt;/em&gt;, &lt;em&gt;confidence-threshold&lt;/em&gt; as well as the maximum lenght of items (&lt;code&gt;maxlen&lt;/code&gt;). If you do not provide threshold values, the function will perform the analysis with these default values: support-threshold of 0.1 and confidence-threshold of 0.8.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Min Support 0.3, confidence as 0.5.
rules &amp;lt;- apriori(trans, 
                 parameter = list(supp=0.3, conf=0.5, 
                                  maxlen=10, 
                                  target= &amp;quot;rules&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Apriori
## 
## Parameter specification:
##  confidence minval smax arem  aval originalSupport maxtime support minlen
##         0.5    0.1    1 none FALSE            TRUE       5     0.3      1
##  maxlen target   ext
##      10  rules FALSE
## 
## Algorithmic control:
##  filter tree heap memopt load sort verbose
##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE
## 
## Absolute minimum support count: 2 
## 
## set item appearances ...[0 item(s)] done [0.00s].
## set transactions ...[6 item(s), 8 transaction(s)] done [0.00s].
## sorting and recoding items ... [4 item(s)] done [0.00s].
## creating transaction tree ... done [0.00s].
## checking subsets of size 1 2 done [0.00s].
## writing ... [10 rule(s)] done [0.00s].
## creating S4 object  ... done [0.00s].&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our simple example, we already know that by using a support-threshold of 0.3, we will eliminate {meat} and {pear} from our analysis, since they have support values below 0.3.&lt;/p&gt;
&lt;p&gt;The summary shows the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(rules)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## set of 10 rules
## 
## rule length distribution (lhs + rhs):sizes
## 1 2 
## 4 6 
## 
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     1.0     1.0     2.0     1.6     2.0     2.0 
## 
## summary of quality measures:
##     support        confidence          lift           count    
##  Min.   :0.375   Min.   :0.5000   Min.   :1.000   Min.   :3.0  
##  1st Qu.:0.375   1st Qu.:0.5000   1st Qu.:1.000   1st Qu.:3.0  
##  Median :0.500   Median :0.5833   Median :1.000   Median :4.0  
##  Mean   :0.475   Mean   :0.6417   Mean   :1.067   Mean   :3.8  
##  3rd Qu.:0.500   3rd Qu.:0.7500   3rd Qu.:1.000   3rd Qu.:4.0  
##  Max.   :0.750   Max.   :1.0000   Max.   :1.333   Max.   :6.0  
## 
## mining info:
##   data ntransactions support confidence
##  trans             8     0.3        0.5&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Set of rules: 10.&lt;/li&gt;
&lt;li&gt;Rule length distribution (LHS + RHS): 4 rules with a length of 1 item; 6 rules with a length of 2 items.&lt;/li&gt;
&lt;li&gt;Summary of quality measures: min, max, median, mean and quantile values for support, confidence and lift.&lt;/li&gt;
&lt;li&gt;Mining info: number of transactions, support-threshold and confidence-threshold.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inspect the 10 rules we obtained:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspect(rules)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      lhs        rhs     support confidence lift     count
## [1]  {}      =&amp;gt; {apple} 0.500   0.5000000  1.000000 4    
## [2]  {}      =&amp;gt; {milk}  0.500   0.5000000  1.000000 4    
## [3]  {}      =&amp;gt; {rice}  0.500   0.5000000  1.000000 4    
## [4]  {}      =&amp;gt; {beer}  0.750   0.7500000  1.000000 6    
## [5]  {apple} =&amp;gt; {beer}  0.375   0.7500000  1.000000 3    
## [6]  {beer}  =&amp;gt; {apple} 0.375   0.5000000  1.000000 3    
## [7]  {milk}  =&amp;gt; {beer}  0.375   0.7500000  1.000000 3    
## [8]  {beer}  =&amp;gt; {milk}  0.375   0.5000000  1.000000 3    
## [9]  {rice}  =&amp;gt; {beer}  0.500   1.0000000  1.333333 4    
## [10] {beer}  =&amp;gt; {rice}  0.500   0.6666667  1.333333 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The rules 1 to 4 with an empty LHS mean that no matter what other items are involved the item in the RHS will appear with the probability given by the rule’s confidence (which equals the support). If you want to avoid these rules then use the argument &lt;code&gt;parameter=list(minlen=2)&lt;/code&gt; (&lt;a href=&#34;https://stackoverflow.com/a/38994066&#34;&gt;stackoverflow&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Min Support 0.3, confidence as 0.5.
rules &amp;lt;- apriori(trans, 
                        parameter = list(supp=0.3, conf=0.5, 
                                         maxlen=10, 
                                         minlen=2,
                                         target= &amp;quot;rules&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Apriori
## 
## Parameter specification:
##  confidence minval smax arem  aval originalSupport maxtime support minlen
##         0.5    0.1    1 none FALSE            TRUE       5     0.3      2
##  maxlen target   ext
##      10  rules FALSE
## 
## Algorithmic control:
##  filter tree heap memopt load sort verbose
##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE
## 
## Absolute minimum support count: 2 
## 
## set item appearances ...[0 item(s)] done [0.00s].
## set transactions ...[6 item(s), 8 transaction(s)] done [0.00s].
## sorting and recoding items ... [4 item(s)] done [0.00s].
## creating transaction tree ... done [0.00s].
## checking subsets of size 1 2 done [0.00s].
## writing ... [6 rule(s)] done [0.00s].
## creating S4 object  ... done [0.00s].&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspect(rules)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     lhs        rhs     support confidence lift     count
## [1] {apple} =&amp;gt; {beer}  0.375   0.7500000  1.000000 3    
## [2] {beer}  =&amp;gt; {apple} 0.375   0.5000000  1.000000 3    
## [3] {milk}  =&amp;gt; {beer}  0.375   0.7500000  1.000000 3    
## [4] {beer}  =&amp;gt; {milk}  0.375   0.5000000  1.000000 3    
## [5] {rice}  =&amp;gt; {beer}  0.500   1.0000000  1.333333 4    
## [6] {beer}  =&amp;gt; {rice}  0.500   0.6666667  1.333333 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can observe that rule 6 states that {beer -&amp;gt; rice} has a support of 50% and a confidence of 67%. This means this rule was found in 50% of all transactions. The confidence that rice (LHS) is purchased given beer (RHS) is purchased (P(rice|beer)) is 67%. In other words, 67% of the times a customer buys beer, rice is bought as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-lhs-and-rhs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.4&lt;/span&gt; Set LHS and RHS&lt;/h2&gt;
&lt;p&gt;If you want to analyze a specific rule, you can use the option &lt;code&gt;appearance&lt;/code&gt; to set a LHS (if part) or RHS (then part) of the rule.&lt;/p&gt;
&lt;p&gt;For example, to analyze what items customers buy &lt;em&gt;before&lt;/em&gt; buying {beer}, we set &lt;code&gt;rhs=beer&lt;/code&gt;and &lt;code&gt;default=lhs&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer_rules_rhs &amp;lt;- apriori(trans, 
                          parameter = list(supp=0.3, conf=0.5, 
                                         maxlen=10, 
                                         minlen=2),
                          appearance = list(default=&amp;quot;lhs&amp;quot;, rhs=&amp;quot;beer&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Apriori
## 
## Parameter specification:
##  confidence minval smax arem  aval originalSupport maxtime support minlen
##         0.5    0.1    1 none FALSE            TRUE       5     0.3      2
##  maxlen target   ext
##      10  rules FALSE
## 
## Algorithmic control:
##  filter tree heap memopt load sort verbose
##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE
## 
## Absolute minimum support count: 2 
## 
## set item appearances ...[1 item(s)] done [0.00s].
## set transactions ...[6 item(s), 8 transaction(s)] done [0.00s].
## sorting and recoding items ... [4 item(s)] done [0.00s].
## creating transaction tree ... done [0.00s].
## checking subsets of size 1 2 done [0.00s].
## writing ... [3 rule(s)] done [0.00s].
## creating S4 object  ... done [0.00s].&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inspect the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspect(beer_rules_rhs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     lhs        rhs    support confidence lift     count
## [1] {apple} =&amp;gt; {beer} 0.375   0.75       1.000000 3    
## [2] {milk}  =&amp;gt; {beer} 0.375   0.75       1.000000 3    
## [3] {rice}  =&amp;gt; {beer} 0.500   1.00       1.333333 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also possible to analyze what items customers buy &lt;em&gt;after&lt;/em&gt; buying {beer}:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beer_rules_lhs &amp;lt;- apriori(trans, 
                          parameter = list(supp=0.3, conf=0.5, 
                                         maxlen=10, 
                                         minlen=2),
                          appearance = list(lhs=&amp;quot;beer&amp;quot;, default=&amp;quot;rhs&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Apriori
## 
## Parameter specification:
##  confidence minval smax arem  aval originalSupport maxtime support minlen
##         0.5    0.1    1 none FALSE            TRUE       5     0.3      2
##  maxlen target   ext
##      10  rules FALSE
## 
## Algorithmic control:
##  filter tree heap memopt load sort verbose
##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE
## 
## Absolute minimum support count: 2 
## 
## set item appearances ...[1 item(s)] done [0.00s].
## set transactions ...[6 item(s), 8 transaction(s)] done [0.00s].
## sorting and recoding items ... [4 item(s)] done [0.00s].
## creating transaction tree ... done [0.00s].
## checking subsets of size 1 2 done [0.00s].
## writing ... [3 rule(s)] done [0.00s].
## creating S4 object  ... done [0.00s].&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inspect the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspect(beer_rules_lhs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     lhs       rhs     support confidence lift     count
## [1] {beer} =&amp;gt; {apple} 0.375   0.5000000  1.000000 3    
## [2] {beer} =&amp;gt; {milk}  0.375   0.5000000  1.000000 3    
## [3] {beer} =&amp;gt; {rice}  0.500   0.6666667  1.333333 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-association-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.5&lt;/span&gt; Visualizing association rules&lt;/h2&gt;
&lt;p&gt;Mining association rules often results in a very large number of found rules, leaving the analyst with the task to go through all the rules and discover interesting ones. Sifting manually through large sets of rules is time consuming and strenuous. Therefore, in addition to our calculations of associations, we can use the package &lt;a href=&#34;https://github.com/mhahsler/arulesViz&#34;&gt;&lt;code&gt;arulesViz&lt;/code&gt;&lt;/a&gt; to visualize our results as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scatter-plots,&lt;/li&gt;
&lt;li&gt;interactive scatter-plots and&lt;/li&gt;
&lt;li&gt;Individual rule representations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a detailed discussion of the different visualization techniques, review Hahsler &amp;amp; Karpienko (2017).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scatter-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.6&lt;/span&gt; Scatter-Plot&lt;/h2&gt;
&lt;p&gt;A scatter plot for association rules uses two interest measures, one on each of the axes. The default plot for association rules in arulesViz is a scatter plot using support and confidence on the axes. The measure defined by shading (default: lift) is visualized by the color of the points. A color key is provided to the right of the plot.&lt;/p&gt;
&lt;p&gt;To visualize our association rules in a scatter plot, we use the function &lt;code&gt;plot()&lt;/code&gt; of the arulesViz package. You can use the function as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;plot(x, method, measure, shading, control, data, engine)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a detailed description, review the &lt;a href=&#34;https://cran.r-project.org/web/packages/arulesViz/arulesViz.pdf&#34;&gt;vignette of the package&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: an object of class “rules” or “itemsets”.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;method&lt;/code&gt;: a string with value “scatterplot”, “two-key plot”, “matrix”, “matrix3D”, “mo-saic”, “doubledecker”, “graph”, “paracoord” or “grouped”, “iplots” selecting the visualization method.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;measure&lt;/code&gt;: measure(s) of interestingness (e.g., “support”, “confidence”, “lift”, “order”) used in the visualization.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;shading&lt;/code&gt;: measure of interestingness used for the color of the points/arrows/nodes (e.g., “support”, “confidence”, “lift”). The default is “lift”.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;control&lt;/code&gt;: a list of control parameters for the plot. The available control parameters depend
on the used visualization method.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt;: the dataset (class “transactions”) used to generate the rules/itemsets. Only “mo-saic” and “doubledecker” require the original data.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;engine&lt;/code&gt;: a string indicating the plotting engine used to render the plot. The “default” en- gine uses (mostly) grid, but some plots can produce interactive interactive grid visualizations using engine “interactive”, or HTML widgets using engine “html- widget”.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a basic plot with default settings, just insert the object x (in our case rules). This visualization method draws a two dimensional scatter plot with different measures of interestingness (parameter “measure”) on the axes and a third measure (parameter “shading”) is represented by the color of the points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(arulesViz)

plot(rules)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-20&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://kirenz.com/post/2020-05-14-r-association-rule-mining/index_files/figure-html/unnamed-chunk-20-1.png&#34; alt=&#34;Scatter plot&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.2: Scatter plot
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The plot shows support on the x-axis and confidence on the y-axis. Lift ist shown as a color with different levels ranging from grey to red.&lt;/p&gt;
&lt;p&gt;We could also use only “confidence” as a specific measure of interest:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(rules, measure = &amp;quot;confidence&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-21&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://kirenz.com/post/2020-05-14-r-association-rule-mining/index_files/figure-html/unnamed-chunk-21-1.png&#34; alt=&#34;Scatter plot with confidence as measure of interest&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.3: Scatter plot with confidence as measure of interest
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There is a special value for shading called “order” which produces a two-key plot where the color of the points represents the length (order) of the rule if you select &lt;code&gt;method = &#34;two-key plot&lt;/code&gt;. This is basically a scatterplot with &lt;code&gt;shading = &#34;order&#34;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(rules, method = &amp;quot;two-key plot&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-22&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://kirenz.com/post/2020-05-14-r-association-rule-mining/index_files/figure-html/unnamed-chunk-22-1.png&#34; alt=&#34;Two-key plot&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.4: Two-key plot
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;interactive-scatter-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.7&lt;/span&gt; Interactive scatter-plot&lt;/h2&gt;
&lt;p&gt;Plot an interactive scatter plot for association rules using &lt;a href=&#34;https://plotly-r.com/index.html&#34;&gt;plotly&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(rules, engine = &amp;quot;plotly&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-23&#34;&gt;&lt;/span&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;visdat&#34;:{&#34;2ebf4965bf82&#34;:[&#34;function () &#34;,&#34;plotlyVisDat&#34;]},&#34;cur_data&#34;:&#34;2ebf4965bf82&#34;,&#34;attrs&#34;:{&#34;2ebf4965bf82&#34;:{&#34;x&#34;:[0.372843329000752,0.374257025711704,0.374854304845212,0.375451007452793,0.50124242358841,0.501907933545299],&#34;y&#34;:[0.749181014851698,0.499736960147629,0.75085147463073,0.499222260504251,0.998594967072699,0.66606791784155],&#34;hoverinfo&#34;:&#34;text&#34;,&#34;text&#34;:[&#34;[1]&lt;BR&gt; &lt;B&gt;{apple}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{beer}&lt;\/B&gt; &lt;BR&gt;&lt;BR&gt;support: 0.375 &lt;BR&gt;confidence: 0.75 &lt;BR&gt;lift: 1&#34;,&#34;[2]&lt;BR&gt; &lt;B&gt;{beer}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{apple}&lt;\/B&gt; &lt;BR&gt;&lt;BR&gt;support: 0.375 &lt;BR&gt;confidence: 0.5 &lt;BR&gt;lift: 1&#34;,&#34;[3]&lt;BR&gt; &lt;B&gt;{milk}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{beer}&lt;\/B&gt; &lt;BR&gt;&lt;BR&gt;support: 0.375 &lt;BR&gt;confidence: 0.75 &lt;BR&gt;lift: 1&#34;,&#34;[4]&lt;BR&gt; &lt;B&gt;{beer}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{milk}&lt;\/B&gt; &lt;BR&gt;&lt;BR&gt;support: 0.375 &lt;BR&gt;confidence: 0.5 &lt;BR&gt;lift: 1&#34;,&#34;[5]&lt;BR&gt; &lt;B&gt;{rice}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{beer}&lt;\/B&gt; &lt;BR&gt;&lt;BR&gt;support: 0.5 &lt;BR&gt;confidence: 1 &lt;BR&gt;lift: 1.33&#34;,&#34;[6]&lt;BR&gt; &lt;B&gt;{beer}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{rice}&lt;\/B&gt; &lt;BR&gt;&lt;BR&gt;support: 0.5 &lt;BR&gt;confidence: 0.667 &lt;BR&gt;lift: 1.33&#34;],&#34;mode&#34;:&#34;markers&#34;,&#34;marker&#34;:[],&#34;color&#34;:[1,1,1,1,1.33333333333333,1.33333333333333],&#34;colors&#34;:[&#34;#EEEEEEFF&#34;,&#34;#EE0000FF&#34;],&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;type&#34;:&#34;scatter&#34;}},&#34;layout&#34;:{&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10},&#34;xaxis&#34;:{&#34;domain&#34;:[0,1],&#34;automargin&#34;:true,&#34;title&#34;:&#34;support&#34;},&#34;yaxis&#34;:{&#34;domain&#34;:[0,1],&#34;automargin&#34;:true,&#34;title&#34;:&#34;confidence&#34;},&#34;hovermode&#34;:&#34;closest&#34;,&#34;showlegend&#34;:false,&#34;legend&#34;:{&#34;yanchor&#34;:&#34;top&#34;,&#34;y&#34;:0.5}},&#34;source&#34;:&#34;A&#34;,&#34;config&#34;:{&#34;showSendToCloud&#34;:false},&#34;data&#34;:[{&#34;x&#34;:[0.372843329000752,0.374257025711704,0.374854304845212,0.375451007452793,0.50124242358841,0.501907933545299],&#34;y&#34;:[0.749181014851698,0.499736960147629,0.75085147463073,0.499222260504251,0.998594967072699,0.66606791784155],&#34;hoverinfo&#34;:[&#34;text&#34;,&#34;text&#34;,&#34;text&#34;,&#34;text&#34;,&#34;text&#34;,&#34;text&#34;],&#34;text&#34;:[&#34;[1]&lt;BR&gt; &lt;B&gt;{apple}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{beer}&lt;\/B&gt; &lt;BR&gt;&lt;BR&gt;support: 0.375 &lt;BR&gt;confidence: 0.75 &lt;BR&gt;lift: 1&#34;,&#34;[2]&lt;BR&gt; &lt;B&gt;{beer}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{apple}&lt;\/B&gt; &lt;BR&gt;&lt;BR&gt;support: 0.375 &lt;BR&gt;confidence: 0.5 &lt;BR&gt;lift: 1&#34;,&#34;[3]&lt;BR&gt; &lt;B&gt;{milk}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{beer}&lt;\/B&gt; &lt;BR&gt;&lt;BR&gt;support: 0.375 &lt;BR&gt;confidence: 0.75 &lt;BR&gt;lift: 1&#34;,&#34;[4]&lt;BR&gt; &lt;B&gt;{beer}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{milk}&lt;\/B&gt; &lt;BR&gt;&lt;BR&gt;support: 0.375 &lt;BR&gt;confidence: 0.5 &lt;BR&gt;lift: 1&#34;,&#34;[5]&lt;BR&gt; &lt;B&gt;{rice}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{beer}&lt;\/B&gt; &lt;BR&gt;&lt;BR&gt;support: 0.5 &lt;BR&gt;confidence: 1 &lt;BR&gt;lift: 1.33&#34;,&#34;[6]&lt;BR&gt; &lt;B&gt;{beer}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{rice}&lt;\/B&gt; &lt;BR&gt;&lt;BR&gt;support: 0.5 &lt;BR&gt;confidence: 0.667 &lt;BR&gt;lift: 1.33&#34;],&#34;mode&#34;:&#34;markers&#34;,&#34;type&#34;:&#34;scatter&#34;,&#34;marker&#34;:{&#34;colorbar&#34;:{&#34;title&#34;:&#34;&#34;,&#34;ticklen&#34;:2},&#34;cmin&#34;:1,&#34;cmax&#34;:1.33333333333333,&#34;colorscale&#34;:[[&#34;0&#34;,&#34;rgba(238,238,238,1)&#34;],[&#34;0.0416666666666665&#34;,&#34;rgba(241,231,228,1)&#34;],[&#34;0.0833333333333331&#34;,&#34;rgba(244,223,218,1)&#34;],[&#34;0.125&#34;,&#34;rgba(247,216,208,1)&#34;],[&#34;0.166666666666667&#34;,&#34;rgba(249,208,198,1)&#34;],[&#34;0.208333333333333&#34;,&#34;rgba(251,201,188,1)&#34;],[&#34;0.25&#34;,&#34;rgba(252,193,178,1)&#34;],[&#34;0.291666666666667&#34;,&#34;rgba(253,186,168,1)&#34;],[&#34;0.333333333333334&#34;,&#34;rgba(254,178,158,1)&#34;],[&#34;0.375&#34;,&#34;rgba(255,170,149,1)&#34;],[&#34;0.416666666666667&#34;,&#34;rgba(255,163,139,1)&#34;],[&#34;0.458333333333333&#34;,&#34;rgba(255,155,130,1)&#34;],[&#34;0.5&#34;,&#34;rgba(255,147,120,1)&#34;],[&#34;0.541666666666667&#34;,&#34;rgba(255,139,111,1)&#34;],[&#34;0.583333333333333&#34;,&#34;rgba(254,131,102,1)&#34;],[&#34;0.625&#34;,&#34;rgba(253,123,92,1)&#34;],[&#34;0.666666666666666&#34;,&#34;rgba(252,114,83,1)&#34;],[&#34;0.708333333333334&#34;,&#34;rgba(251,105,74,1)&#34;],[&#34;0.75&#34;,&#34;rgba(250,96,65,1)&#34;],[&#34;0.791666666666667&#34;,&#34;rgba(248,86,56,1)&#34;],[&#34;0.833333333333333&#34;,&#34;rgba(246,76,46,1)&#34;],[&#34;0.875&#34;,&#34;rgba(245,65,37,1)&#34;],[&#34;0.916666666666667&#34;,&#34;rgba(242,51,26,1)&#34;],[&#34;0.958333333333333&#34;,&#34;rgba(240,34,14,1)&#34;],[&#34;1&#34;,&#34;rgba(238,0,0,1)&#34;]],&#34;showscale&#34;:false,&#34;color&#34;:[1,1,1,1,1.33333333333333,1.33333333333333],&#34;line&#34;:{&#34;colorbar&#34;:{&#34;title&#34;:&#34;&#34;,&#34;ticklen&#34;:2},&#34;cmin&#34;:1,&#34;cmax&#34;:1.33333333333333,&#34;colorscale&#34;:[[&#34;0&#34;,&#34;rgba(238,238,238,1)&#34;],[&#34;0.0416666666666665&#34;,&#34;rgba(241,231,228,1)&#34;],[&#34;0.0833333333333331&#34;,&#34;rgba(244,223,218,1)&#34;],[&#34;0.125&#34;,&#34;rgba(247,216,208,1)&#34;],[&#34;0.166666666666667&#34;,&#34;rgba(249,208,198,1)&#34;],[&#34;0.208333333333333&#34;,&#34;rgba(251,201,188,1)&#34;],[&#34;0.25&#34;,&#34;rgba(252,193,178,1)&#34;],[&#34;0.291666666666667&#34;,&#34;rgba(253,186,168,1)&#34;],[&#34;0.333333333333334&#34;,&#34;rgba(254,178,158,1)&#34;],[&#34;0.375&#34;,&#34;rgba(255,170,149,1)&#34;],[&#34;0.416666666666667&#34;,&#34;rgba(255,163,139,1)&#34;],[&#34;0.458333333333333&#34;,&#34;rgba(255,155,130,1)&#34;],[&#34;0.5&#34;,&#34;rgba(255,147,120,1)&#34;],[&#34;0.541666666666667&#34;,&#34;rgba(255,139,111,1)&#34;],[&#34;0.583333333333333&#34;,&#34;rgba(254,131,102,1)&#34;],[&#34;0.625&#34;,&#34;rgba(253,123,92,1)&#34;],[&#34;0.666666666666666&#34;,&#34;rgba(252,114,83,1)&#34;],[&#34;0.708333333333334&#34;,&#34;rgba(251,105,74,1)&#34;],[&#34;0.75&#34;,&#34;rgba(250,96,65,1)&#34;],[&#34;0.791666666666667&#34;,&#34;rgba(248,86,56,1)&#34;],[&#34;0.833333333333333&#34;,&#34;rgba(246,76,46,1)&#34;],[&#34;0.875&#34;,&#34;rgba(245,65,37,1)&#34;],[&#34;0.916666666666667&#34;,&#34;rgba(242,51,26,1)&#34;],[&#34;0.958333333333333&#34;,&#34;rgba(240,34,14,1)&#34;],[&#34;1&#34;,&#34;rgba(238,0,0,1)&#34;]],&#34;showscale&#34;:false,&#34;color&#34;:[1,1,1,1,1.33333333333333,1.33333333333333]}},&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[0.372843329000752,0.501907933545299],&#34;y&#34;:[0.499222260504251,0.998594967072699],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;markers&#34;,&#34;opacity&#34;:0,&#34;hoverinfo&#34;:&#34;none&#34;,&#34;showlegend&#34;:false,&#34;marker&#34;:{&#34;colorbar&#34;:{&#34;title&#34;:&#34;lift&#34;,&#34;ticklen&#34;:2,&#34;len&#34;:0.5,&#34;lenmode&#34;:&#34;fraction&#34;,&#34;y&#34;:1,&#34;yanchor&#34;:&#34;top&#34;},&#34;cmin&#34;:1,&#34;cmax&#34;:1.33333333333333,&#34;colorscale&#34;:[[&#34;0&#34;,&#34;rgba(238,238,238,1)&#34;],[&#34;0.0416666666666665&#34;,&#34;rgba(241,231,228,1)&#34;],[&#34;0.0833333333333331&#34;,&#34;rgba(244,223,218,1)&#34;],[&#34;0.125&#34;,&#34;rgba(247,216,208,1)&#34;],[&#34;0.166666666666667&#34;,&#34;rgba(249,208,198,1)&#34;],[&#34;0.208333333333333&#34;,&#34;rgba(251,201,188,1)&#34;],[&#34;0.25&#34;,&#34;rgba(252,193,178,1)&#34;],[&#34;0.291666666666667&#34;,&#34;rgba(253,186,168,1)&#34;],[&#34;0.333333333333334&#34;,&#34;rgba(254,178,158,1)&#34;],[&#34;0.375&#34;,&#34;rgba(255,170,149,1)&#34;],[&#34;0.416666666666667&#34;,&#34;rgba(255,163,139,1)&#34;],[&#34;0.458333333333333&#34;,&#34;rgba(255,155,130,1)&#34;],[&#34;0.5&#34;,&#34;rgba(255,147,120,1)&#34;],[&#34;0.541666666666667&#34;,&#34;rgba(255,139,111,1)&#34;],[&#34;0.583333333333333&#34;,&#34;rgba(254,131,102,1)&#34;],[&#34;0.625&#34;,&#34;rgba(253,123,92,1)&#34;],[&#34;0.666666666666666&#34;,&#34;rgba(252,114,83,1)&#34;],[&#34;0.708333333333334&#34;,&#34;rgba(251,105,74,1)&#34;],[&#34;0.75&#34;,&#34;rgba(250,96,65,1)&#34;],[&#34;0.791666666666667&#34;,&#34;rgba(248,86,56,1)&#34;],[&#34;0.833333333333333&#34;,&#34;rgba(246,76,46,1)&#34;],[&#34;0.875&#34;,&#34;rgba(245,65,37,1)&#34;],[&#34;0.916666666666667&#34;,&#34;rgba(242,51,26,1)&#34;],[&#34;0.958333333333333&#34;,&#34;rgba(240,34,14,1)&#34;],[&#34;1&#34;,&#34;rgba(238,0,0,1)&#34;]],&#34;showscale&#34;:true,&#34;color&#34;:[1,1.33333333333333],&#34;line&#34;:{&#34;color&#34;:&#34;rgba(255,127,14,1)&#34;}},&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;frame&#34;:null}],&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.5: Interactive scatter-plot
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;graph-based-visualization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.8&lt;/span&gt; Graph-based visualization&lt;/h2&gt;
&lt;p&gt;Graph-based techniques concentrate on the relationship between individual items in the rule set. They represent the rules (or itemsets) as a graph with items as labeled vertices, and rules (or itemsets) represented as vertices connected to items using arrows.&lt;/p&gt;
&lt;p&gt;For rules, the LHS items are connected with arrows pointing to the vertex representing the rule and the RHS has an arrow pointing to the item.&lt;/p&gt;
&lt;p&gt;Several engines are available. The default engine uses igraph (plot.igraph and tkplot for the interactive visualization). … arguments are passed on to the respective plotting function (use for color, etc.).&lt;/p&gt;
&lt;p&gt;The network graph below shows associations between selected items. Larger circles imply higher support, while red circles imply higher lift. Graphs only work well with very few rules, why we only use a subset of 10 rules from our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subrules &amp;lt;- head(rules, n = 10, by = &amp;quot;confidence&amp;quot;)

plot(subrules, method = &amp;quot;graph&amp;quot;,  engine = &amp;quot;htmlwidget&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-24&#34;&gt;&lt;/span&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;visNetwork html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;nodes&#34;:{&#34;id&#34;:[1,2,3,4,5,6,7,8,9,10],&#34;label&#34;:[&#34;rice&#34;,&#34;apple&#34;,&#34;milk&#34;,&#34;beer&#34;,&#34;rule 1&#34;,&#34;rule 2&#34;,&#34;rule 3&#34;,&#34;rule 4&#34;,&#34;rule 5&#34;,&#34;rule 6&#34;],&#34;group&#34;:[&#34;item&#34;,&#34;item&#34;,&#34;item&#34;,&#34;item&#34;,&#34;rule&#34;,&#34;rule&#34;,&#34;rule&#34;,&#34;rule&#34;,&#34;rule&#34;,&#34;rule&#34;],&#34;value&#34;:[1,1,1,1,100,1,1,100,1,1],&#34;color&#34;:[&#34;#CBD2FC&#34;,&#34;#CBD2FC&#34;,&#34;#CBD2FC&#34;,&#34;#CBD2FC&#34;,&#34;#EE1B1B&#34;,&#34;#EEDCDC&#34;,&#34;#EEDCDC&#34;,&#34;#EE1B1B&#34;,&#34;#EEDCDC&#34;,&#34;#EEDCDC&#34;],&#34;title&#34;:[&#34;rice&#34;,&#34;apple&#34;,&#34;milk&#34;,&#34;beer&#34;,&#34;&lt;B&gt;[1]&lt;\/B&gt;&lt;BR&gt;&lt;B&gt;{rice}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{beer}&lt;\/B&gt;&lt;BR&gt;&lt;BR&gt;support = 0.5&lt;BR&gt;confidence = 1&lt;BR&gt;lift = 1.33&lt;BR&gt;count = 4&lt;BR&gt;order = 2&#34;,&#34;&lt;B&gt;[2]&lt;\/B&gt;&lt;BR&gt;&lt;B&gt;{apple}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{beer}&lt;\/B&gt;&lt;BR&gt;&lt;BR&gt;support = 0.375&lt;BR&gt;confidence = 0.75&lt;BR&gt;lift = 1&lt;BR&gt;count = 3&lt;BR&gt;order = 2&#34;,&#34;&lt;B&gt;[3]&lt;\/B&gt;&lt;BR&gt;&lt;B&gt;{milk}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{beer}&lt;\/B&gt;&lt;BR&gt;&lt;BR&gt;support = 0.375&lt;BR&gt;confidence = 0.75&lt;BR&gt;lift = 1&lt;BR&gt;count = 3&lt;BR&gt;order = 2&#34;,&#34;&lt;B&gt;[4]&lt;\/B&gt;&lt;BR&gt;&lt;B&gt;{beer}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{rice}&lt;\/B&gt;&lt;BR&gt;&lt;BR&gt;support = 0.5&lt;BR&gt;confidence = 0.667&lt;BR&gt;lift = 1.33&lt;BR&gt;count = 4&lt;BR&gt;order = 2&#34;,&#34;&lt;B&gt;[5]&lt;\/B&gt;&lt;BR&gt;&lt;B&gt;{beer}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{apple}&lt;\/B&gt;&lt;BR&gt;&lt;BR&gt;support = 0.375&lt;BR&gt;confidence = 0.5&lt;BR&gt;lift = 1&lt;BR&gt;count = 3&lt;BR&gt;order = 2&#34;,&#34;&lt;B&gt;[6]&lt;\/B&gt;&lt;BR&gt;&lt;B&gt;{beer}&lt;\/B&gt;&lt;BR&gt;&amp;nbsp;&amp;nbsp; =&gt; &lt;B&gt;{milk}&lt;\/B&gt;&lt;BR&gt;&lt;BR&gt;support = 0.375&lt;BR&gt;confidence = 0.5&lt;BR&gt;lift = 1&lt;BR&gt;count = 3&lt;BR&gt;order = 2&#34;],&#34;shape&#34;:[&#34;box&#34;,&#34;box&#34;,&#34;box&#34;,&#34;box&#34;,&#34;circle&#34;,&#34;circle&#34;,&#34;circle&#34;,&#34;circle&#34;,&#34;circle&#34;,&#34;circle&#34;],&#34;x&#34;:[0.996495073723866,-1,1,0.33177320349987,0.929562982997801,-0.455729857058796,0.511988743815424,0.515681028603443,-0.440066774584428,0.927695654841687],&#34;y&#34;:[1,0.0316482297982752,-1,0.00137944417334968,0.481135037289498,-0.194197493726862,-0.69726058497054,0.687591311791381,0.222333646316521,-0.481124848005314]},&#34;edges&#34;:{&#34;from&#34;:[1,2,3,4,4,4,5,6,7,8,9,10],&#34;to&#34;:[5,6,7,8,9,10,4,4,4,1,2,3],&#34;arrows&#34;:[&#34;to&#34;,&#34;to&#34;,&#34;to&#34;,&#34;to&#34;,&#34;to&#34;,&#34;to&#34;,&#34;to&#34;,&#34;to&#34;,&#34;to&#34;,&#34;to&#34;,&#34;to&#34;,&#34;to&#34;]},&#34;nodesToDataframe&#34;:true,&#34;edgesToDataframe&#34;:true,&#34;options&#34;:{&#34;width&#34;:&#34;100%&#34;,&#34;height&#34;:&#34;100%&#34;,&#34;nodes&#34;:{&#34;shape&#34;:&#34;dot&#34;,&#34;scaling&#34;:{&#34;label&#34;:{&#34;enabled&#34;:true}},&#34;physics&#34;:false},&#34;manipulation&#34;:{&#34;enabled&#34;:false},&#34;edges&#34;:{&#34;smooth&#34;:false},&#34;physics&#34;:{&#34;stabilization&#34;:false},&#34;interaction&#34;:{&#34;hover&#34;:true}},&#34;groups&#34;:[&#34;item&#34;,&#34;rule&#34;],&#34;width&#34;:null,&#34;height&#34;:null,&#34;idselection&#34;:{&#34;enabled&#34;:true,&#34;style&#34;:&#34;width: 150px; height: 26px&#34;,&#34;useLabels&#34;:true,&#34;main&#34;:&#34;Select by id&#34;},&#34;byselection&#34;:{&#34;enabled&#34;:false,&#34;style&#34;:&#34;width: 150px; height: 26px&#34;,&#34;multiple&#34;:false,&#34;hideColor&#34;:&#34;rgba(200,200,200,0.5)&#34;,&#34;highlight&#34;:false},&#34;main&#34;:null,&#34;submain&#34;:null,&#34;footer&#34;:null,&#34;background&#34;:&#34;rgba(0, 0, 0, 0)&#34;,&#34;igraphlayout&#34;:{&#34;type&#34;:&#34;square&#34;},&#34;tooltipStay&#34;:300,&#34;tooltipStyle&#34;:&#34;position: fixed;visibility:hidden;padding: 5px;white-space: nowrap;font-family: verdana;font-size:14px;font-color:#000000;background-color: #f5f4ed;-moz-border-radius: 3px;-webkit-border-radius: 3px;border-radius: 3px;border: 1px solid #808074;box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);&#34;,&#34;highlight&#34;:{&#34;enabled&#34;:true,&#34;hoverNearest&#34;:true,&#34;degree&#34;:1,&#34;algorithm&#34;:&#34;all&#34;,&#34;hideColor&#34;:&#34;rgba(200,200,200,0.5)&#34;,&#34;labelOnly&#34;:true},&#34;collapse&#34;:{&#34;enabled&#34;:false,&#34;fit&#34;:false,&#34;resetHighlight&#34;:true,&#34;clusterOptions&#34;:null,&#34;keepCoord&#34;:true,&#34;labelSuffix&#34;:&#34;(cluster)&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.6: Graph-based visualization
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;parallel-coordinate-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.9&lt;/span&gt; Parallel coordinate plot&lt;/h2&gt;
&lt;p&gt;Represents the rules (or itemsets) as a parallel coordinate plot (from LHS to RHS).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(subrules, method=&amp;quot;paracoord&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-25&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://kirenz.com/post/2020-05-14-r-association-rule-mining/index_files/figure-html/unnamed-chunk-25-1.png&#34; alt=&#34;Parallel coordinate plot&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.7: Parallel coordinate plot
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The plot indicates that if a customer buys rice, he is likely to buy beer as well: {rice -&amp;gt; beer}. The same is true for the opposite direction: {beer -&amp;gt; rice}.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; References&lt;/h1&gt;
&lt;p&gt;Hahsler, M., &amp;amp; Karpienko, R. (2017). Visualizing association rules in hierarchical groups. Journal of Business Economics, 87(3), 317–335. &lt;a href=&#34;https://doi.org/10.1007/s11573-016-0822-8&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s11573-016-0822-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hornik, K., Grün, B., &amp;amp; Hahsler, M. (2005). arules - A computational environment for mining association rules and frequent item sets. Journal of Statistical Software, 14(15), 1–25.&lt;/p&gt;
&lt;p&gt;Leskovec, J., Rajaraman, A., &amp;amp; Ullman, J. D. (2020). Mining of massive data sets. Cambridge university press.&lt;/p&gt;
&lt;p&gt;Ng, A., &amp;amp; Soo, K. (2017). Numsense! Data Science for the Layman: No Math Added. Leanpub.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to R Markdown</title>
      <link>https://kirenz.com/project/markdown-first-steps/</link>
      <pubDate>Mon, 18 Nov 2019 10:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/markdown-first-steps/</guid>
      <description>


&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;HTML-presentation &lt;a href=&#34;https://kirenz.com/slides/markdown-first-steps/markdown-guide.html&#34;&gt;“First Steps in Markdown”&lt;/a&gt; (created in Markdown with the &lt;a href=&#34;https://www.kirenz.com/project/r-xaringan/&#34;&gt;R Xaringan package&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.kirenz.com/slides/markdown-report/markdown-report.html#1&#34;&gt;R Markdown Report in HTML&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;Markdown is one of the world’s most popular markup languages used in data science. Both &lt;strong&gt;R Markdown&lt;/strong&gt; and &lt;strong&gt;Jupyter Notebooks&lt;/strong&gt; use &lt;strong&gt;Markdown&lt;/strong&gt; to provide an unified authoring framework for data science, combining code (Python, R, SQL,…), its results, and commentary in Markdown. The documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more.&lt;/p&gt;
&lt;p&gt;According to &lt;a href=&#34;https://r4ds.had.co.nz/r-markdown.html&#34;&gt;Wickham &amp;amp; Grolemund (2016)&lt;/a&gt;, Markdown files are designed to be used in three ways:&lt;/p&gt;
&lt;style&gt;
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
&lt;/style&gt;
&lt;div class=&#34;blue&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;For &lt;strong&gt;communicating&lt;/strong&gt; to decision makers, who want to focus on the conclusions, not the code behind the analysis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For &lt;strong&gt;collaborating&lt;/strong&gt; with other data scientists, who are interested in both your conclusions, and how you reached them (i.e. the code).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As an environment in which &lt;strong&gt;to do data science&lt;/strong&gt;, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Learn the most important basics of Markdown in this excellent interactive &lt;a href=&#34;https://commonmark.org/help/&#34;&gt;“60 Seconds Markdown Tutorial”&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://r4ds.had.co.nz/r-markdown.html&#34;&gt;Wickham, H., &amp;amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Programming Languages for Data Science</title>
      <link>https://kirenz.com/talk/2019-programming-languages/</link>
      <pubDate>Thu, 24 Oct 2019 09:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/talk/2019-programming-languages/</guid>
      <description>&lt;p&gt;Anhand von mehreren Fallstudien wird zunächst die Extraktion, Bearbeitung und Analyse von Daten in unterschiedlichen Datenbanken mit Hilfe von SQL eingehend behandelt. Der dabei gelernte SQL-Syntax – bspw.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Datenimport,&lt;/li&gt;
&lt;li&gt;Verknüpfung von Tabellen,&lt;/li&gt;
&lt;li&gt;Gruppierung und Summierung,&lt;/li&gt;
&lt;li&gt;Berechnung statistischer Kennzahlen, - Datenexploration,&lt;/li&gt;
&lt;li&gt;Analyse von Zeitdaten,&lt;/li&gt;
&lt;li&gt;Textanalysen und&lt;/li&gt;
&lt;li&gt;bedingte Ausdrücke (Conditional Expressions)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;kann auf eine Vielzahl von Datenbanken wie bspw.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PostgreSQL,&lt;/li&gt;
&lt;li&gt;MySQL,&lt;/li&gt;
&lt;li&gt;Microsoft Azure SQL Datenbank, Google BigQuery und Oracle angewendet werden.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Im Rahmen der Datenanalyse mit R werden neben den zentralen Grundkenntnissen der Programmiersprache R insbesondere Kompetenzen im Umgang mit Datentransformationen („Data Wrangling“), der explorativen Datenanalyse und Visualisierung von Daten (bspw. mit Hilfe eines Dashboards) vermittelt. Zudem wird die Erstellung und Ausgabe (bspw. als HTML, PDF, Word, Excel, PPT,…) von Reports in R markdown behandelt.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Web Analytics</title>
      <link>https://kirenz.com/talk/2019-web-analytics/</link>
      <pubDate>Mon, 14 Oct 2019 08:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/talk/2019-web-analytics/</guid>
      <description>&lt;p&gt;In der Veranstaltung Web Analytics befassen wir uns mit der Erfassung, Analyse und Visualisierung von strukturierten und unstrukturierten Daten aus dem Internet -  bspw. Daten aus sozialen Medien, Multi-Media-Plattformen, Microblogs, Foren und anderen Webpräsenzen.&lt;/p&gt;

&lt;p&gt;Von besonderem Interesse sind in diesem Zusammenhang &amp;ldquo;offene Daten&amp;rdquo; (Open Data), die im Internet zur freien Weiterverwendung frei zugänglich gemacht werden (insbesondere von Organisationen wie der OECD, Weltbank und von staatlichen Stellen).&lt;/p&gt;

&lt;p&gt;Wir werden insbesondere mit der Programmiersprache R arbeiten und die folgenden Inhalte behandeln:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Web-scraping&lt;/li&gt;
&lt;li&gt;Data Science&lt;/li&gt;
&lt;li&gt;Web-API&amp;rsquo;s&lt;/li&gt;
&lt;li&gt;Natural Language Processing bzw. Text Mining&lt;/li&gt;
&lt;li&gt;Programmierung von Dashboards&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Big Data and Web Analytics</title>
      <link>https://kirenz.com/talk/2019-big-data-web-analytics/</link>
      <pubDate>Fri, 11 Oct 2019 09:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/talk/2019-big-data-web-analytics/</guid>
      <description>&lt;p&gt;&amp;lsquo;In der Veranstaltung Web Analytics &amp;amp; Big Data befassen wir uns mit der Erfassung und Analyse von strukturierten und unstrukturierten (großen) Daten - bspw. Daten aus sozialen Medien, Multi-Media-Plattformen, Microblogs, Foren, Webpräsenzen. Die dadurch gewonnenen Erkenntnisse sollen einen Beitrag zu dem systematischen Aufbau und der Pflege dauerhafter und profitabler Kundenbeziehungen leisten und beziehen sich insbesondere auf die Phasen der Kundengewinnung, Kundenbindung und Vermeidung der Kundenabwanderung innerhalb des Customer Lifecycle Managements.&amp;rsquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Applied Statistics</title>
      <link>https://kirenz.com/talk/2019-applied-statistics/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/talk/2019-applied-statistics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Text Mining in R</title>
      <link>https://kirenz.com/post/2019-09-16-r-text-mining/</link>
      <pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/post/2019-09-16-r-text-mining/</guid>
      <description>
&lt;script src=&#34;https://kirenz.com/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-textmining-in-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction to Textmining in R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#installation-of-r-packages&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1&lt;/span&gt; Installation of R packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-import&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2&lt;/span&gt; Data import&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-transformation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Data transformation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tokenization&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Tokenization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stop-words&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Stop words&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploratory-data-analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Exploratory data analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#term-frequency-tf&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.1&lt;/span&gt; Term frequency (tf)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#term-frequency-and-inverse-document-frequency-tf-idf&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.2&lt;/span&gt; Term frequency and inverse document frequency (tf-idf)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tokenizing-by-n-gram&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.3&lt;/span&gt; Tokenizing by n-gram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#network-analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.4&lt;/span&gt; Network analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#classification-with-logistic-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Classification with logistic regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#train-test-split&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1&lt;/span&gt; Train test split&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#training-data-sparse-matrix&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2&lt;/span&gt; Training data (sparse matrix)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#response-variable&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3&lt;/span&gt; Response variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.4&lt;/span&gt; Logistic regression model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-evaluation-with-test-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.5&lt;/span&gt; Model evaluation with test data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction-to-textmining-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction to Textmining in R&lt;/h1&gt;
&lt;p&gt;This post demonstrates how various R packages can be used for text mining in R. In particular, we start with common text transformations, perform various data explorations with term frequency (tf) and inverse document frequency (idf) and build a supervised classifiaction model that learns the difference between texts of different authors.&lt;/p&gt;
&lt;p&gt;The content of this tutorial is based on the excellent book &lt;a href=&#34;https://www.tidytextmining.com&#34;&gt;“Textmining with R (2019)”&lt;/a&gt; from Julia Silge and David Robinson and the blog post &lt;a href=&#34;https://www.r-bloggers.com/text-classification-with-tidy-data-principles/&#34;&gt;“Text classification with tidy data principles (2018)”&lt;/a&gt; from Julia Silges.&lt;/p&gt;
&lt;div id=&#34;installation-of-r-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Installation of R packages&lt;/h2&gt;
&lt;p&gt;If you like to install all packages at once, use the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(c(&amp;quot;dplyr&amp;quot;, &amp;quot;gutenbergr&amp;quot;, &amp;quot;stringr&amp;quot;, &amp;quot;tidytext&amp;quot;, &amp;quot;tidyr&amp;quot;,
                   &amp;quot;stopwords&amp;quot;, &amp;quot;wordcloud&amp;quot;, &amp;quot;rsample&amp;quot;, &amp;quot;glmnet&amp;quot;, 
                   &amp;quot;doMC&amp;quot;, &amp;quot;forcats&amp;quot;, &amp;quot;broom&amp;quot;, &amp;quot;igraph&amp;quot;, &amp;quot;ggraph&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-import&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Data import&lt;/h2&gt;
&lt;p&gt;We can access the full texts of various books from “Project Gutenberg” via the &lt;a href=&#34;https://cran.r-project.org/web/packages/gutenbergr/vignettes/intro.html&#34;&gt;&lt;code&gt;gutenbergr&lt;/code&gt; package&lt;/a&gt;. We can look up certain authors or titles with a regular expression using the &lt;code&gt;stringr&lt;/code&gt; package. All functions in &lt;code&gt;stringr&lt;/code&gt; start with &lt;code&gt;str_&lt;/code&gt;and take a vector of strings as the first argument. To learn more about stringr, visit the &lt;a href=&#34;https://stringr.tidyverse.org&#34;&gt;stringr documentation&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gutenbergr)
library(stringr)

doyle &amp;lt;- gutenberg_works(str_detect(author, &amp;quot;Doyle&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
gutenberg_id
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
title
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
author
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
gutenberg_author_id
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
language
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gutenberg_bookshelf
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
rights
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
has_text
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
108
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
The Return of Sherlock Holmes
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Doyle, Arthur Conan
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
en
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Detective Fiction
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Public domain in the USA.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
126
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
The Poison Belt
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Doyle, Arthur Conan
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
en
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Science Fiction
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Public domain in the USA.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
139
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
The Lost World
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Doyle, Arthur Conan
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
en
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Science Fiction
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Public domain in the USA.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
244
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A Study in Scarlet
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Doyle, Arthur Conan
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
en
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Detective Fiction
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Public domain in the USA.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We obtain &lt;em&gt;“Relativity: The Special and General Theory”&lt;/em&gt; by Albert Einstein (gutenberg_id: 30155) and &lt;em&gt;“Experiments with Alternate Currents of High Potential and High Frequency”&lt;/em&gt; by Nikola Tesla (gutenberg_id: 13476) from gutenberg and add the column “author” to the result.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gutenbergr)

books &amp;lt;- gutenberg_download(c(30155, 13476), meta_fields = &amp;quot;author&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Furthermore, we transfrom the data to a &lt;a href=&#34;https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html&#34;&gt;tibble&lt;/a&gt; (tibbles are a modern take on data frames), add the row number with the column name &lt;code&gt;document&lt;/code&gt; to the tibble and drop the column &lt;code&gt;gutenberg_id&lt;/code&gt;. We will use the information in column &lt;code&gt;document&lt;/code&gt; to train a model that can take an individual line (row) and give us a probability that the text in this particular line comes from a certain author.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)

books &amp;lt;- as_tibble(books) %&amp;gt;% 
  mutate(document = row_number()) %&amp;gt;% 
  select(-gutenberg_id)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
text
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
author
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
document
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
EXPERIMENTS WITH ALTERNATE CURRENTS OF HIGH POTENTIAL AND HIGH FREQUENCY
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A Lecture Delivered before the Institution of Electrical Engineers, London
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
by
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NIKOLA TESLA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-transformation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Data transformation&lt;/h1&gt;
&lt;div id=&#34;tokenization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Tokenization&lt;/h2&gt;
&lt;p&gt;First of all, we need to both break the text into individual tokens (a process called &lt;strong&gt;tokenization&lt;/strong&gt;) and transform it to a tidy data structure (i.e. each variable must have its own column, each observation must have its own row and each value must have its own cell). To do this, we use tidytext’s &lt;code&gt;unnest_tokens()&lt;/code&gt; function. We also remove the &lt;em&gt;rarest words&lt;/em&gt; in that step, keeping only words in our dataset that occur more than 10 times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidytext)

tidy_books &amp;lt;- books %&amp;gt;%
  unnest_tokens(word, text) %&amp;gt;%
  group_by(word) %&amp;gt;%
  filter(n() &amp;gt; 10) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
author
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
document
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
experiments
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
with
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
alternate
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
currents
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
of
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
high
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
potential
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
and
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;stop-words&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Stop words&lt;/h2&gt;
&lt;p&gt;Now that the data is in a tidy “one-word-per-row” format, we can manipulate it with packages like &lt;code&gt;dplyr&lt;/code&gt;. Often in text analysis, we will want to remove &lt;strong&gt;stop words&lt;/strong&gt;: Stop words are words that are not useful for an analysis, typically extremely common words such as “the”, “of”, “to”, and so forth. We can remove stop words in our data by using the stop words provided in the package &lt;code&gt;stopwords&lt;/code&gt; with an &lt;code&gt;anti_join()&lt;/code&gt; from the package &lt;code&gt;dplyr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stopwords) 
library(dplyr)
library(tibble)

stopword &amp;lt;- as_tibble(stopwords::stopwords(&amp;quot;en&amp;quot;)) 
stopword &amp;lt;- rename(stopword, word=value)
tb &amp;lt;- anti_join(tidy_books, stopword, by = &amp;#39;word&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
author
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
document
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
experiments
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
alternate
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
currents
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
high
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
potential
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
high
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
frequency
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
lecture
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The tidy data structure allows different types of exploratory data analysis (EDA), which we turn to next.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Exploratory data analysis&lt;/h1&gt;
&lt;div id=&#34;term-frequency-tf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Term frequency (tf)&lt;/h2&gt;
&lt;p&gt;An important question in text mining is how to quantify what a document is about. One measure of how important a word may be is its &lt;strong&gt;term frequency&lt;/strong&gt; (tf), i.e. how frequently a word occurs in a document.&lt;/p&gt;
&lt;p&gt;We can start by using &lt;code&gt;dplyr&lt;/code&gt; to explore the most commonly used words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)

word_count &amp;lt;- count(tb, word, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
one
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
239
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
body
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
230
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
may
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
224
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
can
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
194
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
relativity
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
193
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Term frequency by author:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)

author_count &amp;lt;-  tb %&amp;gt;% 
  count(author, word, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
author
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
relativity
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
193
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
may
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
184
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
theory
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
181
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bulb
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
171
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
coil
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
166
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
high
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
166
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
body
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
156
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
one
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
156
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
reference
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
150
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
tube
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
147
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Plot terms with a frequency greater than 100:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)

tb %&amp;gt;%
  count(author, word, sort = TRUE) %&amp;gt;%
  filter(n &amp;gt; 100) %&amp;gt;%
  mutate(word = reorder(word, n)) %&amp;gt;%
  ggplot(aes(word, n)) +
  geom_col(aes(fill=author)) +
  xlab(NULL) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_flip() +
  theme_classic(base_size = 12) +
  labs(fill= &amp;quot;Author&amp;quot;, title=&amp;quot;Word frequency&amp;quot;, subtitle=&amp;quot;n &amp;gt; 100&amp;quot;)+
  theme(plot.title = element_text(lineheight=.8, face=&amp;quot;bold&amp;quot;)) +
  scale_fill_brewer() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-09-16-r-text-mining/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Plot top 20 terms by author:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

tb %&amp;gt;%
  count(author, word, sort = TRUE) %&amp;gt;%
  group_by(author) %&amp;gt;%
  top_n(20) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(reorder_within(word, n, author), n,
    fill = author)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~author, scales = &amp;quot;free&amp;quot;) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic(base_size = 12) +
  labs(fill= &amp;quot;Author&amp;quot;, 
       title=&amp;quot;Most frequent words&amp;quot;, 
       subtitle=&amp;quot;Top 20 words by book&amp;quot;,
       x= NULL, 
       y= &amp;quot;Word Count&amp;quot;)+
  theme(plot.title = element_text(lineheight=.8, face=&amp;quot;bold&amp;quot;)) +
  scale_fill_brewer()   &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-09-16-r-text-mining/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You may notice expressions like “_k”, “co” in the Einstein text and “fig” in the Tesla text. Let’s remove these and other less meaningful words with a custom list of stop words and use anti_join() to remove them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newstopwords &amp;lt;- tibble(word = c(&amp;quot;eq&amp;quot;, &amp;quot;co&amp;quot;, &amp;quot;rc&amp;quot;, &amp;quot;ac&amp;quot;, &amp;quot;ak&amp;quot;, &amp;quot;bn&amp;quot;, 
                                   &amp;quot;fig&amp;quot;, &amp;quot;file&amp;quot;, &amp;quot;cg&amp;quot;, &amp;quot;cb&amp;quot;, &amp;quot;cm&amp;quot;,
                               &amp;quot;ab&amp;quot;, &amp;quot;_k&amp;quot;, &amp;quot;_k_&amp;quot;, &amp;quot;_x&amp;quot;))

tb &amp;lt;- anti_join(tb, newstopwords, by = &amp;quot;word&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we plot the data again without the new stopwords:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

tb %&amp;gt;%
  count(author, word, sort = TRUE) %&amp;gt;%
  group_by(author) %&amp;gt;%
  top_n(20) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(reorder_within(word, n, author), n,
    fill = author)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~author, scales = &amp;quot;free&amp;quot;) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic(base_size = 12) +
  labs(fill= &amp;quot;Author&amp;quot;, 
       title=&amp;quot;Most frequent words after removing stop words&amp;quot;, 
       subtitle=&amp;quot;Top 20 words by book&amp;quot;,
       x= NULL, 
       y= &amp;quot;Word Count&amp;quot;)+
  theme(plot.title = element_text(lineheight=.8, face=&amp;quot;bold&amp;quot;)) +
  scale_fill_brewer()   &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-09-16-r-text-mining/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You also may want to visualize the most frequent terms as a simple word cloud:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wordcloud)

tb %&amp;gt;%
  count(word) %&amp;gt;%
  with(wordcloud(word, n, max.words = 15))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-09-16-r-text-mining/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;term-frequency-and-inverse-document-frequency-tf-idf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Term frequency and inverse document frequency (tf-idf)&lt;/h2&gt;
&lt;p&gt;Term frequency is a useful measure to determine how frequently a word occurs in a document. There are words in a document, however, that occur many times but may not be important.&lt;/p&gt;
&lt;p&gt;Another approach is to look at a term’s &lt;strong&gt;inverse document frequency (idf)&lt;/strong&gt;, which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. This can be combined with term frequency to calculate a term’s tf-idf (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used.&lt;/p&gt;
&lt;p&gt;The inverse document frequency for any given term is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence, term frequency and inverse document frequency allows us to find words that are characteristic for one document within a collection of documents. The &lt;code&gt;tidytext&lt;/code&gt; package uses an implementation of tf-idf consistent with tidy data principles that enables us to see how different words are important in documents within a collection or corpus of documents.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(forcats)

plot_tb &amp;lt;- tb %&amp;gt;%
  count(author, word, sort = TRUE) %&amp;gt;%
  bind_tf_idf(word, author, n) %&amp;gt;%
  mutate(word = fct_reorder(word, tf_idf)) %&amp;gt;%
  mutate(author = factor(author, 
                         levels = c(&amp;quot;Tesla, Nikola&amp;quot;,
                                    &amp;quot;Einstein, Albert&amp;quot;)))

plot_tb %&amp;gt;% 
  group_by(author) %&amp;gt;% 
  top_n(15, tf_idf) %&amp;gt;% 
  ungroup() %&amp;gt;%
  mutate(word = reorder(word, tf_idf)) %&amp;gt;%
  ggplot(aes(word, tf_idf, fill = author)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &amp;quot;tf-idf&amp;quot;) +
  facet_wrap(~author, ncol = 2, scales = &amp;quot;free&amp;quot;) +
  coord_flip() +
  theme_classic(base_size = 12) +
  labs(fill= &amp;quot;Author&amp;quot;, 
       title=&amp;quot;Term frequency and inverse document frequency (tf-idf)&amp;quot;, 
       subtitle=&amp;quot;Top 20 words by book&amp;quot;,
       x= NULL, 
       y= &amp;quot;tf-idf&amp;quot;) +
  theme(plot.title = element_text(lineheight=.8, face=&amp;quot;bold&amp;quot;)) +
  scale_fill_brewer()  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-09-16-r-text-mining/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In particular, the &lt;code&gt;bind_tf_idf&lt;/code&gt; function in the &lt;code&gt;tidytext&lt;/code&gt; package takes a tidy text dataset as input with one row per token (term), per document. One column (word here) contains the terms/tokens, one column contains the documents (authors in this case), and the last necessary column contains the counts, how many times each document contains each term (n in this example).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tf_idf &amp;lt;- tb %&amp;gt;%
  count(author, word, sort = TRUE) %&amp;gt;%
  bind_tf_idf(word, author, n)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
author
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
tf
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
idf
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
tf_idf
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
relativity
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
193
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0177831
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6931472
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0123263
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
may
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
184
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0139436
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
theory
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
181
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0166774
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6931472
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0115599
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bulb
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
171
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0129585
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6931472
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0089821
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
coil
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
166
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0125796
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6931472
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0087195
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
high
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
166
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0125796
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
body
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
156
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0143739
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
one
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
156
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0118218
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
reference
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0138211
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
tube
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
147
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0111397
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Notice that &lt;em&gt;idf&lt;/em&gt; and thus &lt;em&gt;tf-idf&lt;/em&gt; are zero for extremely common words (like “may”). These are all words that appear in both documents, so the idf term (which will then be the natural log of 1) is zero. The inverse document frequency (and thus tf-idf) is very low (near zero) for words that occur in many of the documents in a collection; this is how this approach decreases the weight for common words. The inverse document frequency will be a higher number for words that occur in fewer of the documents in the collection.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tokenizing-by-n-gram&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3&lt;/span&gt; Tokenizing by n-gram&lt;/h2&gt;
&lt;p&gt;We’ve been using the &lt;code&gt;unnest_tokens&lt;/code&gt; function to tokenize by word, or sometimes by sentence, which is useful for the kinds of frequency analyses we’ve been doing so far. But we can also use the function to tokenize into consecutive sequences of words, called &lt;strong&gt;n-grams&lt;/strong&gt;. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidytext)

einstein_bigrams &amp;lt;- books %&amp;gt;%
  filter(author == &amp;quot;Einstein, Albert&amp;quot;) %&amp;gt;% 
  unnest_tokens(bigram, text, token = &amp;quot;ngrams&amp;quot;, n = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
author
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
document
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
bigram
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3797
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3798
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3799
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3800
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3801
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
relativity the
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3801
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
the special
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3801
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
special and
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3801
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
and general
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3801
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
general theory
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Einstein, Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3802
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can examine the most common bigrams using dplyr’s &lt;code&gt;count()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;einstein_bigrams_count &amp;lt;- einstein_bigrams %&amp;gt;% 
    count(bigram, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
bigram
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
916
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
of the
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
613
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
to the
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
247
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
in the
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
197
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
of relativity
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
164
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
theory of
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
121
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
with the
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
119
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
on the
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
111
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
that the
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
110
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
of a
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
98
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now we use tidyr’s &lt;code&gt;separate()&lt;/code&gt;, which splits a column into multiple columns based on a delimiter. This lets us separate it into two columns, “word1” and “word2”, at which point we can remove cases where either is a stop-word. This time, we use the stopwords from the package &lt;code&gt;tidyr&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)

# seperate words
bigrams_separated &amp;lt;- einstein_bigrams %&amp;gt;%
  separate(bigram, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;)

# filter stop words and NA
bigrams_filtered &amp;lt;- bigrams_separated %&amp;gt;%
  filter(!word1 %in% stop_words$word) %&amp;gt;%
  filter(!word2 %in% stop_words$word) %&amp;gt;% 
  filter(!is.na(word1))

# new bigram counts:
bigram_counts &amp;lt;- bigrams_filtered %&amp;gt;% 
  count(word1, word2, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word1
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word2
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
reference
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
body
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
56
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
gravitational
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
field
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
special
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
theory
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ordinate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
system
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
34
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
space
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
time
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
27
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
classical
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
mechanics
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
lorentz
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
transformation
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
measuring
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
rods
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
straight
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
line
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
rigid
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
body
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This one-bigram-per-row format is helpful for exploratory analyses of the text. As a simple example, we might be interested in the most often mentioned “theory”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bigram_theory &amp;lt;- bigrams_filtered %&amp;gt;%
  filter(word2 == &amp;quot;theory&amp;quot;) %&amp;gt;%
  count(word1, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
special
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
lorentz
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
newton’s
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
_special
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
comprehensive
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
electrodynamic
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
electromagnetic
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In other analyses you may be interested in the most common trigrams, which are consecutive sequences of 3 words. We can find this by setting n = 3:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trigram &amp;lt;- books %&amp;gt;%
  unnest_tokens(trigram, text, token = &amp;quot;ngrams&amp;quot;, n = 3) %&amp;gt;%
  separate(trigram, c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;, &amp;quot;word3&amp;quot;), sep = &amp;quot; &amp;quot;) %&amp;gt;%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word,  
         !is.na(word1)) %&amp;gt;%
  count(word1, word2, word3, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word1
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word2
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
word3
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
_x_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
_x_2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
_x_3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
light
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
_in
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
vacuo_
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
reference
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
body
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;em&gt;k&lt;/em&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
space
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
time
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
continuum
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
_x_2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
_x_3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
_x_4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
reference
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
body
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
_k
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
disruptive
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
discharge
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
coil
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;network-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.4&lt;/span&gt; Network analysis&lt;/h2&gt;
&lt;p&gt;We may be interested in visualizing all of the relationships among words simultaneously, rather than just the top few at a time. As one common visualization, we can arrange the words into a network, or “graph.” Here we’ll be referring to a “graph” not in the sense of a visualization, but as a combination of connected nodes. A graph can be constructed from a tidy object since it has three variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;from: the node an edge is coming from&lt;/li&gt;
&lt;li&gt;to: the node an edge is going towards&lt;/li&gt;
&lt;li&gt;weight: A numeric value associated with each edge&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;igraph&lt;/code&gt; package has many functions for manipulating and analyzing networks. One way to create an igraph object from tidy data is the &lt;code&gt;graph_from_data_frame()&lt;/code&gt; function, which takes a data frame of edges with columns for “from”, “to”, and edge attributes (in this case n):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(igraph)

# filter for only relatively common combinations
bigram_graph &amp;lt;- bigram_counts %&amp;gt;%
  filter(n &amp;gt; 5) %&amp;gt;%
  graph_from_data_frame()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the &lt;a href=&#34;https://cran.r-project.org/web/packages/ggraph/ggraph.pdf&#34;&gt;&lt;code&gt;ggraph&lt;/code&gt;&lt;/a&gt; package to convert the igraph object into a &lt;code&gt;ggraph&lt;/code&gt; with the ggraph function, after which we add layers to it, much like layers are added in ggplot2. For example, for a basic graph we need to add three layers: nodes, edges, and text:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggraph)
set.seed(123)

ggraph(bigram_graph, layout = &amp;quot;fr&amp;quot;) +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-09-16-r-text-mining/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, we will change some settings to obtain to a better looking graph:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We add the &lt;code&gt;edge_alpha&lt;/code&gt; aesthetic to the link layer to make links transparent based on how common or rare the bigram is.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We add directionality with an arrow, constructed using &lt;code&gt;grid::arrow()&lt;/code&gt;, including an &lt;code&gt;end_cap&lt;/code&gt; option that tells the arrow to end before touching the node.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We tinker with the options to the node layer to make the nodes more attractive (larger, blue points).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We add a theme that’s useful for plotting networks, &lt;code&gt;theme_void()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggraph)
set.seed(123)

a &amp;lt;- grid::arrow(type = &amp;quot;closed&amp;quot;, length = unit(.15, &amp;quot;inches&amp;quot;))

ggraph(bigram_graph, layout = &amp;quot;fr&amp;quot;) +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, &amp;#39;inches&amp;#39;)) +
  geom_node_point(color = &amp;quot;lightblue&amp;quot;, size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-09-16-r-text-mining/index_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;classification-with-logistic-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Classification with logistic regression&lt;/h1&gt;
&lt;p&gt;In the first part we will build a statistical learning model. In the second part we will want to test it and assess its quality. Without dividing the dataset we would test the model on the data which the algorithm have already seen, which is why we start by splitting the data.&lt;/p&gt;
&lt;div id=&#34;train-test-split&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Train test split&lt;/h2&gt;
&lt;p&gt;Let’s go back to the original &lt;code&gt;books&lt;/code&gt; dataset (not the &lt;code&gt;tidy_books&lt;/code&gt; dataset) because the lines of text are our individual observations.&lt;/p&gt;
&lt;p&gt;We could use functions from the &lt;a href=&#34;https://tidymodels.github.io/rsample/&#34;&gt;&lt;code&gt;rsample&lt;/code&gt;&lt;/a&gt; package to generate resampled datasets, but the specific modeling approach we’re going to use will do that for us so we only need a simple train/test split.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rsample)

books_split &amp;lt;- books %&amp;gt;%
  select(document) %&amp;gt;%
  initial_split(prop = 3/4)

train_data &amp;lt;- training(books_split)
test_data &amp;lt;- testing(books_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that we just select specific text rows (column &lt;code&gt;document&lt;/code&gt;) for training and others for our test data (we set the proportion of data to be retained for modeling/analysis to 3/4) without selecting the actual text lines at this point.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-data-sparse-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Training data (sparse matrix)&lt;/h2&gt;
&lt;p&gt;Now we want to transform our training data from a tidy data structure to a “sparse matrix” (these objects can be treated as though they were matrices, for example accessing particular rows and columns, but are stored in a more efficient format) to use for our classification algorithm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytext)

sparse_words &amp;lt;- tidy_books %&amp;gt;%
  count(document, word) %&amp;gt;%
  inner_join(train_data, by = &amp;quot;document&amp;quot;) %&amp;gt;%
  cast_sparse(document, word, n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(sparse_words)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4782  892&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have over 4,700 training observations and almost 900 features. Text feature space handled in this way is very high dimensional, so we need to take that into account when considering our modeling approach.&lt;/p&gt;
&lt;p&gt;One reason this overall approach is flexible is that you could at this point &lt;code&gt;cbind()&lt;/code&gt; other columns, such as non-text numeric data, onto this sparse matrix. Then you can use this combination of text and non-text data as your predictors in the classifiaction algorithm, and the regularized regression algorithm we are going to use will find which are important for your problem space.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;response-variable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; Response variable&lt;/h2&gt;
&lt;p&gt;We also need to build a tibble with a &lt;strong&gt;response variable&lt;/strong&gt; to associate each of the &lt;code&gt;rownames()&lt;/code&gt; of the sparse matrix with an author, to use as the quantity we will predict in the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;word_rownames &amp;lt;- as.integer(rownames(sparse_words))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;books_joined &amp;lt;- tibble(document = word_rownames) %&amp;gt;%
  left_join(books  %&amp;gt;%
    select(document, author))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: condensedpx; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
document
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
author
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tesla, Nikola
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.4&lt;/span&gt; Logistic regression model&lt;/h2&gt;
&lt;p&gt;Now it’s time to train our classification model. Let’s use the &lt;code&gt;glmnet&lt;/code&gt; package to fit a logistic regression model with &lt;em&gt;lasso&lt;/em&gt; (least absolute shrinkage and selection operator; also Lasso or LASSO) regularization. This regression analysis method performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Glmnet&lt;/code&gt; is a package that fits lasso models via penalized maximum likelihood. We do not cover the method and glmnet package in detail at this point, but if you want to learn more about glmnet and lasso regression, review the following resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.pdf&#34;&gt;Introduction to glmnet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/glmnet/glmnet.pdf&#34;&gt;glmnet documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kirenz.com/post/2019-08-12-python-lasso-regression-auto/&#34;&gt;LASSO regression in Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The package is very useful for text classification because the variable selection that lasso regularization performs can tell you which words are important for your prediction problem. The glmnet package also supports parallel processing, so we can train on multiple cores with &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&#34;&gt;cross-validation&lt;/a&gt; on the training set using &lt;code&gt;cv.glmnet()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(glmnet)
library(doMC)
registerDoMC(cores = 8)

is_einstein &amp;lt;- books_joined$author == &amp;quot;Einstein, Albert&amp;quot;

model &amp;lt;- cv.glmnet(sparse_words, 
                   is_einstein,
                   family = &amp;quot;binomial&amp;quot;,
                   parallel = TRUE, 
                   keep = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s use the package &lt;a href=&#34;https://cran.r-project.org/web/packages/broom/vignettes/broom.html&#34;&gt;&lt;code&gt;broom&lt;/code&gt;&lt;/a&gt; (the broom package takes the messy output of built-in functions in R, such as lm, nls, or t.test, and turns them into tidy data frames) to check out the coefficients of the model, for the largest value of lambda with error within 1 standard error of the minimum (&lt;code&gt;lambda.1se&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)

coefs &amp;lt;- model$glmnet.fit %&amp;gt;%
  tidy() %&amp;gt;%
  filter(lambda == model$lambda.1se)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which coefficents are the largest in size, in each direction:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(forcats)

coefs %&amp;gt;%
  group_by(estimate &amp;gt; 0) %&amp;gt;%
  top_n(10, abs(estimate)) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate &amp;gt; 0)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  coord_flip() +
  labs(
    x = NULL,
    title = &amp;quot;Coefficients that increase/decrease probability the most&amp;quot;,
    subtitle = &amp;quot;A document mentioning lecture or probably is unlikely to be written by Albert Einstein&amp;quot;
  ) +
  theme_classic(base_size = 12) +
  theme(plot.title = element_text(lineheight=.8, face=&amp;quot;bold&amp;quot;)) +
  scale_fill_brewer()  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-09-16-r-text-mining/index_files/figure-html/unnamed-chunk-44-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-evaluation-with-test-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.5&lt;/span&gt; Model evaluation with test data&lt;/h2&gt;
&lt;p&gt;Now we want to evaluate how well this model is doing using the test data that we held out and did not use for training the model. Let’s create a dataframe that tells us, for each document in the test set, the probability of being written by Albert Einstein.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;intercept &amp;lt;- coefs %&amp;gt;%
  filter(term == &amp;quot;(Intercept)&amp;quot;) %&amp;gt;%
  pull(estimate)

classifications &amp;lt;- tidy_books %&amp;gt;%
  inner_join(test_data) %&amp;gt;%
  inner_join(coefs, by = c(&amp;quot;word&amp;quot; = &amp;quot;term&amp;quot;)) %&amp;gt;%
  group_by(document) %&amp;gt;%
  summarize(score = sum(estimate)) %&amp;gt;%
  mutate(probability = plogis(intercept + score))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: condensedpx; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
document
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
score
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
probability
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.3811800
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2063129
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.9929541
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1235678
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2522803
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7834973
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.8746267
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1369635
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
52
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-5.1987683
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0056813
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
54
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.8148527
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0583613
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2272565
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5649167
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now let’s use the &lt;a href=&#34;https://tidymodels.github.io/yardstick/&#34;&gt;&lt;code&gt;yardstick&lt;/code&gt;&lt;/a&gt; package (yardstick is a package to estimate how well models are working using tidy data principles) to calculate some model performance metrics. For example, what does the &lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc&#34;&gt;ROC curve&lt;/a&gt; (receiver operating characteristic curve - a graph showing the performance of a classification model at all classification thresholds) look like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(yardstick)

comment_classes &amp;lt;- classifications %&amp;gt;%
  left_join(books %&amp;gt;%
    select(author, document), by = &amp;quot;document&amp;quot;) %&amp;gt;%
  mutate(author = as.factor(author))

comment_classes %&amp;gt;%
  roc_curve(author, probability) %&amp;gt;%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(
    color = &amp;quot;midnightblue&amp;quot;,
    size = 1.5
  ) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = &amp;quot;gray50&amp;quot;,
    size = 1.2
  ) +
  labs(
    title = &amp;quot;ROC curve for text classification using regularized regression&amp;quot;,
    subtitle = &amp;quot;Predicting whether text was written by Albert Einstein or Nikola Tesla&amp;quot;
  ) +
  theme_classic(base_size = 12) +
  theme(plot.title = element_text(lineheight=.8, face=&amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-09-16-r-text-mining/index_files/figure-html/unnamed-chunk-47-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s obtain the accuracy (AUC - the fraction of predictions that a classification model got right) on the test data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auc &amp;lt;- comment_classes %&amp;gt;%
  roc_auc(author, probability)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: condensedpx; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
.metric
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
.estimator
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
.estimate
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
roc_auc
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
binary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9757987
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Next we turn to the &lt;strong&gt;confusion matrix&lt;/strong&gt;. Let’s make the following definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“Einstein, Albert” is a positive class.&lt;/li&gt;
&lt;li&gt;“Tesla, Nikola” is a negative class.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;span style=&#34;color:green&#34;&gt; &lt;strong&gt;True Positive (TP):&lt;/strong&gt; &lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span style=&#34;color:red&#34;&gt; &lt;strong&gt;False Positive (FP):&lt;/strong&gt; &lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;strong&gt;Reality&lt;/strong&gt;: Text is from Einstein&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Reality&lt;/strong&gt;: Text is from Tesla&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;: Text is from Einstein&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;: Text is from Einstein&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;span style=&#34;color:red&#34;&gt; &lt;strong&gt;False Negative (FN):&lt;/strong&gt; &lt;/span&gt;&lt;/th&gt;
&lt;th&gt;&lt;span style=&#34;color:green&#34;&gt; &lt;strong&gt;True Negative (TN):&lt;/strong&gt; &lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;strong&gt;Reality&lt;/strong&gt;: Text is from Einstein&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Reality&lt;/strong&gt;: Text is from Tesla&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;: Text is from Tesla&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;: Text is from Tesla&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can summarize our “einstein-text-prediction” model using a 2x2 confusion matrix that depicts all four possible outcomes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;em&gt;true positive&lt;/em&gt; is an outcome where the model correctly predicts the positive class (Einstein). Similarly, a &lt;em&gt;true negative&lt;/em&gt; is an outcome where the model correctly predicts the negative class (Tesla).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A &lt;em&gt;false positive&lt;/em&gt; is an outcome where the model incorrectly predicts the positive class. And a &lt;em&gt;false negative&lt;/em&gt; is an outcome where the model incorrectly predicts the negative class.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s use a probability of 0.5 as our threshold. That means all model predictions with a probability greater than 50% get labeld as beeing text from Einstein:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;comment_classes %&amp;gt;%
  mutate(prediction = case_when(
          probability &amp;gt; 0.5 ~ &amp;quot;Einstein, Albert&amp;quot;,
          TRUE ~ &amp;quot;Tesla, Nikola&amp;quot;),
        prediction = as.factor(prediction)) %&amp;gt;%
  conf_mat(author, prediction)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   Truth
## Prediction         Einstein, Albert Tesla, Nikola
##   Einstein, Albert              628            58
##   Tesla, Nikola                  70           784&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a closer look at these misclassifications: false negatives (FN) and false positives (FP). Which documents here were incorrectly predicted to be written by Albert Einstein, at the extreme probability end of greater than 80% (false positive)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FP&amp;lt;- comment_classes %&amp;gt;%
  filter(probability &amp;gt; .8,
          author == &amp;quot;Tesla, Nikola&amp;quot;) %&amp;gt;%
  sample_n(10) %&amp;gt;%
  inner_join(books %&amp;gt;%
  select(document, text)) %&amp;gt;%
  select(probability, text)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: condensedpx; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
probability
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
text
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8189629
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
through things. He is an omnivorous reader, who never forgets; and he
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9012553
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
our sense of vision.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8094770
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
enormous distance without affecting greatly the character of the
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9058630
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
experience of to-day enables us to see clearly why these coils under
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8509898
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
discharger I have been able to maintain an oscillating motion without
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8119290
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
little thought leads us to the conclusion that, could we but reach
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9086652
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
disc, which could be seen from a considerable distance, such is the
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9440000
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
obtainable at any point of the universe. This idea is not novel. Men
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9069282
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Leaving practicability out of consideration, this, then, would be the
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8595897
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
plant, and on returning to Paris sought to carry out a number of ideas
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These documents were incorrectly predicted to be written by Albert Einstein. However, they were written by Nikola Tesla.&lt;/p&gt;
&lt;p&gt;Finally, let’s take a look at the texts which are from Albert Einstein that the model did not correctly identify (false negative):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FN &amp;lt;- comment_classes %&amp;gt;%
  filter(probability &amp;lt; .3,
         author == &amp;quot;Einstein, Albert&amp;quot;) %&amp;gt;%
  sample_n(10) %&amp;gt;%
  inner_join(books %&amp;gt;%
  select(document, text)) %&amp;gt;%
  select(probability, text)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;font-size: condensedpx; width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
probability
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
text
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0969140
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
be arbitrary, although it was always tacitly made even before the
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1989692
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
strings to the floor, otherwise the slightest impact against the floor
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1994746
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
local variations of temperature, and with which we made acquaintance as
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1932809
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
the conservation of energy (and of impulse).
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0546119
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
me—and rightly so—and you declare: “I maintain my previous definition
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0613870
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
permits of our answering it with a moderate degree of certainty, and in
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2458622
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
treated in detail and with unsurpassable lucidity by Helmholtz and
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1886392
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
gravitational potential, then the study of this displacement will
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1570832
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
for the following reason. As a result of the more careful study of
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0134175
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
throwing it. Then, disregarding the influence of the air resistance, I
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can conclude that the model did a very good job in predicting the authors of the texts. Furthermore, the texts of the misclassifications are quite short and we can imagine, that even a human reader who is familiar with the work of Einstein and Tesla would have difficulties to classify them correctly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Programming Languages for Data Science</title>
      <link>https://kirenz.com/project/programming-languages/</link>
      <pubDate>Tue, 03 Sep 2019 10:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/programming-languages/</guid>
      <description>


&lt;div id=&#34;agenda&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Agenda&lt;/h1&gt;
&lt;div id=&#34;einfuhrung&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Einführung&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.kirenz.com/slides/intro-business-intelligence/assets/player/keynotedhtmlplayer#0&#34;&gt;Einführung in Business Intelligence&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.kirenz.com/slides/markdown-first-steps/markdown-guide.html#1&#34;&gt;First steps in Markdown&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;sql&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SQL&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Datenexploration (Selektieren, Ordnen und Filtern)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Datentypen und Datentransformationen&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Gruppieren und Aggregieren&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tabellen verbinden (Joins)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Tabellen modifizieren&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Subqueries&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kirenz/microsoft_azure_sql_database&#34;&gt;Microsoft Azure SQL Database&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Literatur:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;DeBarros, A. (2018). Practical SQL: A Beginner’s Guide to Storytelling with Data. No Starch Press.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;r-for-data-science&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R for Data Science&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First Steps in R&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.kirenz.com/slides/r-for-data-science/assets/player/keynotedhtmlplayer#0&#34;&gt;Introduction to Data Science with R&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Basic Analytics in R&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data Exploration in R&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Correlation Analysis in R&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Natural Language Processing with R&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Advanced Programming in R&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Erstellung von interaktiven Tutorials in R.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Literatur:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://r4ds.had.co.nz&#34;&gt;Wickham, H., &amp;amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://adv-r.hadley.nz&#34;&gt;Wickham, H. (2019). Advanced r. Chapman and Hall/CRC.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.tidytextmining.com&#34;&gt;Silge, J., &amp;amp; Robinson, D. (2017). Text mining with R: A tidy approach. “O’Reilly Media, Inc.”&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://bookdown.org/yihui/bookdown/&#34;&gt;Xie, Y. (2019). Bookdown: Authoring Books and Technical Documents with R Markdown. Chapman and Hall/CRC.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Social Network Analysis with Python</title>
      <link>https://kirenz.com/post/2019-08-13-network_analysis/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/post/2019-08-13-network_analysis/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#social-network-analysis-with-networkx-in-python&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Social Network Analysis with NetworkX in Python&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#social-network-basics&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1&lt;/span&gt; Social Network Basics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#symmetric-networks-undirected&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1.1&lt;/span&gt; Symmetric Networks (undirected)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#asymmetric-networks-directed&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1.2&lt;/span&gt; Asymmetric Networks (directed)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weighted-networks&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1.3&lt;/span&gt; Weighted Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clustering-coefficient&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2&lt;/span&gt; Clustering coefficient&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#network-distance-measures&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3&lt;/span&gt; Network Distance Measures&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#degree&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3.1&lt;/span&gt; Degree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#distance&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3.2&lt;/span&gt; Distance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#breadth-first-search&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3.3&lt;/span&gt; Breadth-first search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eccentricity&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3.4&lt;/span&gt; Eccentricity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#centrality-measures&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.4&lt;/span&gt; Centrality measures&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#degree-centrality&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.4.1&lt;/span&gt; Degree Centrality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eigenvector-centrality&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.4.2&lt;/span&gt; Eigenvector Centrality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#closeness-centrality&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.4.3&lt;/span&gt; Closeness Centrality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#betweenness-centrality&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.4.4&lt;/span&gt; Betweenness Centrality&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#facebook-case-study&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.5&lt;/span&gt; Facebook Case Study&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;social-network-analysis-with-networkx-in-python&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Social Network Analysis with NetworkX in Python&lt;/h1&gt;
&lt;p&gt;We use the module &lt;a href=&#34;https://networkx.github.io/documentation/stable/&#34;&gt;NetworkX&lt;/a&gt; in this tutorial. It is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.&lt;/p&gt;
&lt;p&gt;If you work with &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;Anaconda&lt;/a&gt;, you can install the package as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install -c anaconda networkx&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Import modules:&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import networkx as nx
import matplotlib.pyplot as plt
%matplotlib inline
import warnings; warnings.simplefilter(&amp;#39;ignore&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;div id=&#34;social-network-basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Social Network Basics&lt;/h2&gt;
&lt;p&gt;Each network consists of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nodes: The individuals whose network we are building.&lt;/li&gt;
&lt;li&gt;Edges: The connection between the nodes. It represents a relationship between the nodes of the network.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;symmetric-networks-undirected&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1.1&lt;/span&gt; Symmetric Networks (undirected)&lt;/h3&gt;
&lt;p&gt;The first network that we create is a group of people who work together. This is called a &lt;strong&gt;symmetric network&lt;/strong&gt; because the relationship “working together” is a symmetric relationship: If A is related to B, B is also related to A.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;G_symmetric = nx.Graph()

G_symmetric.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;Laura&amp;#39;)
G_symmetric.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;Marc&amp;#39;)
G_symmetric.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;John&amp;#39;)
G_symmetric.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;Michelle&amp;#39;)
G_symmetric.add_edge(&amp;#39;Laura&amp;#39;,   &amp;#39;Michelle&amp;#39;)
G_symmetric.add_edge(&amp;#39;Michelle&amp;#39;,&amp;#39;Marc&amp;#39;)
G_symmetric.add_edge(&amp;#39;George&amp;#39;,  &amp;#39;John&amp;#39;)
G_symmetric.add_edge(&amp;#39;George&amp;#39;,  &amp;#39;Steven&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(nx.info(G_symmetric))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Name:
Type: Graph
Number of nodes: 6
Number of edges: 8
Average degree:   2.6667&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we visualize the network with the &lt;code&gt;draw_networkx()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.figure(figsize=(5,5))
nx.draw_networkx(G_symmetric);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-08-13-network_analysis/output_8_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;asymmetric-networks-directed&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1.2&lt;/span&gt; Asymmetric Networks (directed)&lt;/h3&gt;
&lt;p&gt;What if the relationship between nodes is ‘child of’, then the relationship is no longer symmetric. This is the case if someone follows someone else on Twitter. Or in the case of hyperlinks.&lt;/p&gt;
&lt;p&gt;If A is the child of B, then B is not a child of A. Such a network where the relationship is &lt;strong&gt;asymmetric&lt;/strong&gt; (A is related to B, does not necessarily means that B is associated with A) is called an Asymmetric network.&lt;/p&gt;
&lt;p&gt;We can build the asymmetric network in NetworkX using &lt;code&gt;DiGraph&lt;/code&gt; method, which is short of &lt;strong&gt;Directional Graph&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;G_asymmetric = nx.DiGraph()
G_asymmetric.add_edge(&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;)
G_asymmetric.add_edge(&amp;#39;A&amp;#39;,&amp;#39;D&amp;#39;)
G_asymmetric.add_edge(&amp;#39;C&amp;#39;,&amp;#39;A&amp;#39;)
G_asymmetric.add_edge(&amp;#39;D&amp;#39;,&amp;#39;E&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make sure that all nodes are distinctly visible in the network, use the &lt;code&gt;spring_layout()&lt;/code&gt; function, followed by the &lt;code&gt;draw_networkx()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.spring_layout(G_asymmetric)
nx.draw_networkx(G_asymmetric)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-08-13-network_analysis/output_12_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weighted-networks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1.3&lt;/span&gt; Weighted Networks&lt;/h3&gt;
&lt;p&gt;Till now we had networks without weights, but it is possible that networks are made with weights, for example, if in our initial network we consider the number of projects done together as a weight, we will get a weighted Network.&lt;/p&gt;
&lt;p&gt;Let us make one again of the employees, but this time we add weight to the network, each edge has a weight signifying the number of projects they have done together.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;G_weighted = nx.Graph()

G_weighted.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;Laura&amp;#39;,   weight=25)
G_weighted.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;Marc&amp;#39;,    weight=8)
G_weighted.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;John&amp;#39;,    weight=11)
G_weighted.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;Michelle&amp;#39;,weight=1)
G_weighted.add_edge(&amp;#39;Laura&amp;#39;,   &amp;#39;Michelle&amp;#39;,weight=1)
G_weighted.add_edge(&amp;#39;Michelle&amp;#39;,&amp;#39;Marc&amp;#39;,    weight=1)
G_weighted.add_edge(&amp;#39;George&amp;#39;,  &amp;#39;John&amp;#39;,    weight=8)
G_weighted.add_edge(&amp;#39;George&amp;#39;,  &amp;#39;Steven&amp;#39;,  weight=4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;elarge = [(u, v) for (u, v, d) in G_weighted.edges(data=True) if d[&amp;#39;weight&amp;#39;] &amp;gt; 8]
esmall = [(u, v) for (u, v, d) in G_weighted.edges(data=True) if d[&amp;#39;weight&amp;#39;] &amp;lt;= 8]

pos = nx.circular_layout(G_weighted)  # positions for all nodes

# nodes
nx.draw_networkx_nodes(G_weighted, pos, node_size=700)

# edges
nx.draw_networkx_edges(G_weighted, pos, edgelist=elarge,width=6)
nx.draw_networkx_edges(G_weighted, pos, edgelist=esmall,width=6, alpha=0.5, edge_color=&amp;#39;b&amp;#39;, style=&amp;#39;dashed&amp;#39;)

# labels
nx.draw_networkx_labels(G_weighted, pos, font_size=20, font_family=&amp;#39;sans-serif&amp;#39;)

plt.axis(&amp;#39;off&amp;#39;)
plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-08-13-network_analysis/output_15_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;clustering-coefficient&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Clustering coefficient&lt;/h2&gt;
&lt;p&gt;It is observed that people who share connections in a social network tend to form associations. In other words, there is a tendency in a social network to form clusters.&lt;/p&gt;
&lt;p&gt;We can determine the clusters of a node, &lt;strong&gt;local clustering coefficient&lt;/strong&gt;, which is the fraction of pairs of the node’s friends (that is connections) that are connected with each other.&lt;/p&gt;
&lt;p&gt;To determine the local clustering coefficient, we make use of &lt;code&gt;nx.clustering(Graph, Node)&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;In the symmetric employee-network, you will find that Michelle has a local clustering coefficient of 0.67 and Laura has a local clustering coefficient of 1.&lt;/p&gt;
&lt;p&gt;The average clustering coefficient (sum of all the local clustering coefficients divided by the number of nodes) for the symmetric employee-network is 0.867.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.clustering(G_symmetric,&amp;#39;Michelle&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.6666666666666666&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.clustering(G_symmetric,&amp;#39;Laura&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.average_clustering(G_symmetric)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.8277777777777778&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;network-distance-measures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; Network Distance Measures&lt;/h2&gt;
&lt;div id=&#34;degree&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3.1&lt;/span&gt; Degree&lt;/h3&gt;
&lt;p&gt;Degree of a node defines the number of connections a node has. NetworkX has the function &lt;code&gt;degree&lt;/code&gt; which we can use to determine the degree of a node in the network.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.degree(G_symmetric, &amp;#39;Michelle&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will return a value of 3, as Michelle has worked with three employees in the network.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3.2&lt;/span&gt; Distance&lt;/h3&gt;
&lt;p&gt;We can also determine the shortest path between two nodes and its length in NetworkX using &lt;code&gt;nx.shortest_path(Graph, Node1, Node2)&lt;/code&gt; and &lt;code&gt;nx.shortest_path_length(Graph, Node1, Node2)&lt;/code&gt;
functions respectively.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.shortest_path(G_symmetric, &amp;#39;Michelle&amp;#39;, &amp;#39;John&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;Michelle&amp;#39;, &amp;#39;Steven&amp;#39;, &amp;#39;John&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.shortest_path_length(G_symmetric, &amp;#39;Michelle&amp;#39;, &amp;#39;John&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;breadth-first-search&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3.3&lt;/span&gt; Breadth-first search&lt;/h3&gt;
&lt;p&gt;We can find the distance of a node from every other node in the network using breadth-first search algorithm, starting from that node. networkX provides the function bfs_tree to do it.&lt;/p&gt;
&lt;p&gt;And so if you use &lt;code&gt;M = nx.bfs_tree(G_symmetric, &#39;Michelle&#39;)&lt;/code&gt; and now draw this tree, we will get a network structure telling how we can reach other nodes of the network starting from Michelle .&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;S = nx.bfs_tree(G_symmetric, &amp;#39;Steven&amp;#39;)
nx.draw_networkx(S)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-08-13-network_analysis/output_29_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;M = nx.bfs_tree(G_symmetric, &amp;#39;Michelle&amp;#39;)
nx.draw_networkx(M)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-08-13-network_analysis/output_30_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;eccentricity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3.4&lt;/span&gt; Eccentricity&lt;/h3&gt;
&lt;p&gt;Eccentricity of a node A is defined as the largest distance between A and all other nodes.&lt;/p&gt;
&lt;p&gt;It can be found using &lt;code&gt;nx.eccentricity()&lt;/code&gt; function. In the symmetric employee-network, Michelle has an eccentricity of 2, and Steven has an eccentricity of 1 (he is connected to every other node).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.eccentricity(G_symmetric,&amp;#39;Michelle&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.eccentricity(G_symmetric,&amp;#39;Steven&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;centrality-measures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4&lt;/span&gt; Centrality measures&lt;/h2&gt;
&lt;p&gt;Above we learned some of the network distance measures and they are useful in knowing how the information will spread through the network.&lt;/p&gt;
&lt;p&gt;In this section, we will learn how to find the most important nodes (individuals) in the network. These parameters are called as &lt;strong&gt;centrality measures&lt;/strong&gt;. Centrality Measures can help us in identifying popularity, most liked, and biggest influencers within the network.&lt;/p&gt;
&lt;div id=&#34;degree-centrality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4.1&lt;/span&gt; Degree Centrality&lt;/h3&gt;
&lt;p&gt;The people most popular or more liked usually are the ones who have more friends.&lt;/p&gt;
&lt;p&gt;Degree centrality is a measure of the number of connections a particular node has in the network. It is based on the fact that important nodes have many connections. NetworkX has the function &lt;code&gt;degree_centrality()&lt;/code&gt; to calculate the degree centrality of all the nodes of a network.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.degree_centrality(G_symmetric)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;#39;Steven&amp;#39;: 1.0,
 &amp;#39;Laura&amp;#39;: 0.4,
 &amp;#39;Marc&amp;#39;: 0.4,
 &amp;#39;John&amp;#39;: 0.4,
 &amp;#39;Michelle&amp;#39;: 0.6000000000000001,
 &amp;#39;George&amp;#39;: 0.4}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;eigenvector-centrality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4.2&lt;/span&gt; Eigenvector Centrality&lt;/h3&gt;
&lt;p&gt;It is not just how many individuals one is connected too, but the type of people one is connected with that can decide the importance of a node.&lt;/p&gt;
&lt;p&gt;Eigenvector centrality is a measure of how import a node is by accounting for the fact of how well it is connected to other important nodes.&lt;/p&gt;
&lt;p&gt;We can use the &lt;code&gt;eigenvector_centrality()&lt;/code&gt; function of NetworkX to calculate eigenvector centrality of all the nodes in a network.&lt;/p&gt;
&lt;p&gt;The Google’s Pagerank algorithm is a variant of Eigenvector centrality algorithm.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.eigenvector_centrality(G_symmetric)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;#39;Steven&amp;#39;: 0.6006686104947806,
 &amp;#39;Laura&amp;#39;: 0.3545677660798074,
 &amp;#39;Marc&amp;#39;: 0.3545677660798074,
 &amp;#39;John&amp;#39;: 0.30844592433424667,
 &amp;#39;Michelle&amp;#39;: 0.4443904166426225,
 &amp;#39;George&amp;#39;: 0.30844592433424667}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;closeness-centrality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4.3&lt;/span&gt; Closeness Centrality&lt;/h3&gt;
&lt;p&gt;Closeness Centrality is a measure where each node’s importance is determined by &lt;strong&gt;closeness to all other nodes&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.closeness_centrality(G_symmetric)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;#39;Steven&amp;#39;: 1.0,
 &amp;#39;Laura&amp;#39;: 0.625,
 &amp;#39;Marc&amp;#39;: 0.625,
 &amp;#39;John&amp;#39;: 0.625,
 &amp;#39;Michelle&amp;#39;: 0.7142857142857143,
 &amp;#39;George&amp;#39;: 0.625}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;betweenness-centrality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4.4&lt;/span&gt; Betweenness Centrality&lt;/h3&gt;
&lt;p&gt;The Betweenness Centrality is the centrality of control.&lt;/p&gt;
&lt;p&gt;It represents the frequency at which a point occurs on the &lt;strong&gt;shortest paths&lt;/strong&gt; that connected pair of points. It quantifies how many times a particular node comes in the shortest chosen path between two other nodes.&lt;/p&gt;
&lt;p&gt;The nodes with high betweenness centrality play a significant role in the communication/information flow within the network.&lt;/p&gt;
&lt;p&gt;The nodes with high betweenness centrality can have a strategic control and influence on others. An individual at such a strategic position can influence the whole group, by either withholding or coloring the information in transmission.&lt;/p&gt;
&lt;p&gt;Networkx has the function &lt;code&gt;betweenness_centrality()&lt;/code&gt; to measure it for the network. It has options to select if we want betweenness values to be normalized or not, weights to be included in centrality calculation or not, and to include the endpoints in the shortest path counts or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.betweenness_centrality(G_symmetric)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;#39;Steven&amp;#39;: 0.65,
 &amp;#39;Laura&amp;#39;: 0.0,
 &amp;#39;Marc&amp;#39;: 0.0,
 &amp;#39;John&amp;#39;: 0.0,
 &amp;#39;Michelle&amp;#39;: 0.05,
 &amp;#39;George&amp;#39;: 0.0}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pos = nx.spring_layout(G_symmetric)
betCent = nx.betweenness_centrality(G_symmetric, normalized=True, endpoints=True)
node_color = [20000.0 * G_symmetric.degree(v) for v in G_symmetric]
node_size =  [v * 10000 for v in betCent.values()]
plt.figure(figsize=(10,10))
nx.draw_networkx(G_symmetric, pos=pos, with_labels=True,
                 node_color=node_color,
                 node_size=node_size )
plt.axis(&amp;#39;off&amp;#39;);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-08-13-network_analysis/output_45_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sorted(betCent, key=betCent.get, reverse=True)[:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;Steven&amp;#39;, &amp;#39;Michelle&amp;#39;, &amp;#39;Laura&amp;#39;, &amp;#39;Marc&amp;#39;, &amp;#39;John&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;facebook-case-study&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.5&lt;/span&gt; Facebook Case Study&lt;/h2&gt;
&lt;p&gt;This dataset consists of ‘circles’ (or ‘friends lists’) from Facebook. Facebook data was collected from survey participants using this Facebook app. The dataset includes node features (profiles), circles, and ego networks.&lt;/p&gt;
&lt;p&gt;Facebook data has been anonymized by replacing the Facebook-internal ids for each user with a new value. Also, while feature vectors from this dataset have been provided, the interpretation of those features has been obscured. For instance, where the original dataset may have contained a feature “political=Democratic Party”, the new data would simply contain “political=anonymized feature 1”. Thus, using the anonymized data it is possible to determine whether two users have the same political affiliations, but not what their individual political affiliations represent.&lt;/p&gt;
&lt;p&gt;Source: &lt;a href=&#34;https://snap.stanford.edu/data/egonets-Facebook.html&#34;&gt;J. McAuley and J. Leskovec. Learning to Discover Social Circles in Ego Networks. NIPS, 2012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let us start with the Facebook data, for our analysis here we will use Facebook combined ego networks dataset, it contains the aggregated network of ten individuals’ Facebook friends list. You can download the required facebook_combined.txt file from the Stanford University site.&lt;/p&gt;
&lt;p&gt;We read in the file and construct the Graph:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.dropbox.com/s/k34phmodh9nsy9r/facebook_combined.txt?dl=0&#34;&gt;Download the file&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd

df = pd.read_csv(&amp;#39;/Users/jankirenz/Dropbox/Data/facebook_combined.txt&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
RangeIndex: 88233 entries, 0 to 88232
Data columns (total 1 columns):
0 1    88233 non-null object
dtypes: object(1)
memory usage: 689.4+ KB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.tail()&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
&lt;thead&gt;
&lt;tr style=&#34;text-align: right;&#34;&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
0 1
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;
88228
&lt;/th&gt;
&lt;td&gt;
4026 4030
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
88229
&lt;/th&gt;
&lt;td&gt;
4027 4031
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
88230
&lt;/th&gt;
&lt;td&gt;
4027 4032
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
88231
&lt;/th&gt;
&lt;td&gt;
4027 4038
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
88232
&lt;/th&gt;
&lt;td&gt;
4031 4038
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;G_fb = nx.read_edgelist(&amp;quot;/Users/jankirenz/Dropbox/Data/facebook_combined.txt&amp;quot;, create_using = nx.Graph(), nodetype=int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(nx.info(G_fb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Name:
Type: Graph
Number of nodes: 4039
Number of edges: 88234
Average degree: 43.6910&lt;/p&gt;
&lt;p&gt;The network consists of 4,039 nodes, connected via 88,234 edges.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.figure(figsize=(20,20))
nx.draw_networkx(G_fb);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-08-13-network_analysis/net1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also visualize the network such that the node color varies with Degree and node size with Betweenness Centrality. The code to do this is:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pos = nx.spring_layout(G_fb)
betCent = nx.betweenness_centrality(G_fb, normalized=True, endpoints=True)
node_color = [20000.0 * G_fb.degree(v) for v in G_fb]
node_size =  [v * 10000 for v in betCent.values()]
plt.figure(figsize=(20,20))
nx.draw_networkx(G_fb, pos=pos, with_labels=False,
                 node_color=node_color,
                 node_size=node_size )
plt.axis(&amp;#39;off&amp;#39;);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-08-13-network_analysis/net2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can also know the labels of the nodes with the highest betweenness centrality using:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sorted(betCent, key=betCent.get, reverse=True)[:5]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that some nodes are common between Degree Centrality, which is a measure of degree, and Betweenness Centrality which controls the information flow.&lt;/p&gt;
&lt;p&gt;It is natural that nodes that are more connected also lie on shortest paths between other nodes. The node 1912 is an important node as it is crucial according to all three centrality measures that we had considered.&lt;/p&gt;
&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;Sources of examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/community/tutorials/social-network-analysis-python&#34;&gt;Datacamp&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aksakalli.github.io/2017/07/17/network-centrality-measures-and-their-visualization.html&#34;&gt;Aksakalli, C.&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://snap.stanford.edu/data/egonets-Facebook.html&#34;&gt;McAuley, J. &amp;amp; Leskovec, J.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Network Analysis with Python</title>
      <link>https://kirenz.com/project/python-network-analysis-intro/</link>
      <pubDate>Mon, 12 Aug 2019 10:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/python-network-analysis-intro/</guid>
      <description>


&lt;p&gt;NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Material:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kirenz.com/slides/Social_Network_Analysis_Networkx/social-network-analysis.html&#34;&gt;Introduction to network presentation slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/13samMteoTnAkUoHul1iV5pA0bc4qmYjGge019HcT1F4/edit?usp=sharing&#34;&gt;Network analysis tasks with code templates&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Lasso Regression with Python</title>
      <link>https://kirenz.com/post/2019-08-12-python-lasso-regression-auto/</link>
      <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/post/2019-08-12-python-lasso-regression-auto/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lasso-regression-basics&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Lasso Regression Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-of-lasso-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Implementation of Lasso regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#standardization&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Standardization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#split-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Split data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lasso-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.3&lt;/span&gt; Lasso regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lasso-with-different-lambdas&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.4&lt;/span&gt; Lasso with different lambdas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-values-as-a-function-of-lambda&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.5&lt;/span&gt; Plot values as a function of lambda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#identify-best-lambda-and-coefficients&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.6&lt;/span&gt; Identify best lambda and coefficients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cross-validation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.7&lt;/span&gt; Cross Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.8&lt;/span&gt; Best Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;lasso-regression-basics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Lasso Regression Basics&lt;/h1&gt;
&lt;p&gt;Lasso performs a so called &lt;code&gt;L1 regularization&lt;/code&gt; (a process of introducing additional information in order to prevent overfitting), i.e. adds penalty equivalent to absolute value of the magnitude of coefficients.&lt;/p&gt;
&lt;p&gt;In particular, the minimization objective does not only include the residual sum of squares (RSS) - like in the OLS regression setting - but also the sum of the absolute value of coefficients.&lt;/p&gt;
&lt;p&gt;The residual sum of squares (RSS) is calculated as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ RSS = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This formula can be stated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ RSS = \sum_{i=1}^{n} \bigg(y_i - \big( \beta_{0} + \sum_{j=1}^{p} \beta_{j} x_{ij} \big) \bigg)^2  \]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;n represents the number of distinct data points, or observations, in our sample.&lt;/li&gt;
&lt;li&gt;p denotes the number of variables that are available in the dataset.&lt;/li&gt;
&lt;li&gt;x_{ij} represents the value of the jth variable for the ith observation, where i = 1, 2, . . ., n and j = 1, 2, . . . , p.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the lasso regression, the minimization objective becomes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sum_{i=1}^{n} \bigg(y_i - \big( \beta_{0} + \sum_{j=1}^{p} \beta_{j} x_{ij} \big) \bigg)^2 + \lambda \sum_{j=1}^{p} |\beta_j|   \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which equals:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[RSS + \lambda \sum_{j=1}^{p} |\beta_j|  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; (lambda) provides a trade-off between balancing RSS and magnitude of coefficients.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; can take various values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; = 0: Same coefficients as simple linear regression&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; = ∞: All coefficients zero (same logic as before)&lt;/li&gt;
&lt;li&gt;0 &amp;lt; &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; &amp;lt; ∞: coefficients between 0 and that of simple linear regression&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-of-lasso-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Implementation of Lasso regression&lt;/h1&gt;
&lt;p&gt;Python set up:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
plt.style.use(&amp;#39;ggplot&amp;#39;)
import warnings; warnings.simplefilter(&amp;#39;ignore&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This notebook involves the use of the Lasso regression on the “Auto” dataset. In particular, we only use observations 1 to 200 for our analysis. Furthermore, you can drop the &lt;code&gt;name&lt;/code&gt; variable.&lt;/p&gt;
&lt;p&gt;Import data:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df = pd.read_csv(&amp;quot;https://raw.githubusercontent.com/kirenz/datasets/master/Auto.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tidying data:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df = df.iloc[0:200]
df = df.drop([&amp;#39;name&amp;#39;], axis=1)
df.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
RangeIndex: 200 entries, 0 to 199
Data columns (total 8 columns):
mpg             200 non-null float64
cylinders       200 non-null int64
displacement    200 non-null float64
horsepower      200 non-null object
weight          200 non-null int64
acceleration    200 non-null float64
year            200 non-null int64
origin          200 non-null int64
dtypes: float64(3), int64(4), object(1)
memory usage: 12.6+ KB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df[&amp;#39;origin&amp;#39;] = pd.Categorical(df[&amp;#39;origin&amp;#39;])
df[&amp;#39;horsepower&amp;#39;] = pd.to_numeric(df[&amp;#39;horsepower&amp;#39;], errors=&amp;#39;coerce&amp;#39;)
print(df.isnull().sum())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mpg             0
cylinders       0
displacement    0
horsepower      2
weight          0
acceleration    0
year            0
origin          0
dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# drop missing cases
df = df.dropna()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use scikit learn to fit a Lasso regression &lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34;&gt;(see documentation)&lt;/a&gt; and follow a number of steps (note that scikit-learn uses &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; in their notation):&lt;/p&gt;
&lt;div id=&#34;standardization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Standardization&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Standardize the features with the module: &lt;code&gt;from sklearn.preprocessing import StandardScaler&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It is important to standardize the features by removing the mean and scaling to unit variance. The L1 (Lasso) and L2 (Ridge) regularizers of linear models assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs = df.astype(&amp;#39;int&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 198 entries, 0 to 199
Data columns (total 8 columns):
mpg             198 non-null int64
cylinders       198 non-null int64
displacement    198 non-null int64
horsepower      198 non-null int64
weight          198 non-null int64
acceleration    198 non-null int64
year            198 non-null int64
origin          198 non-null int64
dtypes: int64(8)
memory usage: 13.9 KB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs.columns&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Index([&amp;#39;mpg&amp;#39;, &amp;#39;cylinders&amp;#39;, &amp;#39;displacement&amp;#39;, &amp;#39;horsepower&amp;#39;, &amp;#39;weight&amp;#39;,
       &amp;#39;acceleration&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;origin&amp;#39;],
      dtype=&amp;#39;object&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
dfs[[&amp;#39;cylinders&amp;#39;, &amp;#39;displacement&amp;#39;, &amp;#39;horsepower&amp;#39;,
     &amp;#39;weight&amp;#39;, &amp;#39;acceleration&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;origin&amp;#39;]] = scaler.fit_transform(dfs[[&amp;#39;cylinders&amp;#39;,
                                                                              &amp;#39;displacement&amp;#39;,
                                                                              &amp;#39;horsepower&amp;#39;,
                                                                              &amp;#39;weight&amp;#39;,
                                                                              &amp;#39;acceleration&amp;#39;,
                                                                              &amp;#39;year&amp;#39;, &amp;#39;origin&amp;#39;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs.head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
&lt;thead&gt;
&lt;tr style=&#34;text-align: right;&#34;&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
mpg
&lt;/th&gt;
&lt;th&gt;
cylinders
&lt;/th&gt;
&lt;th&gt;
displacement
&lt;/th&gt;
&lt;th&gt;
horsepower
&lt;/th&gt;
&lt;th&gt;
weight
&lt;/th&gt;
&lt;th&gt;
acceleration
&lt;/th&gt;
&lt;th&gt;
year
&lt;/th&gt;
&lt;th&gt;
origin
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;
0
&lt;/th&gt;
&lt;td&gt;
18
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
0.726091
&lt;/td&gt;
&lt;td&gt;
0.325216
&lt;/td&gt;
&lt;td&gt;
0.346138
&lt;/td&gt;
&lt;td&gt;
-0.955578
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
1
&lt;/th&gt;
&lt;td&gt;
15
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
1.100254
&lt;/td&gt;
&lt;td&gt;
1.129264
&lt;/td&gt;
&lt;td&gt;
0.548389
&lt;/td&gt;
&lt;td&gt;
-1.305309
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
2
&lt;/th&gt;
&lt;td&gt;
18
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
0.821807
&lt;/td&gt;
&lt;td&gt;
0.784672
&lt;/td&gt;
&lt;td&gt;
0.273370
&lt;/td&gt;
&lt;td&gt;
-1.305309
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
3
&lt;/th&gt;
&lt;td&gt;
16
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
0.699986
&lt;/td&gt;
&lt;td&gt;
0.784672
&lt;/td&gt;
&lt;td&gt;
0.270160
&lt;/td&gt;
&lt;td&gt;
-0.955578
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
4
&lt;/th&gt;
&lt;td&gt;
17
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
0.682583
&lt;/td&gt;
&lt;td&gt;
0.554944
&lt;/td&gt;
&lt;td&gt;
0.287282
&lt;/td&gt;
&lt;td&gt;
-1.655041
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;split-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Split data&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Split the data set&lt;/strong&gt; into train and test sets (use &lt;code&gt;X_train&lt;/code&gt;, &lt;code&gt;X_test&lt;/code&gt;, &lt;code&gt;y_train&lt;/code&gt;, &lt;code&gt;y_test&lt;/code&gt;), with the first 75% of the data for training and the remaining for testing. (module: &lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;X = dfs.drop([&amp;#39;mpg&amp;#39;], axis=1)
y = dfs[&amp;#39;mpg&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Lasso regression&lt;/h2&gt;
&lt;p&gt;Apply &lt;strong&gt;Lasso regression&lt;/strong&gt; on the training set with the regularization parameter &lt;strong&gt;lambda = 0.5&lt;/strong&gt; (module: &lt;code&gt;from sklearn.linear_model import Lasso&lt;/code&gt;) and print the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;-score for the training and test set. Comment on your findings.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.linear_model import Lasso

reg = Lasso(alpha=0.5)
reg.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000,
normalize=False, positive=False, precompute=False, random_state=None,
selection=‘cyclic’, tol=0.0001, warm_start=False)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;#39;Lasso Regression: R^2 score on training set&amp;#39;, reg.score(X_train, y_train)*100)
print(&amp;#39;Lasso Regression: R^2 score on test set&amp;#39;, reg.score(X_test, y_test)*100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lasso Regression: R^2 score on training set 82.49741060950073
Lasso Regression: R^2 score on test set 85.49734440925533&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso-with-different-lambdas&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Lasso with different lambdas&lt;/h2&gt;
&lt;p&gt;Apply the &lt;strong&gt;Lasso regression&lt;/strong&gt; on the training set with the following &lt;strong&gt;λ parameters: (0.001, 0.01, 0.1, 0.5, 1, 2, 10)&lt;/strong&gt;. Evaluate the R^2 score for all the models you obtain on both the train and test sets.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;lambdas = (0.001, 0.01, 0.1, 0.5, 1, 2, 10)
l_num = 7
pred_num = X.shape[1]

# prepare data for enumerate
coeff_a = np.zeros((l_num, pred_num))
train_r_squared = np.zeros(l_num)
test_r_squared = np.zeros(l_num)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# enumerate through lambdas with index and i
for ind, i in enumerate(lambdas):    
    reg = Lasso(alpha = i)
    reg.fit(X_train, y_train)

    coeff_a[ind,:] = reg.coef_
    train_r_squared[ind] = reg.score(X_train, y_train)
    test_r_squared[ind] = reg.score(X_test, y_test)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-values-as-a-function-of-lambda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.5&lt;/span&gt; Plot values as a function of lambda&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Plot&lt;/strong&gt; all values for both data sets (train and test &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;-values) as a function of λ. Comment on your findings.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Plotting
plt.figure(figsize=(18, 8))
plt.plot(train_r_squared, &amp;#39;bo-&amp;#39;, label=r&amp;#39;$R^2$ Training set&amp;#39;, color=&amp;quot;darkblue&amp;quot;, alpha=0.6, linewidth=3)
plt.plot(test_r_squared, &amp;#39;bo-&amp;#39;, label=r&amp;#39;$R^2$ Test set&amp;#39;, color=&amp;quot;darkred&amp;quot;, alpha=0.6, linewidth=3)
plt.xlabel(&amp;#39;Lamda index&amp;#39;); plt.ylabel(r&amp;#39;$R^2$&amp;#39;)
plt.xlim(0, 6)
plt.title(r&amp;#39;Evaluate lasso regression with lamdas: 0 = 0.001, 1= 0.01, 2 = 0.1, 3 = 0.5, 4= 1, 5= 2, 6 = 10&amp;#39;)
plt.legend(loc=&amp;#39;best&amp;#39;)
plt.grid()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-08-12-python-lasso-regression-auto/output_27_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;identify-best-lambda-and-coefficients&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.6&lt;/span&gt; Identify best lambda and coefficients&lt;/h2&gt;
&lt;p&gt;Store your test data results in a DataFrame and indentify the lambda where the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; has it’s &lt;strong&gt;maximum value&lt;/strong&gt; in the &lt;strong&gt;test data&lt;/strong&gt;. Fit a Lasso model with this lambda parameter (use the training data) and obtain the corresponding &lt;strong&gt;regression coefficients&lt;/strong&gt;. Furthermore, obtain the &lt;strong&gt;mean squared error&lt;/strong&gt; for the test data of this model (module: &lt;code&gt;from sklearn.metrics import mean_squared_error&lt;/code&gt;)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_lam = pd.DataFrame(test_r_squared*100, columns=[&amp;#39;R_squared&amp;#39;])
df_lam[&amp;#39;lambda&amp;#39;] = (lambdas)
# returns the index of the row where column has maximum value.
df_lam.loc[df_lam[&amp;#39;R_squared&amp;#39;].idxmax()]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R_squared 88.105773
lambda 0.001000
Name: 0, dtype: float64&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Coefficients of best model
reg_best = Lasso(alpha = 0.1)
reg_best.fit(X_train, y_train)
reg_best.coef_&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;array([-0.35554113, -1.13104696, -0.00596296, -3.31741775, -0. ,
0.37914648, 0.74902885])&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, reg_best.predict(X_test))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3.586249592807347&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.7&lt;/span&gt; Cross Validation&lt;/h2&gt;
&lt;p&gt;Evaluate the performance of a &lt;strong&gt;Lasso regression&lt;/strong&gt; for different regularization parameters λ using &lt;strong&gt;5-fold cross validation&lt;/strong&gt; on the training set (module: &lt;code&gt;from sklearn.model_selection import cross_val_score&lt;/code&gt;) and plot the cross-validation (CV) &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; scores of the training and test data as a function of λ.&lt;/p&gt;
&lt;p&gt;Use the following lambda parameters:
l_min = 0.05
l_max = 0.2
l_num = 20
lambdas = np.linspace(l_min,l_max, l_num)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;l_min = 0.05
l_max = 0.2
l_num = 20
lambdas = np.linspace(l_min,l_max, l_num)

train_r_squared = np.zeros(l_num)
test_r_squared = np.zeros(l_num)

pred_num = X.shape[1]
coeff_a = np.zeros((l_num, pred_num))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import cross_val_score

for ind, i in enumerate(lambdas):    
    reg = Lasso(alpha = i)
    reg.fit(X_train, y_train)
    results = cross_val_score(reg, X, y, cv=5, scoring=&amp;quot;r2&amp;quot;)

    train_r_squared[ind] = reg.score(X_train, y_train)    
    test_r_squared[ind] = reg.score(X_test, y_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Plotting
plt.figure(figsize=(18, 8))
plt.plot(train_r_squared, &amp;#39;bo-&amp;#39;, label=r&amp;#39;$R^2$ Training set&amp;#39;, color=&amp;quot;darkblue&amp;quot;, alpha=0.6, linewidth=3)
plt.plot(test_r_squared, &amp;#39;bo-&amp;#39;, label=r&amp;#39;$R^2$ Test set&amp;#39;, color=&amp;quot;darkred&amp;quot;, alpha=0.6, linewidth=3)
plt.xlabel(&amp;#39;Lamda value&amp;#39;); plt.ylabel(r&amp;#39;$R^2$&amp;#39;)
plt.xlim(0, 19)
plt.title(r&amp;#39;Evaluate 5-fold cv with different lamdas&amp;#39;)
plt.legend(loc=&amp;#39;best&amp;#39;)
plt.grid()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-08-12-python-lasso-regression-auto/output_35_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;best-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.8&lt;/span&gt; Best Model&lt;/h2&gt;
&lt;p&gt;Finally, store your test data results in a DataFrame and identify the lambda where the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; has it’s &lt;strong&gt;maximum value&lt;/strong&gt; in the &lt;strong&gt;test data&lt;/strong&gt;. Fit a Lasso model with this lambda parameter (use the training data) and obtain the corresponding &lt;strong&gt;regression coefficients&lt;/strong&gt;. Furthermore, obtain the &lt;strong&gt;mean squared error&lt;/strong&gt; for the test data of this model (module: &lt;code&gt;from sklearn.metrics import mean_squared_error&lt;/code&gt;)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_lam = pd.DataFrame(test_r_squared*100, columns=[&amp;#39;R_squared&amp;#39;])
df_lam[&amp;#39;lambda&amp;#39;] = (lambdas)
# returns the index of the row where column has maximum value.
df_lam.loc[df_lam[&amp;#39;R_squared&amp;#39;].idxmax()]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R_squared 87.897525
lambda 0.050000
Name: 0, dtype: float64&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Best Model
reg_best = Lasso(alpha = 0.144737)
reg_best.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lasso(alpha=0.144737, copy_X=True, fit_intercept=True, max_iter=1000,
normalize=False, positive=False, precompute=False, random_state=None,
selection=‘cyclic’, tol=0.0001, warm_start=False)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.metrics import mean_squared_error

mean_squared_error(y_test, reg_best.predict(X_test))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3.635187490993961&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;reg_best.coef_&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;array([-0.34136411, -1.18223273, -0. , -3.27132984, 0. ,
0.33262331, 0.71385488])&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Correlation Tutorial with R</title>
      <link>https://kirenz.com/project/r-correlation-tutorial/</link>
      <pubDate>Sun, 11 Aug 2019 05:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/r-correlation-tutorial/</guid>
      <description>&lt;p&gt;Correlation is a way of measuring the extent to which two variables are related. This means we need to analyze whether as one variable increases, the other&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(1) increases,&lt;/li&gt;
&lt;li&gt;(2) decreases or&lt;/li&gt;
&lt;li&gt;(3) stays the same.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This can be done by calculating the covariance or correlation of two variables.&lt;/p&gt;

&lt;p&gt;In this &lt;a href=&#34;https://github.com/kirenz/correlation/blob/master/Correlation.md&#34; target=&#34;_blank&#34;&gt;Correlation Tutorial in R&lt;/a&gt;, we  use a small dataset to illustrate the concepts of covariance and correlation. You may also download the &lt;a href=&#34;https://github.com/kirenz/correlation/blob/master/Correlation.Rmd&#34; target=&#34;_blank&#34;&gt;Rmarkdown file&lt;/a&gt; and open it in RStudio.&lt;/p&gt;

&lt;p&gt;Check your understanding with &lt;a href=&#34;https://github.com/kirenz/correlation/blob/master/Correlation_task.pdf&#34; target=&#34;_blank&#34;&gt;multiple choice tasks&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lasso Regression with Python</title>
      <link>https://kirenz.com/project/r-lasso-regression/</link>
      <pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/r-lasso-regression/</guid>
      <description>

&lt;h1 id=&#34;lasso-regression&#34;&gt;Lasso Regression&lt;/h1&gt;

&lt;p&gt;In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces (&lt;a href=&#34;https://en.wikipedia.org/wiki/Lasso_(statistics)&#34; target=&#34;_blank&#34;&gt;Wikipedia&lt;/a&gt;).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Lasso Regression with Python (Auto Data): &lt;a href=&#34;https://github.com/kirenz/lasso-regression/blob/master/python-lasso-regression-auto.ipynb&#34; target=&#34;_blank&#34;&gt;Jupyter Notebook&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Time Series Analysis with Python</title>
      <link>https://kirenz.com/project/python-time-series/</link>
      <pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/python-time-series/</guid>
      <description>

&lt;h1 id=&#34;time-series-analysis-with-python&#34;&gt;Time Series Analysis with Python&lt;/h1&gt;

&lt;p&gt;Time series analysis can be used in a multitude of business applications for forecasting a quantity into the future and explaining its historical patterns. Here are just a few examples of possible use cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Explaining seasonal patterns in sales&lt;/li&gt;
&lt;li&gt;Predicting the expected number of incoming or churning customers&lt;/li&gt;
&lt;li&gt;Estimating the effect of a newly launched product on number of sold units&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Introduction to Time Series Analysis with Python: Fit &lt;strong&gt;ARIMA&lt;/strong&gt; and &lt;strong&gt;SARIMAX-Models&lt;/strong&gt; with &lt;code&gt;Statsmodel&lt;/code&gt;: &lt;a href=&#34;https://github.com/kirenz/time-series-analysis/blob/master/time-series-first-steps.ipynb&#34; target=&#34;_blank&#34;&gt;Jupyter Notebook&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Introduction to Facebook&amp;rsquo;s time series analysis modul &lt;strong&gt;Prophet&lt;/strong&gt;: &lt;a href=&#34;https://github.com/kirenz/time-series-analysis/blob/master/Prophet.ipynb&#34; target=&#34;_blank&#34;&gt;Jupyter Notebook&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Deskriptive Statistik in R</title>
      <link>https://kirenz.com/post/2019-08-01-r-descriptive-statistics/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/post/2019-08-01-r-descriptive-statistics/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#deskriptive-statistik-in-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Deskriptive Statistik in R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#datenimport&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1&lt;/span&gt; Datenimport&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deskriptive-statistiken&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2&lt;/span&gt; Deskriptive Statistiken&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mittelwert&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.1&lt;/span&gt; Mittelwert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standardabweichung&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.2&lt;/span&gt; Standardabweichung&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getrimmter-mittelwert&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.3&lt;/span&gt; Getrimmter Mittelwert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#schiefe&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.4&lt;/span&gt; Schiefe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kurtosis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.5&lt;/span&gt; Kurtosis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standardfehler&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.6&lt;/span&gt; Standardfehler&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;deskriptive-statistik-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Deskriptive Statistik in R&lt;/h1&gt;
&lt;p&gt;In diesem Beitrag wird die Berechnung einfacher deskriptiver Statistiken und die Visualisierung von Verteilungen in R am Beispiel des Datensatzes “Advertising” behandelt.&lt;/p&gt;
&lt;div id=&#34;datenimport&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Datenimport&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Datensatz: Advertising.csv&lt;/li&gt;
&lt;li&gt;Variablen: &lt;em&gt;TV&lt;/em&gt;, &lt;em&gt;radio&lt;/em&gt;, &lt;em&gt;newspaper&lt;/em&gt; = jeweils Werbeausgaben in Dollar; &lt;em&gt;sales&lt;/em&gt; = Produkte in Tausend Einheiten&lt;/li&gt;
&lt;li&gt;Abhängige Variable (dependent variable, response): &lt;em&gt;sales&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Unabhängige Variablen (independent variables, predictors): &lt;em&gt;TV&lt;/em&gt;, &lt;em&gt;radio&lt;/em&gt;, &lt;em&gt;newspaper&lt;/em&gt;, &lt;em&gt;sales&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Zunächts möchten wir uns einen Überblick über die Daten verschaffen. Dafür importieren wir die Daten und prüfen, ob die Skalenniveaus korrekt sind. Für die weiteren Berechnungen wird die Variable X1 nicht benötigt, weshalb wir diese löschen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
# Daten importieren
Advertising &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/kirenz/datasets/master/advertising.csv&amp;quot;)
# Überblick über die Daten verschaffen (Skalenniveaus prüfen)
head(Advertising)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##      X1    TV radio newspaper sales
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 230.   37.8      69.2  22.1
## 2     2  44.5  39.3      45.1  10.4
## 3     3  17.2  45.9      69.3   9.3
## 4     4 152.   41.3      58.5  18.5
## 5     5 181.   10.8      58.4  12.9
## 6     6   8.7  48.9      75     7.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Bereinigung der Daten
Advertising$X1 &amp;lt;- NULL&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;deskriptive-statistiken&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Deskriptive Statistiken&lt;/h2&gt;
&lt;p&gt;Ausgabe unterschiedlicher deskriptiver Statistiken:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)

psych::describe(Advertising) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           vars   n   mean    sd median trimmed    mad min   max range
## TV           1 200 147.04 85.85 149.75  147.20 108.82 0.7 296.4 295.7
## radio        2 200  23.26 14.85  22.90   23.00  19.79 0.0  49.6  49.6
## newspaper    3 200  30.55 21.78  25.75   28.41  23.13 0.3 114.0 113.7
## sales        4 200  14.02  5.22  12.90   13.78   4.82 1.6  27.0  25.4
##            skew kurtosis   se
## TV        -0.07    -1.24 6.07
## radio      0.09    -1.28 1.05
## newspaper  0.88     0.57 1.54
## sales      0.40    -0.45 0.37&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Hinweise zu den Kennzahlen:
&lt;ul&gt;
&lt;li&gt;vars: Nummer der Variable&lt;/li&gt;
&lt;li&gt;n: Anzahl der Beobachtungen&lt;/li&gt;
&lt;li&gt;mean: arithmetischer Mittelwert&lt;/li&gt;
&lt;li&gt;sd: empirische Standardabweichung&lt;/li&gt;
&lt;li&gt;median: Median&lt;/li&gt;
&lt;li&gt;trimmed: getrimmter Mittelwert&lt;/li&gt;
&lt;li&gt;mad: Mittlere absolute Abweichung vom Median&lt;/li&gt;
&lt;li&gt;min: kleinster Beobachtungswert&lt;/li&gt;
&lt;li&gt;max: größter Beobachtungswert&lt;/li&gt;
&lt;li&gt;range: Spannweite&lt;/li&gt;
&lt;li&gt;skew: Schiefe&lt;/li&gt;
&lt;li&gt;kurtosis: Wölbung&lt;/li&gt;
&lt;li&gt;se = Standardfehler&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;mittelwert&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.1&lt;/span&gt; Mittelwert&lt;/h3&gt;
&lt;p&gt;Bei der Berechnung des &lt;em&gt;arithmetischen Mittelwerts&lt;/em&gt; in R sollte immer die Anweisung gegeben werden, fehlende Werte auszuschließen (na.rm = “remove values which are not available”). Ansonsten stoppt R bei fehlenden Werten die Berechnung und gibt eine Fehlermeldung aus.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_sales &amp;lt;- mean(Advertising$sales, na.rm = TRUE)
print(paste0(&amp;quot;Mittelwert der Variable Sales: &amp;quot;, mean_sales))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Mittelwert der Variable Sales: 14.0225&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;standardabweichung&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.2&lt;/span&gt; Standardabweichung&lt;/h3&gt;
&lt;p&gt;Die Standardabweichung ist ein häufig verwendetes Streuungsmaß und beschreibt die mittlere Abweichung der einzelnen Messwerte vom empirischen Mittelwert. Die Standardabweichung ist die positive Wurzel der empirischen Varianz. Die Varianz einer Stichprobe wird wie folgt berechnet:
&lt;span class=&#34;math display&#34;&gt;\[s^{2} = \frac{\sum_{i=1}^{n} \left(x_{i} - \bar{x}\right)^{2}} {n-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Berechnung der Standardabweichung: &lt;span class=&#34;math display&#34;&gt;\[s = \sqrt{\frac{\sum\limits_{i=1}^{n} \left(x_{i} - \bar{x}\right)^{2}} {n-1}}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var_sales &amp;lt;- var(Advertising$sales, na.rm = TRUE)
print(paste0(&amp;quot;Varianz der Variable Sales: &amp;quot;, round(var_sales, 2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Varianz der Variable Sales: 27.22&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_sales &amp;lt;-  sd(Advertising$sales, na.rm = TRUE)
print(paste0(&amp;quot;Standardabweichung der Variable Sales: &amp;quot;, round(sd_sales,2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Standardabweichung der Variable Sales: 5.22&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;getrimmter-mittelwert&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.3&lt;/span&gt; Getrimmter Mittelwert&lt;/h3&gt;
&lt;p&gt;Bei dem &lt;em&gt;getrimmten Mittelwert&lt;/em&gt; wird ein bestimmer Anteil der größten und kleinsten Beobachtungen - hier oberhalb des 90% Quantils und unterhalb des 10 % Quantils - ignoriert. Damit sollen Ausreißer aus der Berechnung des Mittelwerts ausgeschlossen werden. Der getrimmte Mittelwert kann wie folgt in R berechnet werden:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_trim_sales &amp;lt;- mean(Advertising$sales, trim = 0.1, na.rm = TRUE)
print(paste0(&amp;quot;Getrimmter Mittelwert der Variable Sales: &amp;quot;, round(mean_trim_sales, 2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Getrimmter Mittelwert der Variable Sales: 13.78&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;schiefe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.4&lt;/span&gt; Schiefe&lt;/h3&gt;
&lt;p&gt;Die &lt;em&gt;Schiefe&lt;/em&gt; ist eine statistische Kennzahl, die die Art und Stärke der Asymmetrie einer Wahrscheinlichkeitsverteilung beschreibt. Sie zeigt an, ob und wie stark die Verteilung nach rechts (positive Schiefe) oder nach links (negative Schiefe) geneigt ist. Jede nicht symmetrische Verteilung heißt schief.&lt;/p&gt;
&lt;p&gt;Darstellung der Verteilung in einem Histogramm:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
# Vorlage für die Erstellung von plots in ggplot2 
plot_1 &amp;lt;-  theme_bw() +
        theme(axis.text.x = element_text(angle = 0, size = 8, family=&amp;quot;Arial&amp;quot;, colour=&amp;#39;black&amp;#39;),
        axis.text.y = element_text(angle = 0, size = 8, family=&amp;quot;Arial&amp;quot;, colour=&amp;#39;black&amp;#39;),
        axis.title = element_text(size=8, face=&amp;quot;bold&amp;quot;, family=&amp;quot;Arial&amp;quot;, colour=&amp;#39;black&amp;#39;),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title=element_text(hjust=0, size=10,  family=&amp;quot;Arial&amp;quot;, face=&amp;quot;bold&amp;quot;, colour=&amp;#39;black&amp;#39;))

ggplot(Advertising, aes(sales)) +
  geom_histogram(binwidth = 2, color=&amp;quot;red&amp;quot;, alpha=.2) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  labs(title=&amp;quot;Histogramm für Sales&amp;quot;, x=&amp;quot;Sales&amp;quot;, y=&amp;quot;Anzahl&amp;quot;) +
  plot_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-08-01-r-descriptive-statistics/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Darstellung der Verteilung in einer Dichtefunktion:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

ggplot(Advertising, aes(sales)) +
  geom_density(fill=&amp;quot;grey&amp;quot;,alpha=.2 ) +
  geom_vline(aes(xintercept=mean(sales, na.rm=TRUE)), color=&amp;quot;red&amp;quot;, linetype=&amp;quot;dotted&amp;quot;, size=0.6) +
  geom_vline(aes(xintercept=median(sales, na.rm=TRUE)), color=&amp;quot;red&amp;quot;, linetype=&amp;quot;dotted&amp;quot;, size=0.6) +
  geom_text(aes(x=median(sales), y=0.02), colour = &amp;quot;grey&amp;quot;, size =3,  
             label=round(mean(Advertising$sales), digits=2), hjust=-1, family=&amp;quot;Arial&amp;quot;) +
  geom_text(aes(x=mean(sales), y=0.02), hjust=-0.7, colour = &amp;quot;grey&amp;quot;, size = 3, label=&amp;quot;Mittelwert&amp;quot;, family=&amp;quot;Arial&amp;quot;) +
  geom_text(aes(x=median(sales), y=0.005), colour = &amp;quot;grey&amp;quot;, size =3, 
             label=round(median(Advertising$sales), digits=2), hjust=1 , family=&amp;quot;Arial&amp;quot;) +
  geom_text(aes(x=median(sales), y=0.01), colour = &amp;quot;grey&amp;quot;, size = 3, label=&amp;quot;Median&amp;quot;, hjust=1, family=&amp;quot;Arial&amp;quot;) +
  labs(x=&amp;quot;Produktabsatz (in Tausend Einheiten)&amp;quot;, y = &amp;quot;Dichte&amp;quot;, title = &amp;quot;Wahrscheinlichkeitsdichtefunktion&amp;quot;) +
  plot_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://kirenz.com/post/2019-08-01-r-descriptive-statistics/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In der Abbildung kann man erkennen, dass es sich um eine asymmetrische Verteilung handelt (d.h. es liegt eine Abweichung von der Normalverteilung vor). Konkret handelt es sich um eine rechtsschiefe Verteilung (Mittelwert &amp;gt; Median; Schiefe = + 0.40).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kurtosis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.5&lt;/span&gt; Kurtosis&lt;/h3&gt;
&lt;p&gt;Die Abweichung des Verlaufs einer Verteilung vom Verlauf einer Normalverteilung wird &lt;em&gt;Kurtosis&lt;/em&gt; (Wölbung) genannt. Sie gibt an, wie spitz die Kurve verläuft. Unterschieden wird zwischen positiver, spitz zulaufender (leptokurtische Verteilung) und negativer, flacher (platykurtische Verteilung) Kurtosis. Die Kurtosis zählt zu den zentralen Momenten einer Verteilung, mittels derer der Kurvenverlauf definiert wird. Eine Kurtosis mit Wert 0 ist normalgipflig (mesokurtisch), ein Wert größer 0 ist steilgipflig und ein Wert unter 0 ist flachgipflig.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standardfehler&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.6&lt;/span&gt; Standardfehler&lt;/h3&gt;
&lt;p&gt;Der &lt;em&gt;Standardfehler&lt;/em&gt; ein Maß für die durchschnittliche Abweichung des geschätzten Parameterwertes vom wahren Parameterwert. Je kleiner der Standardfehler ist, desto genauer kann der unbekannte Parameter der Population mit Hilfe der Schätzfunktion geschätzt werden. Der Standardfehler hängt unter anderem von dem Stichprobenumfang und der Varianz ab. Allgemein gilt: Je größer der Stichprobenumfang, desto kleiner der Standardfehler; je kleiner die Varianz, desto kleiner der Standardfehler.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression Tutorial with Python</title>
      <link>https://kirenz.com/project/python-linear-regression/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/python-linear-regression/</guid>
      <description>

&lt;h1 id=&#34;linear-regression-tutorial-in-python&#34;&gt;Linear Regression Tutorial in Python&lt;/h1&gt;

&lt;p&gt;Linear regression is the fundamental starting point for all regression methods. In this Jupyter Notebook, we fit a regression model in Python and take a closer look at the following topics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Histogramms&lt;/li&gt;
&lt;li&gt;Boxplots&lt;/li&gt;
&lt;li&gt;Mean&lt;/li&gt;
&lt;li&gt;Standard deviation&lt;/li&gt;
&lt;li&gt;Mean squared error&lt;/li&gt;
&lt;li&gt;$R^2$&lt;/li&gt;
&lt;li&gt;Pearson&amp;rsquo;s correlation coefficient&lt;/li&gt;
&lt;li&gt;F-Statistic&lt;/li&gt;
&lt;li&gt;Standard error&lt;/li&gt;
&lt;li&gt;Confidence interval&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Download the linear regression tutorial Jupyter Notebook in &lt;a href=&#34;https://github.com/kirenz/linear-regression/blob/master/python-regression-tutorial.ipynb&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; and open the file in Jupyter Notebook.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>First Steps in Python</title>
      <link>https://kirenz.com/project/python-first-steps/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/python-first-steps/</guid>
      <description>

&lt;h1 id=&#34;first-steps-in-python&#34;&gt;First Steps in Python&lt;/h1&gt;

&lt;p&gt;Download the PDF &lt;a href=&#34;https://github.com/kirenz/first_steps_in_python/blob/master/Python_overview.pdf&#34; target=&#34;_blank&#34;&gt;Python overview&lt;/a&gt; to get an overview about Python and a list of helpful resources (you need to download the file in order to use the embedded links).&lt;/p&gt;

&lt;p&gt;First of all, install &lt;strong&gt;Anaconda&lt;/strong&gt; - it&amp;rsquo;s a free and open-source distribution of the Python programming language that aims to simplify package management and deployment. It already contains Jupyter Notebook (see below) and other important data science modules.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Install &lt;a href=&#34;https://www.anaconda.com/distribution/&#34; target=&#34;_blank&#34;&gt;Anaconda&lt;/a&gt; (select the current version of Python 3). After installation, launch the Anaconda Navigator and start Jupyter Notebook or Jupyter Lab.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One important third-party tool for data science is
&lt;strong&gt;Jupyter&lt;/strong&gt;, an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Learn why Jupyter is data scientists&amp;rsquo; computational notebook of choice:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.nature.com/articles/d41586-018-07196-1&#34; target=&#34;_blank&#34;&gt;Perkel, J. M. (2018). Why Jupyter is data scientists&amp;rsquo; computational notebook of choice. Nature, 563(7729), p. 145.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/bringing-the-best-out-of-jupyter-notebooks-for-data-science-f0871519ca29&#34; target=&#34;_blank&#34;&gt;Pandey, P. (2018). Bringing the best out of Jupyter Notebooks for Data Science. Towards Data Science&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/jupyter-lab-evolution-of-the-jupyter-notebook-5297cacde6b&#34; target=&#34;_blank&#34;&gt;Pandey, P. (2019). Jupyter Lab: Evolution of the Jupyter Notebook. Towards Data Science&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Now, let&amp;rsquo;s start with some code examples:&lt;/p&gt;

&lt;p&gt;1) &lt;a href=&#34;https://github.com/kirenz/first_steps_in_python/blob/master/1_pandas_import_save_csv.ipynb&#34; target=&#34;_blank&#34;&gt;Import and save CSV-files with Pandas&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2) &lt;a href=&#34;https://github.com/kirenz/first_steps_in_python/blob/master/2_data_tidying_missing_values.ipynb&#34; target=&#34;_blank&#34;&gt;Check for missing values&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3) &lt;a href=&#34;https://github.com/kirenz/first_steps_in_python/blob/master/3_level_of_measurement.ipynb&#34; target=&#34;_blank&#34;&gt;Change data type (level of measurment)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;4) &lt;a href=&#34;https://github.com/kirenz/first_steps_in_python/blob/master/4_descriptive_statistics.ipynb&#34; target=&#34;_blank&#34;&gt;Descriptive statistics&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/kirenz/first_steps_in_python/blob/master/4_descriptive_statistics.ipynb&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg&#34; alt=&#34;Open In Colab&#34; /&gt;&lt;/a&gt;
&lt;em&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/welcome.ipynb#scrollTo=5fCEDCU_qrC0&#34; target=&#34;_blank&#34;&gt;Colaboratory&lt;/a&gt; is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud. With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;5) &lt;a href=&#34;https://github.com/kirenz/first_steps_in_python/blob/master/5_data_science_programming_process.ipynb&#34; target=&#34;_blank&#34;&gt;Overview of the data science programming process&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;recommended-reading&#34;&gt;Recommended reading:&lt;/h2&gt;

&lt;p&gt;&amp;ldquo;&lt;strong&gt;A Whirlwind Tour of Python&lt;/strong&gt;&amp;rdquo; is a free and fast-paced introduction to essential features of the Python language. The material is particularly designed for those who wish to use Python for data science and/or scientific programming:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jakevdp.github.io/WhirlwindTourOfPython/&#34; target=&#34;_blank&#34;&gt;VanderPlas, J. (2016). Whirlwind Tour of Python. O&amp;rsquo;Reilly Media.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>First Steps in R</title>
      <link>https://kirenz.com/project/r-first-steps/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/r-first-steps/</guid>
      <description>

&lt;h1 id=&#34;first-steps-in-r&#34;&gt;First Steps in R&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/R_logo.svg/200px-R_logo.svg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Download the PDF &lt;a href=&#34;https://github.com/kirenz/first-steps-in-r/blob/master/R_overview.pdf&#34; target=&#34;_blank&#34;&gt;R overview&lt;/a&gt; to get an overview about R and a list of helpful resources (you need to download the file in order to use the embedded links).&lt;/p&gt;

&lt;h2 id=&#34;installing-r&#34;&gt;Installing R&lt;/h2&gt;

&lt;p&gt;The first step is to install R. You can download and install R from the &lt;a href=&#34;https://cran.r-project.org/&#34; target=&#34;_blank&#34;&gt;Comprehensive R Archive Network&lt;/a&gt; (CRAN).&lt;/p&gt;

&lt;p&gt;Windows:
- Open the &lt;a href=&#34;https://cran.r-project.org/&#34; target=&#34;_blank&#34;&gt;Comprehensive R Archive Network&lt;/a&gt;.
- Click on “CRAN”. You’ll see a list of mirror sites, organized by country.
- Select a site near you.
- Click on “Windows” under “Download and Install R”.
- Click on “base”.
- Click on the link for downloading the latest version of R (an .exe file).
- When the download completes, double-click on the .exe file and answer the usual questions.&lt;/p&gt;

&lt;p&gt;Mac:
- Open the &lt;a href=&#34;https://cran.r-project.org/&#34; target=&#34;_blank&#34;&gt;Comprehensive R Archive Network&lt;/a&gt;.
- Click on “CRAN”.
- You’ll see a list of mirror sites, organized by country.
- Select a site near you.
- Click on “MacOS X”.
- Click on the .pkg file for the latest version of R, under “Files:”, to download it.
- When the download completes, double-click on the .pkg file and answer the usual questions.&lt;/p&gt;

&lt;h2 id=&#34;installing-rstudio&#34;&gt;Installing RStudio&lt;/h2&gt;

&lt;p&gt;The next step is to install &lt;strong&gt;RStudio&lt;/strong&gt;, a free and open-source integrated development environment (IDE) for R. You can use it for viewing and running R scripts.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://www.rstudio.com/products/rstudio/#Desktop&#34; target=&#34;_blank&#34;&gt;RStudio Download&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Click the Download RStudio Desktop button.&lt;/li&gt;
&lt;li&gt;Select the installation file for your system.&lt;/li&gt;
&lt;li&gt;Run the installation file.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;learn-r-basics&#34;&gt;Learn R Basics&lt;/h2&gt;

&lt;p&gt;First of all, you can take an online course to master the basics of R: Visit the interactive &lt;a href=&#34;https://www.datacamp.com/getting-started?step=2&amp;amp;track=r&#34; target=&#34;_blank&#34;&gt;R-Course&lt;/a&gt; from DataCamp. With the knowledge gained in this courses, you will be ready to undertake your first very own data analysis.&lt;/p&gt;

&lt;p&gt;There are also open and free resources and reference guides for R. Two examples are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.statmethods.net/&#34; target=&#34;_blank&#34;&gt;Quick-R&lt;/a&gt;: a quick online reference for data input, basic statistics and plots&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/doc/contrib/Short-refcard.pdf&#34; target=&#34;_blank&#34;&gt;R reference card (PDF)&lt;/a&gt; by Tom Short&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Two key things you need to know about R is that you can get help for a function using &lt;code&gt;help&lt;/code&gt; or &lt;code&gt;?&lt;/code&gt;, like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r,eval=FALSE&#34;&gt;?install.packages
help(&amp;quot;install.packages&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the hash character represents comments, so text following these
characters is not interpreted:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;##This is just a comment
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;installing-r-packages&#34;&gt;Installing R Packages&lt;/h2&gt;

&lt;p&gt;The first R command we will run is &lt;code&gt;install.packages&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;An R package is a collection of functions, data, and documentation that extends the capabilities of base R.
Many of these functions are stored in CRAN. You can easily install packages from within RStudio if you know
the name of the packages.&lt;/p&gt;

&lt;p&gt;As an example, we are going to install the
package &lt;code&gt;dplyr&lt;/code&gt; which we use in our first data
analysis examples:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r,eval=FALSE&#34;&gt;install.packages(&amp;quot;dplyr&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then load the package into our R sessions using the &lt;code&gt;library&lt;/code&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From now on you will see that we sometimes load packages without
installing them. This is because you only need to install a package once,
but you need to reload it with the command &lt;code&gt;library&lt;/code&gt; every time you start
a new R session.&lt;/p&gt;

&lt;p&gt;If you try to load a package and get an error, it probably means you need to install it first.&lt;/p&gt;

&lt;p&gt;Review the &lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html&#34; target=&#34;_blank&#34;&gt;dplyr-documentation&lt;/a&gt; to get an overview about the different functionalities of this package.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating Websites with R Markdown</title>
      <link>https://kirenz.com/project/blogdown-book/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/blogdown-book/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Create Data Presentations with R Markdown</title>
      <link>https://kirenz.com/project/r-xaringan/</link>
      <pubDate>Thu, 01 Aug 2019 10:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/r-xaringan/</guid>
      <description>


&lt;p&gt;The xaringan package is an R Markdown extension based on the JavaScript library &lt;a href=&#34;https://remarkjs.com&#34;&gt;remark.js&lt;/a&gt; to generate HTML5 presentations in different &lt;a href=&#34;https://github.com/yihui/xaringan/tree/master/inst/rmarkdown/templates/xaringan/resources&#34;&gt;themes&lt;/a&gt; (&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/xaringan.html#ref-R-xaringan&#34;&gt;Xie 2019&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;You can learn more about the usage of the xaringan package from this excellent &lt;a href=&#34;http://slides.yihui.name/xaringan/&#34;&gt;documentation&lt;/a&gt;, which is actually a set of slides generated from xaringan.&lt;/p&gt;
&lt;p&gt;Xie, Y. (2019). Xaringan: Presentation Ninja. &lt;a href=&#34;https://CRAN.R-project.org/package=xaringan&#34;&gt;CRAN&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Free books to learn Data Science &amp; Statistics with R</title>
      <link>https://kirenz.com/project/r-data-science-statistics/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/project/r-data-science-statistics/</guid>
      <description>&lt;p&gt;&amp;ldquo;&lt;strong&gt;R for Data Science&lt;/strong&gt;&amp;rdquo; offers an excellent introduction into data science in R with a focus on the popular package collection &lt;a href=&#34;https://www.tidyverse.org&#34; target=&#34;_blank&#34;&gt;tidyverse&lt;/a&gt;. See how the &lt;em&gt;tidyverse&lt;/em&gt; makes data science faster, easier and more fun:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r4ds.had.co.nz&#34; target=&#34;_blank&#34;&gt;Wickham, H., &amp;amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O&amp;rsquo;Reilly Media, Inc.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;ldquo;&lt;strong&gt;An Introduction to Statistical Learning&lt;/strong&gt;&amp;rdquo; provides an accessible overview of the field of statistical learning with applications in R. This book presents important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.statlearning.com&#34; target=&#34;_blank&#34;&gt;James, G., Witten, D., Hastie, T., &amp;amp; Tibshirani, R. (2013). An introduction to statistical learning with applications in R (Corr. 7th printing 2017). New York: Springer.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;ldquo;&lt;strong&gt;Statistical Thinking for the 21 Century&lt;/strong&gt;&amp;rdquo; and &amp;ldquo;&lt;strong&gt;Modern Dive: Statistical Inference via Data Science&lt;/strong&gt;&amp;rdquo; are both open-source digital textbooks which provide a great introduction into the fundamentals of modern quantitative methods which take advantage of today’s increased computing power to solve statistical problems with R:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://statsthinking21.org&#34; target=&#34;_blank&#34;&gt;Poldrack, R. A. (2019). Statistical Thinking for the 21 Century. http://thinkstats.org.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://moderndive.com/index.html#sec:intro-for-students&#34; target=&#34;_blank&#34;&gt;Ismay, C. &amp;amp; Kim, A. Y. (2019). Modern Dive: Statistical Inference via Data Science. https://moderndive.com&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Create and publish a Website with R and Hugo</title>
      <link>https://kirenz.com/post/2019-07-20-up-and-running-with-blogdown/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/post/2019-07-20-up-and-running-with-blogdown/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-blogdown&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction to Blogdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#github&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#terminal-or-github-desktop&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Terminal or GitHub Desktop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rstudio&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; RStudio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#build-your-site-in-rstudio&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Build your site in RStudio&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#picking-a-theme&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.1&lt;/span&gt; Picking a theme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#update-project-options&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.2&lt;/span&gt; Update project options&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#edit-your-configurations&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.3&lt;/span&gt; Edit your configurations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#addins-workflow&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.4&lt;/span&gt; Addins &amp;amp; workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#posting&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.5&lt;/span&gt; Posting&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#draft-posts&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.5.1&lt;/span&gt; Draft posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#new-markdown-posts&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.5.2&lt;/span&gt; New markdown posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#new-r-markdown-.rmd-posts&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.5.3&lt;/span&gt; New R Markdown (.Rmd) posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-images-to-a-post&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.5.4&lt;/span&gt; Adding images to a post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deploy-in-netlify&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Deploy in Netlify&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#going-further&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Going further&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#custom-css&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.1&lt;/span&gt; Custom CSS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#formspree&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.2&lt;/span&gt; Formspree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rbind.io-domain-names&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.3&lt;/span&gt; &lt;code&gt;*.rbind.io&lt;/code&gt; domain names&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction-to-blogdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction to Blogdown&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;The content below is taken from the excellent post &lt;a href=&#34;https://alison.rbind.io/post/2017-06-12-up-and-running-with-blogdown/&#34;&gt;“Up &amp;amp; Running with blogdown”&lt;/a&gt; from Alison Hill&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Before you start, I recommend reading the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;&lt;code&gt;blogdown&lt;/code&gt;: Creating Websites with R Markdown&lt;/a&gt; by Yihui Xie and Amber Thomas&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also note that I am a macOS user, and I use R, RStudio, Git (usually via &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt;), and terminal regularly, so I’m assuming familiarity here with all of these. If that is not the case, here are some places to get started:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For Git: &lt;a href=&#34;http://happygitwithr.com&#34;&gt;Happy Git with R&lt;/a&gt; by Jenny Bryan et al.&lt;/li&gt;
&lt;li&gt;For RStudio: &lt;a href=&#34;https://www.datacamp.com/courses/working-with-the-rstudio-ide-part-1&#34;&gt;DataCamp’s Working with the RStudio IDE (free)&lt;/a&gt; by Garrett Grolemund&lt;/li&gt;
&lt;li&gt;For Terminal: &lt;a href=&#34;https://github.com/veltman/clmystery&#34;&gt;The Command Line Murder Mystery&lt;/a&gt; by Noah Veltman, and &lt;a href=&#34;http://seankross.com/the-unix-workbench/&#34;&gt;The UNIX Workbench&lt;/a&gt; by Sean Kross&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also have &lt;a href=&#34;https://developer.apple.com/xcode/&#34;&gt;Xcode&lt;/a&gt; and &lt;a href=&#34;https://brew.sh&#34;&gt;Homebrew&lt;/a&gt; installed- &lt;a href=&#34;https://bookdown.org/yihui/blogdown/installation.html&#34;&gt;you will probably need these to download Hugo&lt;/a&gt;. If you don’t have either but are on a mac, this link may help:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.moncefbelyamani.com/how-to-install-xcode-homebrew-git-rvm-ruby-on-mac/&#34;&gt;How to install Xcode, Homebrew, Git, RVM, Ruby &amp;amp; Rails on Mac OS X&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Introduction to static site generators and how domain names work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/deployment.html&#34;&gt;“Considering the cost and friendliness to beginners, I currently recommend Netlify.”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/netlify.html&#34;&gt;“If you are not familiar with domain names or do not want to learn more about them, an option for your consideration is a free subdomain &lt;code&gt;*.rbind.io&lt;/code&gt; offered by RStudio, Inc.”&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;github&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; GitHub&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;blogdown-signpost-1.png&#34; /&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Go online to your &lt;a href=&#34;https://github.com&#34;&gt;GitHub&lt;/a&gt; account, and create a new repository (check to initialize with a &lt;code&gt;README&lt;/code&gt; but don’t add &lt;code&gt;.gitignore&lt;/code&gt;- this will be taken care of later). For naming your repo, consider your future deployment plan:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you are going to use &lt;a href=&#34;https://www.netlify.com&#34;&gt;Netlify&lt;/a&gt; to host the site, you can name this repository anything you want!
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can see some of the repo names used by members of the &lt;code&gt;rbind&lt;/code&gt; organization &lt;a href=&#34;https://github.com/rbind/repositories&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you want to host your site as a &lt;a href=&#34;https://pages.github.com&#34;&gt;GitHub Page&lt;/a&gt;, you should name your repository &lt;code&gt;yourgithubusername.github.io&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;github-new-repo.png&#34; alt=&#34;Screenshot above: Creating a new repository in GitHub&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Screenshot above: Creating a new repository in GitHub&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Go to the main page of your new repository, and under the repository name, click the green &lt;strong&gt;Clone or download&lt;/strong&gt; button.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the &lt;strong&gt;Clone with HTTPs&lt;/strong&gt; section, click on the clipboard icon to copy the clone URL for your new repository. You’ll paste this text into terminal in the next section.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;terminal-or-github-desktop&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Terminal or GitHub Desktop&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;blogdown-signpost-2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now you will &lt;a href=&#34;https://help.github.com/articles/cloning-a-repository/&#34;&gt;clone your remote repository&lt;/a&gt; and create a local copy on your computer so you can sync between the two locations (using terminal or your alternative command line tool for a Windows machine). However, I recommend to use &lt;a href=&#34;https://desktop.github.com&#34;&gt;GitHub Desktop&lt;/a&gt; instead of the terminal for the &lt;a href=&#34;https://help.github.com/en/articles/cloning-a-repository#cloning-a-repository-to-github-desktop&#34;&gt;cloning process&lt;/a&gt;. If you instead would like to use the terminal, this is how you proceed:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use &lt;code&gt;cd&lt;/code&gt; to navigate into the directory where you want your repo to be&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once there, type: &lt;code&gt;git clone [paste]&lt;/code&gt;. So my command looked like this:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/apreshill/apreshill.git&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And this is what printed to the terminal window:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Cloning into &amp;#39;apreshill&amp;#39;...
remote: Counting objects: 3, done.
remote: Compressing objects: 100% (2/2), done.
remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0
Unpacking objects: 100% (3/3), done.
Checking connectivity... done.&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Close terminal, you are done in there.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;rstudio&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; RStudio&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;blogdown-signpost-3.png&#34; /&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Install &lt;code&gt;blogdown&lt;/code&gt; from your RStudio console. If you already have &lt;code&gt;devtools&lt;/code&gt; installed like I did, you can just use the second line below:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;if (!requireNamespace(&amp;quot;devtools&amp;quot;)) install.packages(&amp;quot;devtools&amp;quot;)
devtools::install_github(&amp;quot;rstudio/blogdown&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Install Hugo using the &lt;code&gt;blogdown&lt;/code&gt; package helper function:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;blogdown::install_hugo()
# or
library(blogdown)
install_hugo()&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Use the top menu buttons in RStudio to select &lt;code&gt;File -&amp;gt; New Project -&amp;gt; Existing Directory&lt;/code&gt;, then browse to the directory on your computer where your GitHub repo is and click on the &lt;strong&gt;Create Project&lt;/strong&gt; button.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;r-project-existing-directory.png&#34; alt=&#34;Screenshot above: Creating a new project in an existing directory in RStudio&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Screenshot above: Creating a new project in an existing directory in RStudio&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Now you should be “in” your project in RStudio. If you are using git for version control, edit your &lt;code&gt;*gitignore&lt;/code&gt; file. This file should be viewable in your file viewer pane in RStudio. Below is what it should look like: the first four lines will automatically be in this file if you have set up your RStudio Project, but if you plan to use Netlify to deploy, you need to add the &lt;code&gt;public/&lt;/code&gt; line (&lt;a href=&#34;https://bookdown.org/yihui/blogdown/version-control.html&#34;&gt;read about here&lt;/a&gt;.)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;.Rproj.user
.Rhistory
.RData
.Ruserdata
blogdown
.DS_Store # if a windows user, Thumbs.db instead
public/ # if using Netlify&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;build-your-site-in-rstudio&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Build your site in RStudio&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;blogdown-signpost-4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now you can finally build your site using the &lt;code&gt;blogdown::new_site()&lt;/code&gt; function. But &lt;strong&gt;first&lt;/strong&gt; you should at least think about themes…&lt;/p&gt;
&lt;div id=&#34;picking-a-theme&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Picking a theme&lt;/h2&gt;
&lt;p&gt;There are over 90 &lt;a href=&#34;https://themes.gohugo.io&#34;&gt;Hugo themes&lt;/a&gt;. Here you can find an overview of some of the &lt;a href=&#34;https://bookdown.org/yihui/blogdown/other-themes.html&#34;&gt;themes&lt;/a&gt;. Whatever theme you choose, you’ll need to pick one of 3 ways to make your new site:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If you are happy with the default theme, which is the lithium theme, you can use:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;blogdown::new_site() # default theme is lithium&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If you want a theme other than the default, you can specify the theme at the same time as you call the &lt;code&gt;new_site&lt;/code&gt; function:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;# for example, create a new site with the academic theme
blogdown::new_site(theme = &amp;quot;gcushen/hugo-academic&amp;quot;, theme_example = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If instead you want to add the theme later, you can do this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;library(blogdown)
new_site() # default theme is lithium
# need to stop serving so can use the console again
install_theme(&amp;quot;gcushen/hugo-academic&amp;quot;, theme_example = TRUE, update_config = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Now is a good time to re-read about &lt;code&gt;blogdown::serve_site()&lt;/code&gt; and &lt;a href=&#34;https://bookdown.org/yihui/blogdown/a-quick-example.html&#34;&gt;how &lt;em&gt;LiveReload&lt;/em&gt; works&lt;/a&gt; (and how it blocks your R console by default)
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;update-project-options&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Update project options&lt;/h2&gt;
&lt;p&gt;In your project in RStudio, go to the top menu bar of RStudio and select &lt;code&gt;Tools -&amp;gt; Project Options&lt;/code&gt; and update following &lt;a href=&#34;https://bookdown.org/yihui/blogdown/rstudio-ide.html#fig:project-options&#34;&gt;Yihui and Amber’s instructions&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;edit-your-configurations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.3&lt;/span&gt; Edit your configurations&lt;/h2&gt;
&lt;p&gt;Relevant reading:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/configuration.html&#34;&gt;&lt;code&gt;blogdown&lt;/code&gt; book chapter on configuration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;You can also view Alison Hill’s &lt;a href=&#34;https://github.com/apreshill/apreshill/blob/master/config.toml&#34;&gt;&lt;code&gt;config.toml&lt;/code&gt; file&lt;/a&gt; in GitHub&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, edit the &lt;code&gt;baseurl&lt;/code&gt; in your &lt;code&gt;config.toml&lt;/code&gt; file. The URL &lt;em&gt;should always&lt;/em&gt; end with a &lt;code&gt;/&lt;/code&gt; trailing slash. At this point, you probably haven’t deployed your site yet, so to view it locally you can use the &lt;strong&gt;Serve Site&lt;/strong&gt; add-in, or run the &lt;code&gt;blogdown::serve_site&lt;/code&gt; function. Both of these baseurls worked for me when viewing locally:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;baseurl = &amp;quot;https://example.com/&amp;quot;
baseurl = &amp;quot;/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Make sure that the &lt;code&gt;baseurl =&lt;/code&gt; listed ends with a trailing slash &lt;code&gt;/&lt;/code&gt;!
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Go ahead and edit all the other elements in the &lt;code&gt;config.toml&lt;/code&gt; file now as you please- this is how you personalize your site.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;addins-workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.4&lt;/span&gt; Addins &amp;amp; workflow&lt;/h2&gt;
&lt;p&gt;Relevant reading:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/rstudio-ide.html&#34;&gt;&lt;code&gt;blogdown&lt;/code&gt; book chapter on the RStudio IDE&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Addins: use them- you won’t need the &lt;code&gt;blogdown&lt;/code&gt; library loaded in the console if you use the Addins. The workflow in RStudio at this point (again, just viewing locally because we haven’t deployed yet) works best like this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open the RStudio project for the site&lt;/li&gt;
&lt;li&gt;Use the &lt;strong&gt;Serve Site&lt;/strong&gt; add-in (only once due to &lt;em&gt;LiveReload&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;View site in the RStudio viewer pane, and open in a new browser window while you work&lt;/li&gt;
&lt;li&gt;Select existing files to edit using the file pane in RStudio&lt;/li&gt;
&lt;li&gt;After making changes, click the save button (don’t &lt;code&gt;knit&lt;/code&gt;!)- the console will reload, the viewer pane will update, and if you hit refresh in the browser your local view will also be updated&lt;/li&gt;
&lt;li&gt;When happy with changes, add/commit/push changes to GitHub&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Having &lt;code&gt;blogdown::serve_site&lt;/code&gt; running locally with &lt;em&gt;LiveReload&lt;/em&gt; is especially useful as you can immediately see if you have made any mistakes.&lt;/p&gt;
&lt;p&gt;The above workflow is only for editing existing files or posts, but not for &lt;strong&gt;creating new posts&lt;/strong&gt;. For that, read on…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;posting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.5&lt;/span&gt; Posting&lt;/h2&gt;
&lt;p&gt;Relevant reading:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/rstudio-ide.html&#34;&gt;&lt;code&gt;blogdown&lt;/code&gt; book chapter on RStudio IDE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/output-format.html&#34;&gt;&lt;code&gt;blogdown&lt;/code&gt; book chapter on output formats&lt;/a&gt;: on .md versus .Rmd posts&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bottom line:&lt;/p&gt;
&lt;p&gt;Use the &lt;strong&gt;New Post&lt;/strong&gt; addin. But, you need the console to do this, so you have to stop &lt;code&gt;blogdown::serve_site&lt;/code&gt; by clicking on the red &lt;strong&gt;Stop&lt;/strong&gt; button first. The Addin is a &lt;a href=&#34;https://shiny.rstudio.com&#34;&gt;Shiny&lt;/a&gt; interface that runs this code in your console: &lt;code&gt;blogdown:::new_post_addin()&lt;/code&gt;. So, your console needs to be unblocked for it to run. You also need to be “in” your RStudio project or it won’t work.&lt;/p&gt;
&lt;div id=&#34;draft-posts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.5.1&lt;/span&gt; Draft posts&lt;/h3&gt;
&lt;p&gt;Relevant reading:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/local-preview.html&#34;&gt;&lt;code&gt;blogdown&lt;/code&gt; book chapter on building a website for local preview&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Whether you do a markdown or R Markdown post (see below), you should know that in the YAML front matter of your new file, you can add &lt;code&gt;draft: TRUE&lt;/code&gt; and you will be able to preview your post using &lt;code&gt;blogdown::serve_site()&lt;/code&gt;, but conveniently your post will not show up on your deployed site until you set it to false. Because this is a function built into Hugo, all posts (draft or not) will still end up in your GitHub repo though.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;new-markdown-posts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.5.2&lt;/span&gt; New markdown posts&lt;/h3&gt;
&lt;p&gt;Pick one of 2 methods:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Use the &lt;strong&gt;New Post&lt;/strong&gt; addin and with the radio button at the bottom select &lt;strong&gt;Format: Markdown&lt;/strong&gt; (recommended)&lt;/li&gt;
&lt;li&gt;Use the console to author a new &lt;code&gt;.md&lt;/code&gt; post:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;blogdown::new_post()
blogdown::new_post(ext = &amp;#39;.md&amp;#39;) # md is the default!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the &lt;code&gt;?new_post&lt;/code&gt; arguments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_post(title, kind = &amp;quot;&amp;quot;, open = interactive(),
    author = getOption(&amp;quot;blogdown.author&amp;quot;), categories = NULL, tags = NULL,
    date = Sys.Date(), file = NULL, slug = NULL,
    title_case = getOption(&amp;quot;blogdown.title_case&amp;quot;),
    subdir = getOption(&amp;quot;blogdown.subdir&amp;quot;, &amp;quot;post&amp;quot;),
    ext = getOption(&amp;quot;blogdown.ext&amp;quot;, &amp;quot;.md&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Remember to use the &lt;strong&gt;Serve Site&lt;/strong&gt; addin again so that you can immediately view your changes with every save using &lt;em&gt;LiveReload&lt;/em&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;new-r-markdown-.rmd-posts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.5.3&lt;/span&gt; New R Markdown (.Rmd) posts&lt;/h3&gt;
&lt;p&gt;Again, you have your choice of one of 2 methods:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Use the &lt;strong&gt;New Post&lt;/strong&gt; addin and with the radio button at the bottom select &lt;strong&gt;Format: R Markdown (.Rmd)&lt;/strong&gt; (recommended)&lt;/li&gt;
&lt;li&gt;Use the console to author a new &lt;code&gt;.Rmd&lt;/code&gt; post:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;blogdown::new_post(ext = &amp;#39;.Rmd&amp;#39;) # md is the default!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After you edit your &lt;code&gt;.Rmd&lt;/code&gt; post, in addition to saving the changes in your &lt;code&gt;.Rmd&lt;/code&gt; file, you &lt;em&gt;must&lt;/em&gt; use &lt;code&gt;blogdown::serve_site&lt;/code&gt;- this is how the output &lt;code&gt;html&lt;/code&gt; file needs to be generated.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Do &lt;em&gt;not&lt;/em&gt; knit your &lt;code&gt;.Rmd&lt;/code&gt; posts- use &lt;code&gt;blogdown::serve_site&lt;/code&gt; instead. If you happen to hit the knit button, just &lt;strong&gt;Serve Site&lt;/strong&gt; again to rewrite the &lt;code&gt;.html&lt;/code&gt; file.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Ultimately, your &lt;a href=&#34;https://bookdown.org/yihui/blogdown/output-format.html#output-format&#34;&gt;YAML front matter looks something like this&lt;/a&gt;; note that some but not all features of &lt;code&gt;rmarkdown::html_document&lt;/code&gt; &lt;a href=&#34;https://bookdown.org/yihui/blogdown/output-format.html#fn15&#34;&gt;are supported in &lt;code&gt;blogdown&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: &amp;quot;My Post&amp;quot;
author: &amp;quot;John Doe&amp;quot;
date: &amp;quot;2017-02-14&amp;quot;
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
    number_sections: true
    fig_width: 6
---&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Remember to use the &lt;strong&gt;Serve Site&lt;/strong&gt; addin again so that you can immediately view your changes with every save using &lt;em&gt;LiveReload&lt;/em&gt; and your &lt;code&gt;.html&lt;/code&gt; file is properly output.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-images-to-a-post&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.5.4&lt;/span&gt; Adding images to a post&lt;/h3&gt;
&lt;p&gt;If you want to include an image that is not a figure created from an R chunk, the &lt;a href=&#34;https://github.com/rstudio/blogdown/issues/45&#34;&gt;recommended method&lt;/a&gt; is to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Add the image to your &lt;code&gt;/static/img/&lt;/code&gt; folder, then&lt;/li&gt;
&lt;li&gt;Reference the image using the relative file path as follows:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;![my-image](/img/my-image.png)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;deploy-in-netlify&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Deploy in Netlify&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;blogdown-signpost-5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Deploying in Netlify through GitHub is smooth. Here are some &lt;a href=&#34;https://bookdown.org/yihui/blogdown/deployment.html&#34;&gt;beginner instructions&lt;/a&gt;, but Netlify is so easy, I recommend that you skip dragging your &lt;code&gt;public&lt;/code&gt; folder in and instead &lt;a href=&#34;https://bookdown.org/yihui/blogdown/netlify.html#netlify&#34;&gt;automate the process through GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;When you are ready to deploy, commit your changes and push to GitHub, then go online to &lt;a href=&#34;https://www.netlify.com&#34;&gt;Netlify&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Click on the &lt;strong&gt;Sign Up&lt;/strong&gt; button and sign up using your existing GitHub account (no need to create another account)&lt;/li&gt;
&lt;li&gt;Log in, and select: &lt;code&gt;New site from Git -&amp;gt; Continuous Deployment: GitHub&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From there, Netlify will allow you to select from your existing GitHub repositories. You’ll pick the repo you’ve been working from with &lt;code&gt;blogdown&lt;/code&gt;, then you’ll configure your build. This involves specifying two important things: the build command and the publish directory (this should be &lt;code&gt;public&lt;/code&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More about the build command from &lt;a href=&#34;https://www.netlify.com/docs/continuous-deployment/#common-configuration-directives&#34;&gt;Netlify&lt;/a&gt;: &lt;em&gt;“For Hugo hosting, &lt;code&gt;hugo&lt;/code&gt; will build and deploy with the version 0.17 of &lt;code&gt;hugo&lt;/code&gt;. You can specify a specific &lt;code&gt;hugo&lt;/code&gt; release like this: &lt;code&gt;hugo_0.15&lt;/code&gt;. Currently &lt;code&gt;0.13&lt;/code&gt;, &lt;code&gt;0.14&lt;/code&gt;, &lt;code&gt;0.15&lt;/code&gt;, &lt;code&gt;0.16&lt;/code&gt;, &lt;code&gt;0.17&lt;/code&gt;, &lt;code&gt;0.18&lt;/code&gt; and &lt;code&gt;0.19&lt;/code&gt; are supported. For version &lt;code&gt;0.20&lt;/code&gt; and above, you’ll need to create a Build environment variable called &lt;code&gt;HUGO_VERSION&lt;/code&gt; and set it to the version of your choice.”&lt;/em&gt; I opted for the former, and specified &lt;code&gt;hugo_0.19&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can check your &lt;code&gt;hugo&lt;/code&gt; version in terminal using the command &lt;a href=&#34;https://gohugo.io/overview/quickstart/&#34;&gt;&lt;code&gt;hugo version&lt;/code&gt;&lt;/a&gt;. This is what my output looked like, so I could run version &lt;code&gt;0.20&lt;/code&gt; if I wanted to through Netlify, but I went with &lt;code&gt;0.19&lt;/code&gt; and it works just fine.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ hugo version
Hugo Static Site Generator v0.20.7 darwin/amd64 BuildDate: 2017-05-08T18:37:40-07:00&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;netlify-build-settings.png&#34; alt=&#34;Screenshot above: Basic build settings in Netlify&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Screenshot above: Basic build settings in Netlify&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Netlify will deploy your site and assign you a random subdomain name of the form &lt;code&gt;random-word-12345.netlify.com&lt;/code&gt;. You should know that you can change this; e.g. to &lt;code&gt;mynewsite.netlify.com&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Anytime you change your subdomain name, you need to update the &lt;code&gt;baseurl&lt;/code&gt; in your &lt;code&gt;config.toml&lt;/code&gt; file (e.g., baseurl = “&lt;a href=&#34;https://mynewsite.netlify.com/&#34; class=&#34;uri&#34;&gt;&lt;a href=&#34;https://mynewsite.netlify.com/&#34; target=&#34;_blank&#34;&gt;https://mynewsite.netlify.com/&lt;/a&gt;&lt;/a&gt;”).
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;At this point, you should be up and running with &lt;code&gt;blogdown&lt;/code&gt;, GitHub, and Netlify, but here are some ideas if you want to go further…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;going-further&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Going further&lt;/h1&gt;
&lt;div id=&#34;custom-css&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.1&lt;/span&gt; Custom CSS&lt;/h2&gt;
&lt;p&gt;Every Hugo theme is structured a little differently, but if you are interested, you can check out Alison Hill’s &lt;a href=&#34;https://github.com/apreshill/apreshill/blob/master/static/css/blue.css&#34;&gt;custom css&lt;/a&gt; to see how she customized the academic theme, which provides a way to link to a custom CSS file in the &lt;code&gt;config.toml&lt;/code&gt; file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  # Link custom CSS and JS assets
  #   (relative to /static/css and /static/js respectively)
  custom_css = [&amp;quot;blue.css&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;formspree&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.2&lt;/span&gt; Formspree&lt;/h2&gt;
&lt;p&gt;Alison Hill used &lt;a href=&#34;https://formspree.io&#34;&gt;Formspree&lt;/a&gt; to make a contact form, which is an online service (managed on &lt;a href=&#34;https://github.com/formspree/formspree&#34;&gt;GitHub&lt;/a&gt;) that allows you to add an HTML form to your static site. No registration, just use the form and confirm your email address once. She added the following code into &lt;a href=&#34;https://github.com/apreshill/apreshill/blob/master/themes/hugo-academic/layouts/partials/widgets/contact.html&#34;&gt;the contact widget&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;form action=&amp;quot;https://formspree.io/your@email.com&amp;quot; method=&amp;quot;POST&amp;quot;&amp;gt;
  &amp;lt;label for=&amp;quot;name&amp;quot;&amp;gt;Your name: &amp;lt;/label&amp;gt;
  &amp;lt;input type=&amp;quot;text&amp;quot; name=&amp;quot;name&amp;quot; required=&amp;quot;required&amp;quot; placeholder=&amp;quot;here&amp;quot;&amp;gt;&amp;lt;br&amp;gt;
  &amp;lt;label for=&amp;quot;email&amp;quot;&amp;gt;Your email: &amp;lt;/label&amp;gt;
  &amp;lt;input type=&amp;quot;email&amp;quot; name=&amp;quot;_replyto&amp;quot; required=&amp;quot;required&amp;quot; placeholder=&amp;quot;here&amp;quot;&amp;gt;&amp;lt;br&amp;gt;
  &amp;lt;label for=&amp;quot;message&amp;quot;&amp;gt;Your message:&amp;lt;/label&amp;gt;&amp;lt;br&amp;gt;
  &amp;lt;textarea rows=&amp;quot;4&amp;quot; name=&amp;quot;message&amp;quot; id=&amp;quot;message&amp;quot; required=&amp;quot;required&amp;quot; class=&amp;quot;form-control&amp;quot; placeholder=&amp;quot;I can&amp;#39;t wait to read this!&amp;quot;&amp;gt;&amp;lt;/textarea&amp;gt;
  &amp;lt;input type=&amp;quot;hidden&amp;quot; name=&amp;quot;_next&amp;quot; value=&amp;quot;/html/thanks.html&amp;quot; /&amp;gt;
  &amp;lt;input type=&amp;quot;submit&amp;quot; value=&amp;quot;Send&amp;quot; name=&amp;quot;submit&amp;quot; class=&amp;quot;btn btn-primary btn-outline&amp;quot;&amp;gt;
  &amp;lt;input type=&amp;quot;hidden&amp;quot; name=&amp;quot;_subject&amp;quot; value=&amp;quot;Website message&amp;quot; /&amp;gt;
  &amp;lt;input type=&amp;quot;text&amp;quot; name=&amp;quot;_gotcha&amp;quot; style=&amp;quot;display:none&amp;quot; /&amp;gt;
&amp;lt;/form&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;rbind.io-domain-names&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.3&lt;/span&gt; &lt;code&gt;*.rbind.io&lt;/code&gt; domain names&lt;/h2&gt;
&lt;p&gt;You may want a different &lt;a href=&#34;https://bookdown.org/yihui/blogdown/domain-name.html&#34;&gt;domain name&lt;/a&gt; than the one provided by Netlify. Alison opted for a free subdomain &lt;code&gt;*.rbind.io&lt;/code&gt; offered by RStudio. To do the same, head over to the &lt;a href=&#34;https://github.com/rbind/support/issues&#34;&gt;rbind/support GitHub page&lt;/a&gt; and open a new issue. All you need to do is let them know what your Netlify subdomain name is (&lt;code&gt;*.netlify.com&lt;/code&gt;), and what you want your subdomain name to be (&lt;code&gt;*.rbind.io&lt;/code&gt;). The &lt;a href=&#34;https://support.rbind.io&#34;&gt;&lt;code&gt;rbind&lt;/code&gt; support team&lt;/a&gt; will help you take it from there!&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Again, you will need to update the &lt;code&gt;baseurl&lt;/code&gt; in your &lt;code&gt;config.toml&lt;/code&gt; file to reflect your new rbind subdomain name (so Alison’s is baseurl = “&lt;a href=&#34;https://alison.rbind.io/&#34; class=&#34;uri&#34;&gt;&lt;a href=&#34;https://alison.rbind.io/&#34; target=&#34;_blank&#34;&gt;https://alison.rbind.io/&lt;/a&gt;&lt;/a&gt;”).
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;That’s it!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>About / Contact</title>
      <link>https://kirenz.com/contact/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>https://kirenz.com/projects/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Seminars &amp; Lectures</title>
      <link>https://kirenz.com/teaching/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://kirenz.com/teaching/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LICENSE: CC-BY-SA</title>
      <link>https://kirenz.com/license/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>https://kirenz.com/license/</guid>
      <description>&lt;p&gt;My &lt;a href=&#34;https://kirenz.com/post/&#34;&gt;blog posts&lt;/a&gt; are released under a &lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34; target=&#34;_blank&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;i class=&#34;fab fa-creative-commons fa-2x&#34;&gt;&lt;/i&gt;&lt;i class=&#34;fab fa-creative-commons-by fa-2x&#34;&gt;&lt;/i&gt;&lt;i class=&#34;fab fa-creative-commons-sa fa-2x&#34;&gt;&lt;/i&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
