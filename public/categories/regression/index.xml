<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Regression on Jan Kirenz</title>
    <link>/categories/regression/</link>
    <description>Recent content in Regression on Jan Kirenz</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Jan Kirenz, {year}</copyright>
    <lastBuildDate>Mon, 12 Aug 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/categories/regression/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Lasso Regression with Python</title>
      <link>/post/2019-08-lasso-regression-with-python/</link>
      <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-08-lasso-regression-with-python/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lasso-regression-basics&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Lasso Regression Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-of-lasso-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Implementation of Lasso regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#standardization&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Standardization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#split-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Split data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lasso-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.3&lt;/span&gt; Lasso regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lasso-with-different-lambdas&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.4&lt;/span&gt; Lasso with different lambdas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-values-as-a-function-of-lambda&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.5&lt;/span&gt; Plot values as a function of lambda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#identify-best-lambda-and-coefficients&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.6&lt;/span&gt; Identify best lambda and coefficients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cross-validation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.7&lt;/span&gt; Cross Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.8&lt;/span&gt; Best Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;lasso-regression-basics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Lasso Regression Basics&lt;/h1&gt;
&lt;p&gt;Lasso performs a so called &lt;code&gt;L1 regularization&lt;/code&gt; (a process of introducing additional information in order to prevent overfitting), i.e. adds penalty equivalent to absolute value of the magnitude of coefficients.&lt;/p&gt;
&lt;p&gt;In particular, the minimization objective does not only include the residual sum of squares (RSS) - like in the OLS regression setting - but also the sum of the absolute value of coefficients.&lt;/p&gt;
&lt;p&gt;The residual sum of squares (RSS) is calculated as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ RSS = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This formula can be stated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ RSS = \sum_{i=1}^{n} \bigg(y_i - \big( \beta_{0} + \sum_{j=1}^{p} \beta_{j} x_{ij} \big) \bigg)^2  \]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;n represents the number of distinct data points, or observations, in our sample.&lt;/li&gt;
&lt;li&gt;p denotes the number of variables that are available in the dataset.&lt;/li&gt;
&lt;li&gt;x_{ij} represents the value of the jth variable for the ith observation, where i = 1, 2, . . ., n and j = 1, 2, . . . , p.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the lasso regression, the minimization objective becomes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sum_{i=1}^{n} \bigg(y_i - \big( \beta_{0} + \sum_{j=1}^{p} \beta_{j} x_{ij} \big) \bigg)^2 + \lambda \sum_{j=1}^{p} |\beta_j|   \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which equals:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[RSS + \lambda \sum_{j=1}^{p} |\beta_j|  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; (lambda) provides a trade-off between balancing RSS and magnitude of coefficients.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; can take various values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; = 0: Same coefficients as simple linear regression&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; = ∞: All coefficients zero (same logic as before)&lt;/li&gt;
&lt;li&gt;0 &amp;lt; &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; &amp;lt; ∞: coefficients between 0 and that of simple linear regression&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-of-lasso-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Implementation of Lasso regression&lt;/h1&gt;
&lt;p&gt;Python set up:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
plt.style.use(&amp;#39;ggplot&amp;#39;)
import warnings; warnings.simplefilter(&amp;#39;ignore&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This notebook involves the use of the Lasso regression on the “Auto” dataset. In particular, we only use observations 1 to 200 for our analysis. Furthermore, you can drop the &lt;code&gt;name&lt;/code&gt; variable.&lt;/p&gt;
&lt;p&gt;Import data:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df = pd.read_csv(&amp;quot;https://raw.githubusercontent.com/kirenz/datasets/master/Auto.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tidying data:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df = df.iloc[0:200]
df = df.drop([&amp;#39;name&amp;#39;], axis=1)
df.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
RangeIndex: 200 entries, 0 to 199
Data columns (total 8 columns):
mpg             200 non-null float64
cylinders       200 non-null int64
displacement    200 non-null float64
horsepower      200 non-null object
weight          200 non-null int64
acceleration    200 non-null float64
year            200 non-null int64
origin          200 non-null int64
dtypes: float64(3), int64(4), object(1)
memory usage: 12.6+ KB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df[&amp;#39;origin&amp;#39;] = pd.Categorical(df[&amp;#39;origin&amp;#39;])
df[&amp;#39;horsepower&amp;#39;] = pd.to_numeric(df[&amp;#39;horsepower&amp;#39;], errors=&amp;#39;coerce&amp;#39;)
print(df.isnull().sum())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mpg             0
cylinders       0
displacement    0
horsepower      2
weight          0
acceleration    0
year            0
origin          0
dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# drop missing cases
df = df.dropna()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use scikit learn to fit a Lasso regression &lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34;&gt;(see documentation)&lt;/a&gt; and follow a number of steps (note that scikit-learn uses &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; in their notation):&lt;/p&gt;
&lt;div id=&#34;standardization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Standardization&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Standardize the features with the module: &lt;code&gt;from sklearn.preprocessing import StandardScaler&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It is important to standardize the features by removing the mean and scaling to unit variance. The L1 (Lasso) and L2 (Ridge) regularizers of linear models assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs = df.astype(&amp;#39;int&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 198 entries, 0 to 199
Data columns (total 8 columns):
mpg             198 non-null int64
cylinders       198 non-null int64
displacement    198 non-null int64
horsepower      198 non-null int64
weight          198 non-null int64
acceleration    198 non-null int64
year            198 non-null int64
origin          198 non-null int64
dtypes: int64(8)
memory usage: 13.9 KB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs.columns&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Index([&amp;#39;mpg&amp;#39;, &amp;#39;cylinders&amp;#39;, &amp;#39;displacement&amp;#39;, &amp;#39;horsepower&amp;#39;, &amp;#39;weight&amp;#39;,
       &amp;#39;acceleration&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;origin&amp;#39;],
      dtype=&amp;#39;object&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
dfs[[&amp;#39;cylinders&amp;#39;, &amp;#39;displacement&amp;#39;, &amp;#39;horsepower&amp;#39;,
     &amp;#39;weight&amp;#39;, &amp;#39;acceleration&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;origin&amp;#39;]] = scaler.fit_transform(dfs[[&amp;#39;cylinders&amp;#39;,
                                                                              &amp;#39;displacement&amp;#39;,
                                                                              &amp;#39;horsepower&amp;#39;,
                                                                              &amp;#39;weight&amp;#39;,
                                                                              &amp;#39;acceleration&amp;#39;,
                                                                              &amp;#39;year&amp;#39;, &amp;#39;origin&amp;#39;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs.head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
&lt;thead&gt;
&lt;tr style=&#34;text-align: right;&#34;&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
mpg
&lt;/th&gt;
&lt;th&gt;
cylinders
&lt;/th&gt;
&lt;th&gt;
displacement
&lt;/th&gt;
&lt;th&gt;
horsepower
&lt;/th&gt;
&lt;th&gt;
weight
&lt;/th&gt;
&lt;th&gt;
acceleration
&lt;/th&gt;
&lt;th&gt;
year
&lt;/th&gt;
&lt;th&gt;
origin
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;
0
&lt;/th&gt;
&lt;td&gt;
18
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
0.726091
&lt;/td&gt;
&lt;td&gt;
0.325216
&lt;/td&gt;
&lt;td&gt;
0.346138
&lt;/td&gt;
&lt;td&gt;
-0.955578
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
1
&lt;/th&gt;
&lt;td&gt;
15
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
1.100254
&lt;/td&gt;
&lt;td&gt;
1.129264
&lt;/td&gt;
&lt;td&gt;
0.548389
&lt;/td&gt;
&lt;td&gt;
-1.305309
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
2
&lt;/th&gt;
&lt;td&gt;
18
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
0.821807
&lt;/td&gt;
&lt;td&gt;
0.784672
&lt;/td&gt;
&lt;td&gt;
0.273370
&lt;/td&gt;
&lt;td&gt;
-1.305309
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
3
&lt;/th&gt;
&lt;td&gt;
16
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
0.699986
&lt;/td&gt;
&lt;td&gt;
0.784672
&lt;/td&gt;
&lt;td&gt;
0.270160
&lt;/td&gt;
&lt;td&gt;
-0.955578
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
4
&lt;/th&gt;
&lt;td&gt;
17
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
0.682583
&lt;/td&gt;
&lt;td&gt;
0.554944
&lt;/td&gt;
&lt;td&gt;
0.287282
&lt;/td&gt;
&lt;td&gt;
-1.655041
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;split-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Split data&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Split the data set&lt;/strong&gt; into train and test sets (use &lt;code&gt;X_train&lt;/code&gt;, &lt;code&gt;X_test&lt;/code&gt;, &lt;code&gt;y_train&lt;/code&gt;, &lt;code&gt;y_test&lt;/code&gt;), with the first 75% of the data for training and the remaining for testing. (module: &lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;X = dfs.drop([&amp;#39;mpg&amp;#39;], axis=1)
y = dfs[&amp;#39;mpg&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Lasso regression&lt;/h2&gt;
&lt;p&gt;Apply &lt;strong&gt;Lasso regression&lt;/strong&gt; on the training set with the regularization parameter &lt;strong&gt;lambda = 0.5&lt;/strong&gt; (module: &lt;code&gt;from sklearn.linear_model import Lasso&lt;/code&gt;) and print the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;-score for the training and test set. Comment on your findings.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.linear_model import Lasso

reg = Lasso(alpha=0.5)
reg.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000,
normalize=False, positive=False, precompute=False, random_state=None,
selection=‘cyclic’, tol=0.0001, warm_start=False)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;#39;Lasso Regression: R^2 score on training set&amp;#39;, reg.score(X_train, y_train)*100)
print(&amp;#39;Lasso Regression: R^2 score on test set&amp;#39;, reg.score(X_test, y_test)*100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lasso Regression: R^2 score on training set 82.49741060950073
Lasso Regression: R^2 score on test set 85.49734440925533&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso-with-different-lambdas&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Lasso with different lambdas&lt;/h2&gt;
&lt;p&gt;Apply the &lt;strong&gt;Lasso regression&lt;/strong&gt; on the training set with the following &lt;strong&gt;λ parameters: (0.001, 0.01, 0.1, 0.5, 1, 2, 10)&lt;/strong&gt;. Evaluate the R^2 score for all the models you obtain on both the train and test sets.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;lambdas = (0.001, 0.01, 0.1, 0.5, 1, 2, 10)
l_num = 7
pred_num = X.shape[1]

# prepare data for enumerate
coeff_a = np.zeros((l_num, pred_num))
train_r_squared = np.zeros(l_num)
test_r_squared = np.zeros(l_num)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# enumerate through lambdas with index and i
for ind, i in enumerate(lambdas):    
    reg = Lasso(alpha = i)
    reg.fit(X_train, y_train)

    coeff_a[ind,:] = reg.coef_
    train_r_squared[ind] = reg.score(X_train, y_train)
    test_r_squared[ind] = reg.score(X_test, y_test)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-values-as-a-function-of-lambda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.5&lt;/span&gt; Plot values as a function of lambda&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Plot&lt;/strong&gt; all values for both data sets (train and test &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;-values) as a function of λ. Comment on your findings.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Plotting
plt.figure(figsize=(18, 8))
plt.plot(train_r_squared, &amp;#39;bo-&amp;#39;, label=r&amp;#39;$R^2$ Training set&amp;#39;, color=&amp;quot;darkblue&amp;quot;, alpha=0.6, linewidth=3)
plt.plot(test_r_squared, &amp;#39;bo-&amp;#39;, label=r&amp;#39;$R^2$ Test set&amp;#39;, color=&amp;quot;darkred&amp;quot;, alpha=0.6, linewidth=3)
plt.xlabel(&amp;#39;Lamda index&amp;#39;); plt.ylabel(r&amp;#39;$R^2$&amp;#39;)
plt.xlim(0, 6)
plt.title(r&amp;#39;Evaluate lasso regression with lamdas: 0 = 0.001, 1= 0.01, 2 = 0.1, 3 = 0.5, 4= 1, 5= 2, 6 = 10&amp;#39;)
plt.legend(loc=&amp;#39;best&amp;#39;)
plt.grid()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-12-python-lasso-regression-auto/output_27_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;identify-best-lambda-and-coefficients&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.6&lt;/span&gt; Identify best lambda and coefficients&lt;/h2&gt;
&lt;p&gt;Store your test data results in a DataFrame and indentify the lambda where the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; has it’s &lt;strong&gt;maximum value&lt;/strong&gt; in the &lt;strong&gt;test data&lt;/strong&gt;. Fit a Lasso model with this lambda parameter (use the training data) and obtain the corresponding &lt;strong&gt;regression coefficients&lt;/strong&gt;. Furthermore, obtain the &lt;strong&gt;mean squared error&lt;/strong&gt; for the test data of this model (module: &lt;code&gt;from sklearn.metrics import mean_squared_error&lt;/code&gt;)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_lam = pd.DataFrame(test_r_squared*100, columns=[&amp;#39;R_squared&amp;#39;])
df_lam[&amp;#39;lambda&amp;#39;] = (lambdas)
# returns the index of the row where column has maximum value.
df_lam.loc[df_lam[&amp;#39;R_squared&amp;#39;].idxmax()]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R_squared 88.105773
lambda 0.001000
Name: 0, dtype: float64&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Coefficients of best model
reg_best = Lasso(alpha = 0.1)
reg_best.fit(X_train, y_train)
reg_best.coef_&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;array([-0.35554113, -1.13104696, -0.00596296, -3.31741775, -0. ,
0.37914648, 0.74902885])&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, reg_best.predict(X_test))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3.586249592807347&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.7&lt;/span&gt; Cross Validation&lt;/h2&gt;
&lt;p&gt;Evaluate the performance of a &lt;strong&gt;Lasso regression&lt;/strong&gt; for different regularization parameters λ using &lt;strong&gt;5-fold cross validation&lt;/strong&gt; on the training set (module: &lt;code&gt;from sklearn.model_selection import cross_val_score&lt;/code&gt;) and plot the cross-validation (CV) &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; scores of the training and test data as a function of λ.&lt;/p&gt;
&lt;p&gt;Use the following lambda parameters:
l_min = 0.05
l_max = 0.2
l_num = 20
lambdas = np.linspace(l_min,l_max, l_num)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;l_min = 0.05
l_max = 0.2
l_num = 20
lambdas = np.linspace(l_min,l_max, l_num)

train_r_squared = np.zeros(l_num)
test_r_squared = np.zeros(l_num)

pred_num = X.shape[1]
coeff_a = np.zeros((l_num, pred_num))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import cross_val_score

for ind, i in enumerate(lambdas):    
    reg = Lasso(alpha = i)
    reg.fit(X_train, y_train)
    results = cross_val_score(reg, X, y, cv=5, scoring=&amp;quot;r2&amp;quot;)

    train_r_squared[ind] = reg.score(X_train, y_train)    
    test_r_squared[ind] = reg.score(X_test, y_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Plotting
plt.figure(figsize=(18, 8))
plt.plot(train_r_squared, &amp;#39;bo-&amp;#39;, label=r&amp;#39;$R^2$ Training set&amp;#39;, color=&amp;quot;darkblue&amp;quot;, alpha=0.6, linewidth=3)
plt.plot(test_r_squared, &amp;#39;bo-&amp;#39;, label=r&amp;#39;$R^2$ Test set&amp;#39;, color=&amp;quot;darkred&amp;quot;, alpha=0.6, linewidth=3)
plt.xlabel(&amp;#39;Lamda value&amp;#39;); plt.ylabel(r&amp;#39;$R^2$&amp;#39;)
plt.xlim(0, 19)
plt.title(r&amp;#39;Evaluate 5-fold cv with different lamdas&amp;#39;)
plt.legend(loc=&amp;#39;best&amp;#39;)
plt.grid()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-12-python-lasso-regression-auto/output_35_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;best-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.8&lt;/span&gt; Best Model&lt;/h2&gt;
&lt;p&gt;Finally, store your test data results in a DataFrame and identify the lambda where the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; has it’s &lt;strong&gt;maximum value&lt;/strong&gt; in the &lt;strong&gt;test data&lt;/strong&gt;. Fit a Lasso model with this lambda parameter (use the training data) and obtain the corresponding &lt;strong&gt;regression coefficients&lt;/strong&gt;. Furthermore, obtain the &lt;strong&gt;mean squared error&lt;/strong&gt; for the test data of this model (module: &lt;code&gt;from sklearn.metrics import mean_squared_error&lt;/code&gt;)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_lam = pd.DataFrame(test_r_squared*100, columns=[&amp;#39;R_squared&amp;#39;])
df_lam[&amp;#39;lambda&amp;#39;] = (lambdas)
# returns the index of the row where column has maximum value.
df_lam.loc[df_lam[&amp;#39;R_squared&amp;#39;].idxmax()]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R_squared 87.897525
lambda 0.050000
Name: 0, dtype: float64&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Best Model
reg_best = Lasso(alpha = 0.144737)
reg_best.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lasso(alpha=0.144737, copy_X=True, fit_intercept=True, max_iter=1000,
normalize=False, positive=False, precompute=False, random_state=None,
selection=‘cyclic’, tol=0.0001, warm_start=False)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.metrics import mean_squared_error

mean_squared_error(y_test, reg_best.predict(X_test))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3.635187490993961&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;reg_best.coef_&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;array([-0.34136411, -1.18223273, -0. , -3.27132984, 0. ,
0.33262331, 0.71385488])&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
