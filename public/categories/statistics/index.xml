<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on Jan Kirenz</title>
    <link>/categories/statistics/</link>
    <description>Recent content in Statistics on Jan Kirenz</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Jan Kirenz, {year}</copyright>
    <lastBuildDate>Thu, 15 Aug 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Social Network Analysis with Python</title>
      <link>/post/2019-08-13-network_analysis/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-08-13-network_analysis/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#social-network-analysis-with-networkx-in-python&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Social Network Analysis with NetworkX in Python&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#social-network-basics&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1&lt;/span&gt; Social Network Basics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#symmetric-networks-undirected&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1.1&lt;/span&gt; Symmetric Networks (undirected)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#asymmetric-networks-directed&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1.2&lt;/span&gt; Asymmetric Networks (directed)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weighted-networks&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1.3&lt;/span&gt; Weighted Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clustering-coefficient&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2&lt;/span&gt; Clustering coefficient&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#network-distance-measures&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3&lt;/span&gt; Network Distance Measures&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#degree&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3.1&lt;/span&gt; Degree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#distance&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3.2&lt;/span&gt; Distance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#breadth-first-search&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3.3&lt;/span&gt; Breadth-first search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eccentricity&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3.4&lt;/span&gt; Eccentricity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#centrality-measures&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.4&lt;/span&gt; Centrality measures&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#degree-centrality&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.4.1&lt;/span&gt; Degree Centrality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eigenvector-centrality&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.4.2&lt;/span&gt; Eigenvector Centrality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#closeness-centrality&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.4.3&lt;/span&gt; Closeness Centrality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#betweenness-centrality&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.4.4&lt;/span&gt; Betweenness Centrality&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#facebook-case-study&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.5&lt;/span&gt; Facebook Case Study&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;social-network-analysis-with-networkx-in-python&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Social Network Analysis with NetworkX in Python&lt;/h1&gt;
&lt;p&gt;We use the module &lt;a href=&#34;https://networkx.github.io/documentation/stable/&#34;&gt;NetworkX&lt;/a&gt; in this tutorial. It is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.&lt;/p&gt;
&lt;p&gt;If you work with &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;Anaconda&lt;/a&gt;, you can install the package as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install -c anaconda networkx&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Import modules:&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import networkx as nx
import matplotlib.pyplot as plt
%matplotlib inline
import warnings; warnings.simplefilter(&amp;#39;ignore&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;div id=&#34;social-network-basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Social Network Basics&lt;/h2&gt;
&lt;p&gt;Each network consists of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nodes: The individuals whose network we are building.&lt;/li&gt;
&lt;li&gt;Edges: The connection between the nodes. It represents a relationship between the nodes of the network.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;symmetric-networks-undirected&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1.1&lt;/span&gt; Symmetric Networks (undirected)&lt;/h3&gt;
&lt;p&gt;The first network that we create is a group of people who work together. This is called a &lt;strong&gt;symmetric network&lt;/strong&gt; because the relationship “working together” is a symmetric relationship: If A is related to B, B is also related to A.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;G_symmetric = nx.Graph()

G_symmetric.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;Laura&amp;#39;)
G_symmetric.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;Marc&amp;#39;)
G_symmetric.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;John&amp;#39;)
G_symmetric.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;Michelle&amp;#39;)
G_symmetric.add_edge(&amp;#39;Laura&amp;#39;,   &amp;#39;Michelle&amp;#39;)
G_symmetric.add_edge(&amp;#39;Michelle&amp;#39;,&amp;#39;Marc&amp;#39;)
G_symmetric.add_edge(&amp;#39;George&amp;#39;,  &amp;#39;John&amp;#39;)
G_symmetric.add_edge(&amp;#39;George&amp;#39;,  &amp;#39;Steven&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(nx.info(G_symmetric))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Name:
Type: Graph
Number of nodes: 6
Number of edges: 8
Average degree:   2.6667&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we visualize the network with the &lt;code&gt;draw_networkx()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.figure(figsize=(5,5))
nx.draw_networkx(G_symmetric);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-13-network_analysis/output_8_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;asymmetric-networks-directed&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1.2&lt;/span&gt; Asymmetric Networks (directed)&lt;/h3&gt;
&lt;p&gt;What if the relationship between nodes is ‘child of’, then the relationship is no longer symmetric. This is the case if someone follows someone else on Twitter. Or in the case of hyperlinks.&lt;/p&gt;
&lt;p&gt;If A is the child of B, then B is not a child of A. Such a network where the relationship is &lt;strong&gt;asymmetric&lt;/strong&gt; (A is related to B, does not necessarily means that B is associated with A) is called an Asymmetric network.&lt;/p&gt;
&lt;p&gt;We can build the asymmetric network in NetworkX using &lt;code&gt;DiGraph&lt;/code&gt; method, which is short of &lt;strong&gt;Directional Graph&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;G_asymmetric = nx.DiGraph()
G_asymmetric.add_edge(&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;)
G_asymmetric.add_edge(&amp;#39;A&amp;#39;,&amp;#39;D&amp;#39;)
G_asymmetric.add_edge(&amp;#39;C&amp;#39;,&amp;#39;A&amp;#39;)
G_asymmetric.add_edge(&amp;#39;D&amp;#39;,&amp;#39;E&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make sure that all nodes are distinctly visible in the network, use the &lt;code&gt;spring_layout()&lt;/code&gt; function, followed by the &lt;code&gt;draw_networkx()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.spring_layout(G_asymmetric)
nx.draw_networkx(G_asymmetric)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-13-network_analysis/output_12_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weighted-networks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1.3&lt;/span&gt; Weighted Networks&lt;/h3&gt;
&lt;p&gt;Till now we had networks without weights, but it is possible that networks are made with weights, for example, if in our initial network we consider the number of projects done together as a weight, we will get a weighted Network.&lt;/p&gt;
&lt;p&gt;Let us make one again of the employees, but this time we add weight to the network, each edge has a weight signifying the number of projects they have done together.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;G_weighted = nx.Graph()

G_weighted.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;Laura&amp;#39;,   weight=25)
G_weighted.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;Marc&amp;#39;,    weight=8)
G_weighted.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;John&amp;#39;,    weight=11)
G_weighted.add_edge(&amp;#39;Steven&amp;#39;,  &amp;#39;Michelle&amp;#39;,weight=1)
G_weighted.add_edge(&amp;#39;Laura&amp;#39;,   &amp;#39;Michelle&amp;#39;,weight=1)
G_weighted.add_edge(&amp;#39;Michelle&amp;#39;,&amp;#39;Marc&amp;#39;,    weight=1)
G_weighted.add_edge(&amp;#39;George&amp;#39;,  &amp;#39;John&amp;#39;,    weight=8)
G_weighted.add_edge(&amp;#39;George&amp;#39;,  &amp;#39;Steven&amp;#39;,  weight=4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;elarge = [(u, v) for (u, v, d) in G_weighted.edges(data=True) if d[&amp;#39;weight&amp;#39;] &amp;gt; 8]
esmall = [(u, v) for (u, v, d) in G_weighted.edges(data=True) if d[&amp;#39;weight&amp;#39;] &amp;lt;= 8]

pos = nx.circular_layout(G_weighted)  # positions for all nodes

# nodes
nx.draw_networkx_nodes(G_weighted, pos, node_size=700)

# edges
nx.draw_networkx_edges(G_weighted, pos, edgelist=elarge,width=6)
nx.draw_networkx_edges(G_weighted, pos, edgelist=esmall,width=6, alpha=0.5, edge_color=&amp;#39;b&amp;#39;, style=&amp;#39;dashed&amp;#39;)

# labels
nx.draw_networkx_labels(G_weighted, pos, font_size=20, font_family=&amp;#39;sans-serif&amp;#39;)

plt.axis(&amp;#39;off&amp;#39;)
plt.show();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-13-network_analysis/output_15_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;clustering-coefficient&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Clustering coefficient&lt;/h2&gt;
&lt;p&gt;It is observed that people who share connections in a social network tend to form associations. In other words, there is a tendency in a social network to form clusters.&lt;/p&gt;
&lt;p&gt;We can determine the clusters of a node, &lt;strong&gt;local clustering coefficient&lt;/strong&gt;, which is the fraction of pairs of the node’s friends (that is connections) that are connected with each other.&lt;/p&gt;
&lt;p&gt;To determine the local clustering coefficient, we make use of &lt;code&gt;nx.clustering(Graph, Node)&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;In the symmetric employee-network, you will find that Michelle has a local clustering coefficient of 0.67 and Laura has a local clustering coefficient of 1.&lt;/p&gt;
&lt;p&gt;The average clustering coefficient (sum of all the local clustering coefficients divided by the number of nodes) for the symmetric employee-network is 0.867.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.clustering(G_symmetric,&amp;#39;Michelle&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.6666666666666666&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.clustering(G_symmetric,&amp;#39;Laura&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.average_clustering(G_symmetric)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.8277777777777778&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;network-distance-measures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; Network Distance Measures&lt;/h2&gt;
&lt;div id=&#34;degree&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3.1&lt;/span&gt; Degree&lt;/h3&gt;
&lt;p&gt;Degree of a node defines the number of connections a node has. NetworkX has the function &lt;code&gt;degree&lt;/code&gt; which we can use to determine the degree of a node in the network.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.degree(G_symmetric, &amp;#39;Michelle&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will return a value of 3, as Michelle has worked with three employees in the network.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3.2&lt;/span&gt; Distance&lt;/h3&gt;
&lt;p&gt;We can also determine the shortest path between two nodes and its length in NetworkX using &lt;code&gt;nx.shortest_path(Graph, Node1, Node2)&lt;/code&gt; and &lt;code&gt;nx.shortest_path_length(Graph, Node1, Node2)&lt;/code&gt;
functions respectively.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.shortest_path(G_symmetric, &amp;#39;Michelle&amp;#39;, &amp;#39;John&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;Michelle&amp;#39;, &amp;#39;Steven&amp;#39;, &amp;#39;John&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.shortest_path_length(G_symmetric, &amp;#39;Michelle&amp;#39;, &amp;#39;John&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;breadth-first-search&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3.3&lt;/span&gt; Breadth-first search&lt;/h3&gt;
&lt;p&gt;We can find the distance of a node from every other node in the network using breadth-first search algorithm, starting from that node. networkX provides the function bfs_tree to do it.&lt;/p&gt;
&lt;p&gt;And so if you use &lt;code&gt;M = nx.bfs_tree(G_symmetric, &#39;Michelle&#39;)&lt;/code&gt; and now draw this tree, we will get a network structure telling how we can reach other nodes of the network starting from Michelle .&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;S = nx.bfs_tree(G_symmetric, &amp;#39;Steven&amp;#39;)
nx.draw_networkx(S)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-13-network_analysis/output_29_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;M = nx.bfs_tree(G_symmetric, &amp;#39;Michelle&amp;#39;)
nx.draw_networkx(M)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-13-network_analysis/output_30_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;eccentricity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3.4&lt;/span&gt; Eccentricity&lt;/h3&gt;
&lt;p&gt;Eccentricity of a node A is defined as the largest distance between A and all other nodes.&lt;/p&gt;
&lt;p&gt;It can be found using &lt;code&gt;nx.eccentricity()&lt;/code&gt; function. In the symmetric employee-network, Michelle has an eccentricity of 2, and Steven has an eccentricity of 1 (he is connected to every other node).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.eccentricity(G_symmetric,&amp;#39;Michelle&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.eccentricity(G_symmetric,&amp;#39;Steven&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;centrality-measures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4&lt;/span&gt; Centrality measures&lt;/h2&gt;
&lt;p&gt;Above we learned some of the network distance measures and they are useful in knowing how the information will spread through the network.&lt;/p&gt;
&lt;p&gt;In this section, we will learn how to find the most important nodes (individuals) in the network. These parameters are called as &lt;strong&gt;centrality measures&lt;/strong&gt;. Centrality Measures can help us in identifying popularity, most liked, and biggest influencers within the network.&lt;/p&gt;
&lt;div id=&#34;degree-centrality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4.1&lt;/span&gt; Degree Centrality&lt;/h3&gt;
&lt;p&gt;The people most popular or more liked usually are the ones who have more friends.&lt;/p&gt;
&lt;p&gt;Degree centrality is a measure of the number of connections a particular node has in the network. It is based on the fact that important nodes have many connections. NetworkX has the function &lt;code&gt;degree_centrality()&lt;/code&gt; to calculate the degree centrality of all the nodes of a network.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.degree_centrality(G_symmetric)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;#39;Steven&amp;#39;: 1.0,
 &amp;#39;Laura&amp;#39;: 0.4,
 &amp;#39;Marc&amp;#39;: 0.4,
 &amp;#39;John&amp;#39;: 0.4,
 &amp;#39;Michelle&amp;#39;: 0.6000000000000001,
 &amp;#39;George&amp;#39;: 0.4}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;eigenvector-centrality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4.2&lt;/span&gt; Eigenvector Centrality&lt;/h3&gt;
&lt;p&gt;It is not just how many individuals one is connected too, but the type of people one is connected with that can decide the importance of a node.&lt;/p&gt;
&lt;p&gt;Eigenvector centrality is a measure of how import a node is by accounting for the fact of how well it is connected to other important nodes.&lt;/p&gt;
&lt;p&gt;We can use the &lt;code&gt;eigenvector_centrality()&lt;/code&gt; function of NetworkX to calculate eigenvector centrality of all the nodes in a network.&lt;/p&gt;
&lt;p&gt;The Google’s Pagerank algorithm is a variant of Eigenvector centrality algorithm.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.eigenvector_centrality(G_symmetric)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;#39;Steven&amp;#39;: 0.6006686104947806,
 &amp;#39;Laura&amp;#39;: 0.3545677660798074,
 &amp;#39;Marc&amp;#39;: 0.3545677660798074,
 &amp;#39;John&amp;#39;: 0.30844592433424667,
 &amp;#39;Michelle&amp;#39;: 0.4443904166426225,
 &amp;#39;George&amp;#39;: 0.30844592433424667}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;closeness-centrality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4.3&lt;/span&gt; Closeness Centrality&lt;/h3&gt;
&lt;p&gt;Closeness Centrality is a measure where each node’s importance is determined by &lt;strong&gt;closeness to all other nodes&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.closeness_centrality(G_symmetric)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;#39;Steven&amp;#39;: 1.0,
 &amp;#39;Laura&amp;#39;: 0.625,
 &amp;#39;Marc&amp;#39;: 0.625,
 &amp;#39;John&amp;#39;: 0.625,
 &amp;#39;Michelle&amp;#39;: 0.7142857142857143,
 &amp;#39;George&amp;#39;: 0.625}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;betweenness-centrality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4.4&lt;/span&gt; Betweenness Centrality&lt;/h3&gt;
&lt;p&gt;The Betweenness Centrality is the centrality of control.&lt;/p&gt;
&lt;p&gt;It represents the frequency at which a point occurs on the &lt;strong&gt;shortest paths&lt;/strong&gt; that connected pair of points. It quantifies how many times a particular node comes in the shortest chosen path between two other nodes.&lt;/p&gt;
&lt;p&gt;The nodes with high betweenness centrality play a significant role in the communication/information flow within the network.&lt;/p&gt;
&lt;p&gt;The nodes with high betweenness centrality can have a strategic control and influence on others. An individual at such a strategic position can influence the whole group, by either withholding or coloring the information in transmission.&lt;/p&gt;
&lt;p&gt;Networkx has the function &lt;code&gt;betweenness_centrality()&lt;/code&gt; to measure it for the network. It has options to select if we want betweenness values to be normalized or not, weights to be included in centrality calculation or not, and to include the endpoints in the shortest path counts or not.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;nx.betweenness_centrality(G_symmetric)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&amp;#39;Steven&amp;#39;: 0.65,
 &amp;#39;Laura&amp;#39;: 0.0,
 &amp;#39;Marc&amp;#39;: 0.0,
 &amp;#39;John&amp;#39;: 0.0,
 &amp;#39;Michelle&amp;#39;: 0.05,
 &amp;#39;George&amp;#39;: 0.0}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pos = nx.spring_layout(G_symmetric)
betCent = nx.betweenness_centrality(G_symmetric, normalized=True, endpoints=True)
node_color = [20000.0 * G_symmetric.degree(v) for v in G_symmetric]
node_size =  [v * 10000 for v in betCent.values()]
plt.figure(figsize=(10,10))
nx.draw_networkx(G_symmetric, pos=pos, with_labels=True,
                 node_color=node_color,
                 node_size=node_size )
plt.axis(&amp;#39;off&amp;#39;);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-13-network_analysis/output_45_0.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sorted(betCent, key=betCent.get, reverse=True)[:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;Steven&amp;#39;, &amp;#39;Michelle&amp;#39;, &amp;#39;Laura&amp;#39;, &amp;#39;Marc&amp;#39;, &amp;#39;John&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;facebook-case-study&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.5&lt;/span&gt; Facebook Case Study&lt;/h2&gt;
&lt;p&gt;This dataset consists of ‘circles’ (or ‘friends lists’) from Facebook. Facebook data was collected from survey participants using this Facebook app. The dataset includes node features (profiles), circles, and ego networks.&lt;/p&gt;
&lt;p&gt;Facebook data has been anonymized by replacing the Facebook-internal ids for each user with a new value. Also, while feature vectors from this dataset have been provided, the interpretation of those features has been obscured. For instance, where the original dataset may have contained a feature “political=Democratic Party”, the new data would simply contain “political=anonymized feature 1”. Thus, using the anonymized data it is possible to determine whether two users have the same political affiliations, but not what their individual political affiliations represent.&lt;/p&gt;
&lt;p&gt;Source: &lt;a href=&#34;https://snap.stanford.edu/data/egonets-Facebook.html&#34;&gt;J. McAuley and J. Leskovec. Learning to Discover Social Circles in Ego Networks. NIPS, 2012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let us start with the Facebook data, for our analysis here we will use Facebook combined ego networks dataset, it contains the aggregated network of ten individuals’ Facebook friends list. You can download the required facebook_combined.txt file from the Stanford University site.&lt;/p&gt;
&lt;p&gt;We read in the file and construct the Graph:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.dropbox.com/s/k34phmodh9nsy9r/facebook_combined.txt?dl=0&#34;&gt;Download the file&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd

df = pd.read_csv(&amp;#39;/Users/jankirenz/Dropbox/Data/facebook_combined.txt&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
RangeIndex: 88233 entries, 0 to 88232
Data columns (total 1 columns):
0 1    88233 non-null object
dtypes: object(1)
memory usage: 689.4+ KB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.tail()&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
&lt;thead&gt;
&lt;tr style=&#34;text-align: right;&#34;&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
0 1
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;
88228
&lt;/th&gt;
&lt;td&gt;
4026 4030
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
88229
&lt;/th&gt;
&lt;td&gt;
4027 4031
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
88230
&lt;/th&gt;
&lt;td&gt;
4027 4032
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
88231
&lt;/th&gt;
&lt;td&gt;
4027 4038
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
88232
&lt;/th&gt;
&lt;td&gt;
4031 4038
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;G_fb = nx.read_edgelist(&amp;quot;/Users/jankirenz/Dropbox/Data/facebook_combined.txt&amp;quot;, create_using = nx.Graph(), nodetype=int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(nx.info(G_fb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Name:
Type: Graph
Number of nodes: 4039
Number of edges: 88234
Average degree: 43.6910&lt;/p&gt;
&lt;p&gt;The network consists of 4,039 nodes, connected via 88,234 edges.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.figure(figsize=(20,20))
nx.draw_networkx(G_fb);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-13-network_analysis/net1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also visualize the network such that the node color varies with Degree and node size with Betweenness Centrality. The code to do this is:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pos = nx.spring_layout(G_fb)
betCent = nx.betweenness_centrality(G_fb, normalized=True, endpoints=True)
node_color = [20000.0 * G_fb.degree(v) for v in G_fb]
node_size =  [v * 10000 for v in betCent.values()]
plt.figure(figsize=(20,20))
nx.draw_networkx(G_fb, pos=pos, with_labels=False,
                 node_color=node_color,
                 node_size=node_size )
plt.axis(&amp;#39;off&amp;#39;);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-13-network_analysis/net2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can also know the labels of the nodes with the highest betweenness centrality using:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sorted(betCent, key=betCent.get, reverse=True)[:5]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that some nodes are common between Degree Centrality, which is a measure of degree, and Betweenness Centrality which controls the information flow.&lt;/p&gt;
&lt;p&gt;It is natural that nodes that are more connected also lie on shortest paths between other nodes. The node 1912 is an important node as it is crucial according to all three centrality measures that we had considered.&lt;/p&gt;
&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;Sources of examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/community/tutorials/social-network-analysis-python&#34;&gt;Datacamp&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aksakalli.github.io/2017/07/17/network-centrality-measures-and-their-visualization.html&#34;&gt;Aksakalli, C.&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://snap.stanford.edu/data/egonets-Facebook.html&#34;&gt;McAuley, J. &amp;amp; Leskovec, J.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Lasso Regression with Python</title>
      <link>/post/2019-08-12-python-lasso-regression-auto/</link>
      <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-08-12-python-lasso-regression-auto/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lasso-regression-basics&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Lasso Regression Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-of-lasso-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Implementation of Lasso regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#standardization&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Standardization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#split-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Split data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lasso-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.3&lt;/span&gt; Lasso regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lasso-with-different-lambdas&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.4&lt;/span&gt; Lasso with different lambdas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-values-as-a-function-of-lambda&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.5&lt;/span&gt; Plot values as a function of lambda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#identify-best-lambda-and-coefficients&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.6&lt;/span&gt; Identify best lambda and coefficients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cross-validation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.7&lt;/span&gt; Cross Validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.8&lt;/span&gt; Best Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;lasso-regression-basics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Lasso Regression Basics&lt;/h1&gt;
&lt;p&gt;Lasso performs a so called &lt;code&gt;L1 regularization&lt;/code&gt; (a process of introducing additional information in order to prevent overfitting), i.e. adds penalty equivalent to absolute value of the magnitude of coefficients.&lt;/p&gt;
&lt;p&gt;In particular, the minimization objective does not only include the residual sum of squares (RSS) - like in the OLS regression setting - but also the sum of the absolute value of coefficients.&lt;/p&gt;
&lt;p&gt;The residual sum of squares (RSS) is calculated as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ RSS = \sum_{i=1}^{n} (y_i - \hat{y_i})^2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This formula can be stated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ RSS = \sum_{i=1}^{n} \bigg(y_i - \big( \beta_{0} + \sum_{j=1}^{p} \beta_{j} x_{ij} \big) \bigg)^2  \]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;n represents the number of distinct data points, or observations, in our sample.&lt;/li&gt;
&lt;li&gt;p denotes the number of variables that are available in the dataset.&lt;/li&gt;
&lt;li&gt;x_{ij} represents the value of the jth variable for the ith observation, where i = 1, 2, . . ., n and j = 1, 2, . . . , p.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the lasso regression, the minimization objective becomes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sum_{i=1}^{n} \bigg(y_i - \big( \beta_{0} + \sum_{j=1}^{p} \beta_{j} x_{ij} \big) \bigg)^2 + \lambda \sum_{j=1}^{p} |\beta_j|   \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which equals:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[RSS + \lambda \sum_{j=1}^{p} |\beta_j|  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; (lambda) provides a trade-off between balancing RSS and magnitude of coefficients.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; can take various values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; = 0: Same coefficients as simple linear regression&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; = ∞: All coefficients zero (same logic as before)&lt;/li&gt;
&lt;li&gt;0 &amp;lt; &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; &amp;lt; ∞: coefficients between 0 and that of simple linear regression&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-of-lasso-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Implementation of Lasso regression&lt;/h1&gt;
&lt;p&gt;Python set up:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
plt.style.use(&amp;#39;ggplot&amp;#39;)
import warnings; warnings.simplefilter(&amp;#39;ignore&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This notebook involves the use of the Lasso regression on the “Auto” dataset. In particular, we only use observations 1 to 200 for our analysis. Furthermore, you can drop the &lt;code&gt;name&lt;/code&gt; variable.&lt;/p&gt;
&lt;p&gt;Import data:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df = pd.read_csv(&amp;quot;https://raw.githubusercontent.com/kirenz/datasets/master/Auto.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tidying data:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df = df.iloc[0:200]
df = df.drop([&amp;#39;name&amp;#39;], axis=1)
df.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
RangeIndex: 200 entries, 0 to 199
Data columns (total 8 columns):
mpg             200 non-null float64
cylinders       200 non-null int64
displacement    200 non-null float64
horsepower      200 non-null object
weight          200 non-null int64
acceleration    200 non-null float64
year            200 non-null int64
origin          200 non-null int64
dtypes: float64(3), int64(4), object(1)
memory usage: 12.6+ KB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df[&amp;#39;origin&amp;#39;] = pd.Categorical(df[&amp;#39;origin&amp;#39;])
df[&amp;#39;horsepower&amp;#39;] = pd.to_numeric(df[&amp;#39;horsepower&amp;#39;], errors=&amp;#39;coerce&amp;#39;)
print(df.isnull().sum())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mpg             0
cylinders       0
displacement    0
horsepower      2
weight          0
acceleration    0
year            0
origin          0
dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# drop missing cases
df = df.dropna()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use scikit learn to fit a Lasso regression &lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34;&gt;(see documentation)&lt;/a&gt; and follow a number of steps:&lt;/p&gt;
&lt;div id=&#34;standardization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Standardization&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Standardize&lt;/strong&gt; the features (module: &lt;code&gt;from sklearn.preprocessing import StandardScaler&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hint: It is important to standardize the features by removing the mean and scaling to unit variance. The L1 (Lasso) and L2 (Ridge) regularizers of linear models assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs = df.astype(&amp;#39;int&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
Int64Index: 198 entries, 0 to 199
Data columns (total 8 columns):
mpg             198 non-null int64
cylinders       198 non-null int64
displacement    198 non-null int64
horsepower      198 non-null int64
weight          198 non-null int64
acceleration    198 non-null int64
year            198 non-null int64
origin          198 non-null int64
dtypes: int64(8)
memory usage: 13.9 KB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs.columns&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Index([&amp;#39;mpg&amp;#39;, &amp;#39;cylinders&amp;#39;, &amp;#39;displacement&amp;#39;, &amp;#39;horsepower&amp;#39;, &amp;#39;weight&amp;#39;,
       &amp;#39;acceleration&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;origin&amp;#39;],
      dtype=&amp;#39;object&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
dfs[[&amp;#39;cylinders&amp;#39;, &amp;#39;displacement&amp;#39;, &amp;#39;horsepower&amp;#39;,
     &amp;#39;weight&amp;#39;, &amp;#39;acceleration&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;origin&amp;#39;]] = scaler.fit_transform(dfs[[&amp;#39;cylinders&amp;#39;,
                                                                              &amp;#39;displacement&amp;#39;,
                                                                              &amp;#39;horsepower&amp;#39;,
                                                                              &amp;#39;weight&amp;#39;,
                                                                              &amp;#39;acceleration&amp;#39;,
                                                                              &amp;#39;year&amp;#39;, &amp;#39;origin&amp;#39;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs.head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
&lt;thead&gt;
&lt;tr style=&#34;text-align: right;&#34;&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
mpg
&lt;/th&gt;
&lt;th&gt;
cylinders
&lt;/th&gt;
&lt;th&gt;
displacement
&lt;/th&gt;
&lt;th&gt;
horsepower
&lt;/th&gt;
&lt;th&gt;
weight
&lt;/th&gt;
&lt;th&gt;
acceleration
&lt;/th&gt;
&lt;th&gt;
year
&lt;/th&gt;
&lt;th&gt;
origin
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;
0
&lt;/th&gt;
&lt;td&gt;
18
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
0.726091
&lt;/td&gt;
&lt;td&gt;
0.325216
&lt;/td&gt;
&lt;td&gt;
0.346138
&lt;/td&gt;
&lt;td&gt;
-0.955578
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
1
&lt;/th&gt;
&lt;td&gt;
15
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
1.100254
&lt;/td&gt;
&lt;td&gt;
1.129264
&lt;/td&gt;
&lt;td&gt;
0.548389
&lt;/td&gt;
&lt;td&gt;
-1.305309
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
2
&lt;/th&gt;
&lt;td&gt;
18
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
0.821807
&lt;/td&gt;
&lt;td&gt;
0.784672
&lt;/td&gt;
&lt;td&gt;
0.273370
&lt;/td&gt;
&lt;td&gt;
-1.305309
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
3
&lt;/th&gt;
&lt;td&gt;
16
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
0.699986
&lt;/td&gt;
&lt;td&gt;
0.784672
&lt;/td&gt;
&lt;td&gt;
0.270160
&lt;/td&gt;
&lt;td&gt;
-0.955578
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
4
&lt;/th&gt;
&lt;td&gt;
17
&lt;/td&gt;
&lt;td&gt;
1.179744
&lt;/td&gt;
&lt;td&gt;
0.682583
&lt;/td&gt;
&lt;td&gt;
0.554944
&lt;/td&gt;
&lt;td&gt;
0.287282
&lt;/td&gt;
&lt;td&gt;
-1.655041
&lt;/td&gt;
&lt;td&gt;
-1.516818
&lt;/td&gt;
&lt;td&gt;
-0.629372
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;split-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Split data&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Split the data set&lt;/strong&gt; into train and test sets (use &lt;code&gt;X_train&lt;/code&gt;, &lt;code&gt;X_test&lt;/code&gt;, &lt;code&gt;y_train&lt;/code&gt;, &lt;code&gt;y_test&lt;/code&gt;), with the first 75% of the data for training and the remaining for testing. (module: &lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;X = dfs.drop([&amp;#39;mpg&amp;#39;], axis=1)
y = dfs[&amp;#39;mpg&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Lasso regression&lt;/h2&gt;
&lt;p&gt;Apply &lt;strong&gt;Lasso regression&lt;/strong&gt; on the training set with the regularization parameter &lt;strong&gt;lambda = 0.5&lt;/strong&gt; (module: &lt;code&gt;from sklearn.linear_model import Lasso&lt;/code&gt;) and print the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;-score for the training and test set. Comment on your findings.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.linear_model import Lasso

reg = Lasso(alpha=0.5)
reg.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000,
normalize=False, positive=False, precompute=False, random_state=None,
selection=‘cyclic’, tol=0.0001, warm_start=False)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;#39;Lasso Regression: R^2 score on training set&amp;#39;, reg.score(X_train, y_train)*100)
print(&amp;#39;Lasso Regression: R^2 score on test set&amp;#39;, reg.score(X_test, y_test)*100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lasso Regression: R^2 score on training set 82.49741060950073
Lasso Regression: R^2 score on test set 85.49734440925533&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso-with-different-lambdas&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Lasso with different lambdas&lt;/h2&gt;
&lt;p&gt;Apply the &lt;strong&gt;Lasso regression&lt;/strong&gt; on the training set with the following &lt;strong&gt;λ parameters: (0.001, 0.01, 0.1, 0.5, 1, 2, 10)&lt;/strong&gt;. Evaluate the R^2 score for all the models you obtain on both the train and test sets.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;lambdas = (0.001, 0.01, 0.1, 0.5, 1, 2, 10)
l_num = 7
pred_num = X.shape[1]

# prepare data for enumerate
coeff_a = np.zeros((l_num, pred_num))
train_r_squared = np.zeros(l_num)
test_r_squared = np.zeros(l_num)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# enumerate through lambdas with index and i
for ind, i in enumerate(lambdas):    
    reg = Lasso(alpha = i)
    reg.fit(X_train, y_train)

    coeff_a[ind,:] = reg.coef_
    train_r_squared[ind] = reg.score(X_train, y_train)
    test_r_squared[ind] = reg.score(X_test, y_test)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-values-as-a-function-of-lambda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.5&lt;/span&gt; Plot values as a function of lambda&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Plot&lt;/strong&gt; all values for both data sets (train and test &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;-values) as a function of λ. Comment on your findings.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Plotting
plt.figure(figsize=(18, 8))
plt.plot(train_r_squared, &amp;#39;bo-&amp;#39;, label=r&amp;#39;$R^2$ Training set&amp;#39;, color=&amp;quot;darkblue&amp;quot;, alpha=0.6, linewidth=3)
plt.plot(test_r_squared, &amp;#39;bo-&amp;#39;, label=r&amp;#39;$R^2$ Test set&amp;#39;, color=&amp;quot;darkred&amp;quot;, alpha=0.6, linewidth=3)
plt.xlabel(&amp;#39;Lamda index&amp;#39;); plt.ylabel(r&amp;#39;$R^2$&amp;#39;)
plt.xlim(0, 6)
plt.title(r&amp;#39;Evaluate lasso regression with lamdas: 0 = 0.001, 1= 0.01, 2 = 0.1, 3 = 0.5, 4= 1, 5= 2, 6 = 10&amp;#39;)
plt.legend(loc=&amp;#39;best&amp;#39;)
plt.grid()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-12-python-lasso-regression-auto/output_27_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;identify-best-lambda-and-coefficients&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.6&lt;/span&gt; Identify best lambda and coefficients&lt;/h2&gt;
&lt;p&gt;Store your test data results in a DataFrame and indentify the lambda where the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; has it’s &lt;strong&gt;maximum value&lt;/strong&gt; in the &lt;strong&gt;test data&lt;/strong&gt;. Fit a Lasso model with this lambda parameter (use the training data) and obtain the corresponding &lt;strong&gt;regression coefficients&lt;/strong&gt;. Furthermore, obtain the &lt;strong&gt;mean squared error&lt;/strong&gt; for the test data of this model (module: &lt;code&gt;from sklearn.metrics import mean_squared_error&lt;/code&gt;)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_lam = pd.DataFrame(test_r_squared*100, columns=[&amp;#39;R_squared&amp;#39;])
df_lam[&amp;#39;lambda&amp;#39;] = (lambdas)
# returns the index of the row where column has maximum value.
df_lam.loc[df_lam[&amp;#39;R_squared&amp;#39;].idxmax()]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R_squared 88.105773
lambda 0.001000
Name: 0, dtype: float64&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Coefficients of best model
reg_best = Lasso(alpha = 0.1)
reg_best.fit(X_train, y_train)
reg_best.coef_&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;array([-0.35554113, -1.13104696, -0.00596296, -3.31741775, -0. ,
0.37914648, 0.74902885])&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, reg_best.predict(X_test))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3.586249592807347&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.7&lt;/span&gt; Cross Validation&lt;/h2&gt;
&lt;p&gt;Evaluate the performance of a &lt;strong&gt;Lasso regression&lt;/strong&gt; for different regularization parameters λ using &lt;strong&gt;5-fold cross validation&lt;/strong&gt; on the training set (module: &lt;code&gt;from sklearn.model_selection import cross_val_score&lt;/code&gt;) and plot the cross-validation (CV) &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; scores of the training and test data as a function of λ.&lt;/p&gt;
&lt;p&gt;Use the following lambda parameters:
l_min = 0.05
l_max = 0.2
l_num = 20
lambdas = np.linspace(l_min,l_max, l_num)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;l_min = 0.05
l_max = 0.2
l_num = 20
lambdas = np.linspace(l_min,l_max, l_num)

train_r_squared = np.zeros(l_num)
test_r_squared = np.zeros(l_num)

pred_num = X.shape[1]
coeff_a = np.zeros((l_num, pred_num))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.model_selection import cross_val_score

for ind, i in enumerate(lambdas):    
    reg = Lasso(alpha = i)
    reg.fit(X_train, y_train)
    results = cross_val_score(reg, X, y, cv=5, scoring=&amp;quot;r2&amp;quot;)

    train_r_squared[ind] = reg.score(X_train, y_train)    
    test_r_squared[ind] = reg.score(X_test, y_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Plotting
plt.figure(figsize=(18, 8))
plt.plot(train_r_squared, &amp;#39;bo-&amp;#39;, label=r&amp;#39;$R^2$ Training set&amp;#39;, color=&amp;quot;darkblue&amp;quot;, alpha=0.6, linewidth=3)
plt.plot(test_r_squared, &amp;#39;bo-&amp;#39;, label=r&amp;#39;$R^2$ Test set&amp;#39;, color=&amp;quot;darkred&amp;quot;, alpha=0.6, linewidth=3)
plt.xlabel(&amp;#39;Lamda value&amp;#39;); plt.ylabel(r&amp;#39;$R^2$&amp;#39;)
plt.xlim(0, 19)
plt.title(r&amp;#39;Evaluate 5-fold cv with different lamdas&amp;#39;)
plt.legend(loc=&amp;#39;best&amp;#39;)
plt.grid()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-12-python-lasso-regression-auto/output_35_0.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;best-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.8&lt;/span&gt; Best Model&lt;/h2&gt;
&lt;p&gt;Finally, store your test data results in a DataFrame and identify the lambda where the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; has it’s &lt;strong&gt;maximum value&lt;/strong&gt; in the &lt;strong&gt;test data&lt;/strong&gt;. Fit a Lasso model with this lambda parameter (use the training data) and obtain the corresponding &lt;strong&gt;regression coefficients&lt;/strong&gt;. Furthermore, obtain the &lt;strong&gt;mean squared error&lt;/strong&gt; for the test data of this model (module: &lt;code&gt;from sklearn.metrics import mean_squared_error&lt;/code&gt;)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_lam = pd.DataFrame(test_r_squared*100, columns=[&amp;#39;R_squared&amp;#39;])
df_lam[&amp;#39;lambda&amp;#39;] = (lambdas)
# returns the index of the row where column has maximum value.
df_lam.loc[df_lam[&amp;#39;R_squared&amp;#39;].idxmax()]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R_squared 87.897525
lambda 0.050000
Name: 0, dtype: float64&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Best Model
reg_best = Lasso(alpha = 0.144737)
reg_best.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lasso(alpha=0.144737, copy_X=True, fit_intercept=True, max_iter=1000,
normalize=False, positive=False, precompute=False, random_state=None,
selection=‘cyclic’, tol=0.0001, warm_start=False)&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.metrics import mean_squared_error

mean_squared_error(y_test, reg_best.predict(X_test))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3.635187490993961&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;reg_best.coef_&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;array([-0.34136411, -1.18223273, -0. , -3.27132984, 0. ,
0.33262331, 0.71385488])&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deskriptive Statistik in R</title>
      <link>/post/2019-08-01-r-descriptive-statistics/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-08-01-r-descriptive-statistics/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#deskriptive-statistik-in-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Deskriptive Statistik in R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#datenimport&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1&lt;/span&gt; Datenimport&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deskriptive-statistiken&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2&lt;/span&gt; Deskriptive Statistiken&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mittelwert&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.1&lt;/span&gt; Mittelwert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standardabweichung&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.2&lt;/span&gt; Standardabweichung&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getrimmter-mittelwert&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.3&lt;/span&gt; Getrimmter Mittelwert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#schiefe&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.4&lt;/span&gt; Schiefe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kurtosis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.5&lt;/span&gt; Kurtosis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standardfehler&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.6&lt;/span&gt; Standardfehler&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;deskriptive-statistik-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Deskriptive Statistik in R&lt;/h1&gt;
&lt;p&gt;In diesem Beitrag wird die Berechnung einfacher deskriptiver Statistiken und die Visualisierung von Verteilungen in R am Beispiel des Datensatzes “Advertising” behandelt.&lt;/p&gt;
&lt;div id=&#34;datenimport&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Datenimport&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Datensatz: Advertising.csv&lt;/li&gt;
&lt;li&gt;Variablen: &lt;em&gt;TV&lt;/em&gt;, &lt;em&gt;radio&lt;/em&gt;, &lt;em&gt;newspaper&lt;/em&gt; = jeweils Werbeausgaben in Dollar; &lt;em&gt;sales&lt;/em&gt; = Produkte in Tausend Einheiten&lt;/li&gt;
&lt;li&gt;Abhängige Variable (dependent variable, response): &lt;em&gt;sales&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Unabhängige Variablen (independent variables, predictors): &lt;em&gt;TV&lt;/em&gt;, &lt;em&gt;radio&lt;/em&gt;, &lt;em&gt;newspaper&lt;/em&gt;, &lt;em&gt;sales&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Zunächts möchten wir uns einen Überblick über die Daten verschaffen. Dafür importieren wir die Daten und prüfen, ob die Skalenniveaus korrekt sind. Für die weiteren Berechnungen wird die Variable X1 nicht benötigt, weshalb wir diese löschen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
# Daten importieren
Advertising &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/kirenz/datasets/master/advertising.csv&amp;quot;)
# Überblick über die Daten verschaffen (Skalenniveaus prüfen)
head(Advertising)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##      X1    TV radio newspaper sales
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 230.   37.8      69.2  22.1
## 2     2  44.5  39.3      45.1  10.4
## 3     3  17.2  45.9      69.3   9.3
## 4     4 152.   41.3      58.5  18.5
## 5     5 181.   10.8      58.4  12.9
## 6     6   8.7  48.9      75     7.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Bereinigung der Daten
Advertising$X1 &amp;lt;- NULL&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;deskriptive-statistiken&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Deskriptive Statistiken&lt;/h2&gt;
&lt;p&gt;Ausgabe unterschiedlicher deskriptiver Statistiken:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)

psych::describe(Advertising) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           vars   n   mean    sd median trimmed    mad min   max range
## TV           1 200 147.04 85.85 149.75  147.20 108.82 0.7 296.4 295.7
## radio        2 200  23.26 14.85  22.90   23.00  19.79 0.0  49.6  49.6
## newspaper    3 200  30.55 21.78  25.75   28.41  23.13 0.3 114.0 113.7
## sales        4 200  14.02  5.22  12.90   13.78   4.82 1.6  27.0  25.4
##            skew kurtosis   se
## TV        -0.07    -1.24 6.07
## radio      0.09    -1.28 1.05
## newspaper  0.88     0.57 1.54
## sales      0.40    -0.45 0.37&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Hinweise zu den Kennzahlen:
&lt;ul&gt;
&lt;li&gt;vars: Nummer der Variable&lt;/li&gt;
&lt;li&gt;n: Anzahl der Beobachtungen&lt;/li&gt;
&lt;li&gt;mean: arithmetischer Mittelwert&lt;/li&gt;
&lt;li&gt;sd: empirische Standardabweichung&lt;/li&gt;
&lt;li&gt;median: Median&lt;/li&gt;
&lt;li&gt;trimmed: getrimmter Mittelwert&lt;/li&gt;
&lt;li&gt;mad: Mittlere absolute Abweichung vom Median&lt;/li&gt;
&lt;li&gt;min: kleinster Beobachtungswert&lt;/li&gt;
&lt;li&gt;max: größter Beobachtungswert&lt;/li&gt;
&lt;li&gt;range: Spannweite&lt;/li&gt;
&lt;li&gt;skew: Schiefe&lt;/li&gt;
&lt;li&gt;kurtosis: Wölbung&lt;/li&gt;
&lt;li&gt;se = Standardfehler&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;mittelwert&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.1&lt;/span&gt; Mittelwert&lt;/h3&gt;
&lt;p&gt;Bei der Berechnung des &lt;em&gt;arithmetischen Mittelwerts&lt;/em&gt; in R sollte immer die Anweisung gegeben werden, fehlende Werte auszuschließen (na.rm = “remove values which are not available”). Ansonsten stoppt R bei fehlenden Werten die Berechnung und gibt eine Fehlermeldung aus.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_sales &amp;lt;- mean(Advertising$sales, na.rm = TRUE)
print(paste0(&amp;quot;Mittelwert der Variable Sales: &amp;quot;, mean_sales))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Mittelwert der Variable Sales: 14.0225&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;standardabweichung&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.2&lt;/span&gt; Standardabweichung&lt;/h3&gt;
&lt;p&gt;Die Standardabweichung ist ein häufig verwendetes Streuungsmaß und beschreibt die mittlere Abweichung der einzelnen Messwerte vom empirischen Mittelwert. Die Standardabweichung ist die positive Wurzel der empirischen Varianz. Die Varianz einer Stichprobe wird wie folgt berechnet:
&lt;span class=&#34;math display&#34;&gt;\[s^{2} = \frac{\sum_{i=1}^{n} \left(x_{i} - \bar{x}\right)^{2}} {n-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Berechnung der Standardabweichung: &lt;span class=&#34;math display&#34;&gt;\[s = \sqrt{\frac{\sum\limits_{i=1}^{n} \left(x_{i} - \bar{x}\right)^{2}} {n-1}}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var_sales &amp;lt;- var(Advertising$sales, na.rm = TRUE)
print(paste0(&amp;quot;Varianz der Variable Sales: &amp;quot;, round(var_sales, 2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Varianz der Variable Sales: 27.22&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_sales &amp;lt;-  sd(Advertising$sales, na.rm = TRUE)
print(paste0(&amp;quot;Standardabweichung der Variable Sales: &amp;quot;, round(sd_sales,2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Standardabweichung der Variable Sales: 5.22&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;getrimmter-mittelwert&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.3&lt;/span&gt; Getrimmter Mittelwert&lt;/h3&gt;
&lt;p&gt;Bei dem &lt;em&gt;getrimmten Mittelwert&lt;/em&gt; wird ein bestimmer Anteil der größten und kleinsten Beobachtungen - hier oberhalb des 90% Quantils und unterhalb des 10 % Quantils - ignoriert. Damit sollen Ausreißer aus der Berechnung des Mittelwerts ausgeschlossen werden. Der getrimmte Mittelwert kann wie folgt in R berechnet werden:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_trim_sales &amp;lt;- mean(Advertising$sales, trim = 0.1, na.rm = TRUE)
print(paste0(&amp;quot;Getrimmter Mittelwert der Variable Sales: &amp;quot;, round(mean_trim_sales, 2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Getrimmter Mittelwert der Variable Sales: 13.78&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;schiefe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.4&lt;/span&gt; Schiefe&lt;/h3&gt;
&lt;p&gt;Die &lt;em&gt;Schiefe&lt;/em&gt; ist eine statistische Kennzahl, die die Art und Stärke der Asymmetrie einer Wahrscheinlichkeitsverteilung beschreibt. Sie zeigt an, ob und wie stark die Verteilung nach rechts (positive Schiefe) oder nach links (negative Schiefe) geneigt ist. Jede nicht symmetrische Verteilung heißt schief.&lt;/p&gt;
&lt;p&gt;Darstellung der Verteilung in einem Histogramm:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
# Vorlage für die Erstellung von plots in ggplot2 
plot_1 &amp;lt;-  theme_bw() +
        theme(axis.text.x = element_text(angle = 0, size = 8, family=&amp;quot;Arial&amp;quot;, colour=&amp;#39;black&amp;#39;),
        axis.text.y = element_text(angle = 0, size = 8, family=&amp;quot;Arial&amp;quot;, colour=&amp;#39;black&amp;#39;),
        axis.title = element_text(size=8, face=&amp;quot;bold&amp;quot;, family=&amp;quot;Arial&amp;quot;, colour=&amp;#39;black&amp;#39;),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title=element_text(hjust=0, size=10,  family=&amp;quot;Arial&amp;quot;, face=&amp;quot;bold&amp;quot;, colour=&amp;#39;black&amp;#39;))

ggplot(Advertising, aes(sales)) +
  geom_histogram(binwidth = 2, color=&amp;quot;red&amp;quot;, alpha=.2) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  labs(title=&amp;quot;Histogramm für Sales&amp;quot;, x=&amp;quot;Sales&amp;quot;, y=&amp;quot;Anzahl&amp;quot;) +
  plot_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-01-r-descriptive-statistics/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Darstellung der Verteilung in einer Dichtefunktion:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

ggplot(Advertising, aes(sales)) +
  geom_density(fill=&amp;quot;grey&amp;quot;,alpha=.2 ) +
  geom_vline(aes(xintercept=mean(sales, na.rm=TRUE)), color=&amp;quot;red&amp;quot;, linetype=&amp;quot;dotted&amp;quot;, size=0.6) +
  geom_vline(aes(xintercept=median(sales, na.rm=TRUE)), color=&amp;quot;red&amp;quot;, linetype=&amp;quot;dotted&amp;quot;, size=0.6) +
  geom_text(aes(x=median(sales), y=0.02), colour = &amp;quot;grey&amp;quot;, size =3,  
             label=round(mean(Advertising$sales), digits=2), hjust=-1, family=&amp;quot;Arial&amp;quot;) +
  geom_text(aes(x=mean(sales), y=0.02), hjust=-0.7, colour = &amp;quot;grey&amp;quot;, size = 3, label=&amp;quot;Mittelwert&amp;quot;, family=&amp;quot;Arial&amp;quot;) +
  geom_text(aes(x=median(sales), y=0.005), colour = &amp;quot;grey&amp;quot;, size =3, 
             label=round(median(Advertising$sales), digits=2), hjust=1 , family=&amp;quot;Arial&amp;quot;) +
  geom_text(aes(x=median(sales), y=0.01), colour = &amp;quot;grey&amp;quot;, size = 3, label=&amp;quot;Median&amp;quot;, hjust=1, family=&amp;quot;Arial&amp;quot;) +
  labs(x=&amp;quot;Produktabsatz (in Tausend Einheiten)&amp;quot;, y = &amp;quot;Dichte&amp;quot;, title = &amp;quot;Wahrscheinlichkeitsdichtefunktion&amp;quot;) +
  plot_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-01-r-descriptive-statistics/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In der Abbildung kann man erkennen, dass es sich um eine asymmetrische Verteilung handelt (d.h. es liegt eine Abweichung von der Normalverteilung vor). Konkret handelt es sich um eine rechtsschiefe Verteilung (Mittelwert &amp;gt; Median; Schiefe = + 0.40).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kurtosis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.5&lt;/span&gt; Kurtosis&lt;/h3&gt;
&lt;p&gt;Die Abweichung des Verlaufs einer Verteilung vom Verlauf einer Normalverteilung wird &lt;em&gt;Kurtosis&lt;/em&gt; (Wölbung) genannt. Sie gibt an, wie spitz die Kurve verläuft. Unterschieden wird zwischen positiver, spitz zulaufender (leptokurtische Verteilung) und negativer, flacher (platykurtische Verteilung) Kurtosis. Die Kurtosis zählt zu den zentralen Momenten einer Verteilung, mittels derer der Kurvenverlauf definiert wird. Eine Kurtosis mit Wert 0 ist normalgipflig (mesokurtisch), ein Wert größer 0 ist steilgipflig und ein Wert unter 0 ist flachgipflig.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standardfehler&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.6&lt;/span&gt; Standardfehler&lt;/h3&gt;
&lt;p&gt;Der &lt;em&gt;Standardfehler&lt;/em&gt; ein Maß für die durchschnittliche Abweichung des geschätzten Parameterwertes vom wahren Parameterwert. Je kleiner der Standardfehler ist, desto genauer kann der unbekannte Parameter der Population mit Hilfe der Schätzfunktion geschätzt werden. Der Standardfehler hängt unter anderem von dem Stichprobenumfang und der Varianz ab. Allgemein gilt: Je größer der Stichprobenumfang, desto kleiner der Standardfehler; je kleiner die Varianz, desto kleiner der Standardfehler.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
