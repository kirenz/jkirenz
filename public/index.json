[{"authors":["jan"],"categories":null,"content":"I\u0026rsquo;m a Professor at HdM Stuttgart, where I help students and organizations to learn and use data science, statistics, and machine learning with Python and R programming to extract meaningful information from data. Please feel free to contact me if you are interested in data science seminars or data science consulting.\n","date":1589414400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1589414400,"objectID":"2124dc84130bde003e7c6b380610af0e","permalink":"/authors/jan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jan/","section":"authors","summary":"I\u0026rsquo;m a Professor at HdM Stuttgart, where I help students and organizations to learn and use data science, statistics, and machine learning with Python and R programming to extract meaningful information from data. Please feel free to contact me if you are interested in data science seminars or data science consulting.","tags":null,"title":"Jan Kirenz","type":"authors"},{"authors":["jan"],"categories":null,"content":"I\u0026rsquo;m a Professor at HdM Stuttgart, where I help students and organizations to learn and use data science, statistics, and machine learning with Python and R programming to extract meaningful information from data.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e23cb71100d51c11123b1b2597de799d","permalink":"/authors/alison/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alison/","section":"authors","summary":"I\u0026rsquo;m a Professor at HdM Stuttgart, where I help students and organizations to learn and use data science, statistics, and machine learning with Python and R programming to extract meaningful information from data.","tags":null,"title":"Jan Kirenz","type":"authors"},{"authors":["Jan Kirenz"],"categories":["R"],"content":"          1 Introduction 1.1 The market-basket model 1.2 Association rules  2 Association measures 2.1 Support 2.2 Confidence 2.3 Lift  3 A-Priori Algorithm 4 Implementation in R 4.1 Transform data 4.2 Inspect data 4.3 A-Priori Algorithm 4.4 Set LHS and RHS 4.5 Visualizing association rules 4.6 Scatter-Plot 4.7 Interactive scatter-plot 4.8 Graph-based visualization 4.9 Parallel coordinate plot  5 References   Association rule mining is one of the most popular data mining methods. This kind of analysis is also called frequent itemset analysis, association analysis or association rule learning. To perform the analysis in R, we use the arules and arulesViz packages.\n1 Introduction In association analysis, we are usually interested in the absolute number of customer transactions (also called baskets) that contain a particular set of items (usually products). A typical application of association analysis is the analysis of consumer buying behavior in supermarkets and chain stores where they record the contents of shopping carts brought to the register for checkout. These transaction data are normally recorded by point-of-sale scanners and often consist of tuples of the form: {transaction ID, item ID, item ID, …}. By finding frequent itemsets, a retailer can learn what is commonly bought together and use this information to increase sales in several ways.\nImagine there is a pair of different products (which we call items), X and Y, that are frequently bought together in a store (Ng \u0026amp; Soo, 2017):\n Both X and Y can be placed on the same shelf, so that buyers of one item would be prompted to buy the other. Promotional discounts could be applied to just one out of the two items. Advertisements on X could be targeted at buyers who purchase Y. X and Y could be combined into a new product, such as having Y in flavors of X.  Note that online retailers like Amazon.com or online platforms like Spotify have little need for this kind of analysis, since it is designed to search for itemsets that appear frequently. If the online retailer was limited to frequent itemsets, they would miss all the opportunities that are present in the “long tail” to select advertisements for each customer individually (for example to recommend certain products or songs). Instead of searching for frequent itemsets, they use similarity search algorithms (like collaborative filtering) to detect similar customers that have a large fraction of their baskets in common, even if the absolute number of baskets is small. (Leskovec, Rajaraman, \u0026amp; Ullman, 2020)\n1.1 The market-basket model Association rule mining is based on the so called “market-basket” model of data. This is essentially a many-many relationship between two kinds of elements, called items and baskets (also called transactions) with some assumptions about the shape of the data (Leskovec, Rajaraman, \u0026amp; Ullman, 2020):\n div.blue { background-color:#e6f0ff; border-radius: 2px; padding: 10px;}   Each basket (i.e. transaction) consists of a set of items (usually products). Usually we assume that the number of items in a basket is small (much smaller than the total number of all items). The number of all baskets (transactions) is usually assumed to be very large. The data is assumed to be represented in a file consisting of a sequence of baskets (transactions).   \nTo illustrate the logic of association rule mining, let’s create a sequence of baskets (transactions) with a small number of items from different customers in a grocery store. Note that because we use a very simple example with only a few baskets and items, the results of the analysis will differ from the results we may obtain from a real world example. We save the data as a sequence of transactions with the name market_basket:\n# create a list of baskets market_basket \u0026lt;- list( c(\u0026quot;apple\u0026quot;, \u0026quot;beer\u0026quot;, \u0026quot;rice\u0026quot;, \u0026quot;meat\u0026quot;), c(\u0026quot;apple\u0026quot;, \u0026quot;beer\u0026quot;, \u0026quot;rice\u0026quot;), c(\u0026quot;apple\u0026quot;, \u0026quot;beer\u0026quot;), c(\u0026quot;apple\u0026quot;, \u0026quot;pear\u0026quot;), c(\u0026quot;milk\u0026quot;, \u0026quot;beer\u0026quot;, \u0026quot;rice\u0026quot;, \u0026quot;meat\u0026quot;), c(\u0026quot;milk\u0026quot;, \u0026quot;beer\u0026quot;, \u0026quot;rice\u0026quot;), c(\u0026quot;milk\u0026quot;, \u0026quot;beer\u0026quot;), c(\u0026quot;milk\u0026quot;, \u0026quot;pear\u0026quot;) ) # set transaction names (T1 to T8) names(market_basket) \u0026lt;- paste(\u0026quot;T\u0026quot;, c(1:8), sep = \u0026quot;\u0026quot;) Each basket includes so called itemsets (like {apple, beer, etc.}). You can observe that “apple” is bought together with “beer” in three transactions:\n Figure 1.1: Market basket example (Ng \u0026amp; Soo, 2017)  The frequent-itemsets problem is that of finding sets of items that appear in many of the baskets. Hence, a set of items that appears in many baskets is said to be “frequent”.\n 1.2 Association rules While we are interested in extracting frequent sets of items, this information is often presented as a collection of if–then rules, called association rules.\nThe form of an association rule is {X -\u0026gt; Y}, where {X} is a set of items and {Y} is an item. The implication of this association rule is that if all of the items in {X} appear in some basket, then {Y} is “likely” to appear in that basket as well.\n {X} is also called antecedent or left-hand-side (LHS) and {Y} is called consequent or right-hand-side (RHS).  An example association rule for products from Apple could be {Apple iPad, Apple iPad Cover} -\u0026gt; {Apple Pencil}, meaning that if Apple’s iPad and iPad Cover {X} are bought, customers are also likely to buy Apple’s Pencil {Y}. Notice that the logical implication symbol “-\u0026gt;” does not indicate a causal relationship between {X} and {Y}. It is merely an estimate of the conditional probability of {Y} given {X}.\nNow imagine a grocery store with tens of thousands of different products. We wouldn’t want to calculate all associations between every possible combination of products. Instead, we would want to select only potentially “relevant” rules from the set of all possible rules. Therefore, we use the measures support, confidence and lift to reduce the number of relationships we need to analyze:\n Support is an indication of how frequently a set of items appear in baskets. Confidence is an indication of how often the support-rule has been found to be true. Lift is a measure of association using both support and confidence.   \nIf we are looking for association rules {X -\u0026gt; Y} that apply to a reasonable fraction of the baskets, then the support of X must be reasonably high. In practice, such as for marketing in brick-and-mortar stores, “reasonably high” is often around 1% to 10% of the baskets. We also want the conﬁdence of the rule to be reasonably high, perhaps 50%, or else the rule has little practical effect. (Leskovec, Rajaraman, \u0026amp; Ullman, 2020)\nFurthermore, it must be assumed that there are not too many frequent itemsets and thus not too many candidates for high-support, high-conﬁdence association rules. The reason for this is that if we give companies to many association rules that meet our thresholds for support and conﬁdence, they cannot even read them, let alone act on them. Thus, it is normal to adjust the support and confidence thresholds so that we do not get too many frequent itemsets. (Leskovec, Rajaraman, \u0026amp; Ullman, 2020)\nNext, we take a closer look at the measures support, confidence and lift.\n  2 Association measures 2.1 Support The metric support tells us how popular a set of items is, as measured by the proportion of transactions in which the itemset appears.\nIn our data, the support of {apple} is 4 out of 8, or 50%. The support of {apple, beer, rice} is 2 out of 8, or 25%.\n\\[Support(apple) = \\frac{4}{8} = 0.5\\]\nOr in general, for a set of items X:\n\\[ Support(X) = \\frac{frequency(X)}{n} \\]\n with n = number of all transactions (baskets).  Usually, a specific support-threshold is used to reduce the number of itemsets we need to analyze. At the beginning of the analysis, we could set our support-threshold to 10%.\n 2.2 Confidence Confidence tells us how likely an item Y is purchased given that item X is purchased, expressed as {X -\u0026gt; Y}. It is measured by the proportion of transactions with item X, in which item Y also appears. The confidence of a rule is defined as:\n\\[ Confidence(X -\u0026gt; Y) = \\frac{support(X \\cup Y)}{support(X)} \\]\nHence, the confidence can be interpreted as an estimate of the probability P(Y|X). In other words, this is the probability of finding the RHS (Y) of the rule in transactions under the condition that these transactions also contain the LHS (X) (Hornik, Grün, \u0026amp; Hahsler, 2005). Confidence is directed and gives different values for the rules X -\u0026gt; Y and Y -\u0026gt; X.\nNote that \\(support(X ∪ Y)\\) means the support of the union of the items in X and Y. Since we usually state probabilities of events and not sets of items, we can rewrite \\(support(X \\cup Y)\\) as the probability \\(P(E_X \\cap E_Y)\\), where \\(E_{X}\\) and \\(E_{Y}\\) are the events that a transaction contains itemset X and Y, respectively (review this site from Michael Hahsler for a detailed explanation).\nIn our example, the confidence that beer is purchased given that apple is purchased ({apple -\u0026gt; beer}) is 3 out of 4, or 75%. This means the conditional probability P(beer|apple) = 75%. Apple is the antecedent or left-hand-side (LHS) and beer is the consequent or right-hand-side (RHS).\n\\[Confidence(apple -\u0026gt; beer ) = \\frac{support(apple ∪ beer)}{support(apple)} = \\frac{\\frac{3}{8}{}{}}{\\frac{4}{8}{}} = \\frac{3}{4} = 0.75\\]\nNote that the confidence measure might misrepresent the importance of an association. This is because it only accounts for how popular item Y is (in our case apple) but not X (in our case beer).\nIf beer is also very popular in general, there will be a higher chance that a transaction containing apple will also contain beer, thus inflating the confidence measure. To account for the base popularity of both items, we use a third measure called lift.\n 2.3 Lift Lift tells us how likely item Y is purchased when item X is purchased, while controlling for how popular items Y and X are. It measures how many times more often X and Y occur together than expected if they were statistically independent.\nIn our example, lift is calculated as:\n\\[Lift(apple -\u0026gt; beer ) = \\frac{support(apple ∪ beer)}{support(apple) \\times support(beer)} = \\frac{\\frac{3}{8}{}{}}{\\frac{4}{8}{\\times \\frac{6}{8}}} = \\frac{\\frac{3}{8}{}{}}{\\frac{24}{64}} = \\frac{\\frac{3}{8}{}{}}{\\frac{3}{8}} = 1\\]\nA lift value of:\n lift = 1: implies no association between items.\n lift \u0026gt; 1: greater than 1 means that item Y is likely to be bought if item X is bought,\n lift \u0026lt; 1: less than 1 means that item Y is unlikely to be bought if item X is bought.\n  The lift of {apple -\u0026gt; beer} is 1, which implies no association between the two items.\n  3 A-Priori Algorithm There are different algorithms for finding frequent item-sets. In this tutorial we cover the main idea behind the A-Priori Algorithm, which reduces the number of itemsets we need to examine. It works by eliminating itemsets by looking ﬁrst at smaller sets and recognizing that a large set cannot be frequent unless all its subsets are. Put simply, the algorithm states that if an itemset is infrequent, then all its subsets must also be infrequent.\nThis means that if item {beer} was found to be infrequent, we can expect the itemset {beer, pizza} to be equally or even more infrequent. So in consolidating the list of popular itemsets, we need not consider {beer, pizza}, nor any other itemset configuration that contains {beer}.\nThe A-Priori Algorithm uses a so called breadth-first search strategy, which can be viewed in this decision tree:\n Figure 3.1: Example of breadth-first search (source: Matheny, 2007  Using this principle, the number of itemsets that have to be examined can be pruned (i.e. removing sections of the decision tree).\nThe list of popular itemsets can be obtained in these steps (Ng \u0026amp; Soo, 2017):\n Step 0. Start with itemsets containing just a single item, such as {apple} and {pear}.\n Step 1. Determine the support-threshold for itemsets. Keep the itemsets that meet your minimum support threshold, and remove itemsets that do not.\n Step 2. Using the itemsets you have kept from Step 1, generate all the possible itemset configurations.\n Step 3. Repeat Steps 1 \u0026amp; 2 until there are no more new itemsets.\n   This iterative process is illustrated in the animation below:\n Figure 3.2: A-Priori Algorithm (Ng \u0026amp; Soo, 2017)  As seen in the animation, {apple} was determine to have low support, hence it was removed and all other itemset configurations that contain apple need not be considered. This reduced the number of itemsets to consider by more than half.\nNote that the support threshold that you pick in Step 1 could be based on a formal analysis or past experience. If you discover that sales of items beyond a certain proportion tend to have a significant impact on your profits, you might consider using that proportion as your support threshold (otherwise you may use 1% as a starting value).\nWe have seen how the A-Priori Algorithm can be used to identify itemsets with high support. The same principle can also be used to identify item associations with high confidence or lift. Finding rules with high confidence or lift is less computationally taxing once high-support itemsets have been identified, because confidence and lift values are calculated using support values (Ng \u0026amp; Soo, 2017).\nTake for example the task of finding high-confidence rules. If the rule\n{beer, chips -\u0026gt; apple}\nhas low confidence, all other rules with the same left hand side (LHS) items and with apple on the right hand side (RHS) would have low confidence too. Specifically, the rules\n{beer -\u0026gt; apple, chips} {chips -\u0026gt; apple, beer}\nwould have low confidence as well. As before, lower level candidate item rules can be pruned using the A-Priori Algorithm, so that fewer candidate rules need to be examined (Ng \u0026amp; Soo, 2017).\nIn summary, when you apply the A-Priori Algorithm on a given set of transactions, your goal will be to find all rules with support confidence greater than or equal to your support threshold and confidence greater than or equal to your confidence threshold.\n 4 Implementation in R install.packages(\u0026quot;arules\u0026quot;) install.packages(\u0026quot;arulesViz\u0026quot;) To perform the association analysis in R, we use the arules and arulesViz packages. Review Hornik et al. (2005) for a detailed description of the packages or visit the arules documentation site.\n4.1 Transform data First of all, you have to load the transaction data into an object of the “transaction class” to be able to analyze the data. This is done by using the following function of the arules package:\nlibrary(arules) trans \u0026lt;- as(market_basket, \u0026quot;transactions\u0026quot;)  4.2 Inspect data Take a look at the dimensions of this object:\ndim(trans) ## [1] 8 6 This means we have 8 transactions and 6 distinct items.\nObtain a list of the distinct items in the data:\nitemLabels(trans) ## [1] \u0026quot;apple\u0026quot; \u0026quot;beer\u0026quot; \u0026quot;meat\u0026quot; \u0026quot;milk\u0026quot; \u0026quot;pear\u0026quot; \u0026quot;rice\u0026quot; View the summary of the transaction data:\nsummary(trans) ## transactions as itemMatrix in sparse format with ## 8 rows (elements/itemsets/transactions) and ## 6 columns (items) and a density of 0.4583333 ## ## most frequent items: ## beer apple milk rice meat (Other) ## 6 4 4 4 2 2 ## ## element (itemset/transaction) length distribution: ## sizes ## 2 3 4 ## 4 2 2 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 2.00 2.50 2.75 3.25 4.00 ## ## includes extended item information - examples: ## labels ## 1 apple ## 2 beer ## 3 meat ## ## includes extended transaction information - examples: ## transactionID ## 1 T1 ## 2 T2 ## 3 T3 The summary() gives us information about our transaction object:\n There are 8 transactions (rows) and 6 items (columns) and we can view the most frequent items.\n Density tells us the percentage of non-zero cells in this 8x6-matrix.\n Element length distribution: a set of 2 items in 4 transactions; 3 items in 2 of the transactions and 4 items in 2 transactions.\n  Note that a matrix is called a sparse matrix if most of the elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. The number of zero-valued elements divided by the total number of elements is called the sparsity of the matrix (which is equal to 1 minus the density of the matrix).\nTake a look at all transactions and items in a matrix like fashion:\nimage(trans) You can observe that almost half of the “cells” (45,83 %) are non zero values.\nDisplay the relative item frequency:\nitemFrequencyPlot(trans, topN=10, cex.names=1)  Figure 4.1: Relative item frequency  The items {apple}, {milk} and {rice} all have a relative item frequency (i.e. support) of 50%.\n 4.3 A-Priori Algorithm The next step is to analyze the rules using the A-Priori Algorithm with the function apriori(). This function requires both a minimum support and a minimum confidence constraint at the same time. The option parameter will allow you to set the support-threshold, confidence-threshold as well as the maximum lenght of items (maxlen). If you do not provide threshold values, the function will perform the analysis with these default values: support-threshold of 0.1 and confidence-threshold of 0.8.\n#Min Support 0.3, confidence as 0.5. rules \u0026lt;- apriori(trans, parameter = list(supp=0.3, conf=0.5, maxlen=10, target= \u0026quot;rules\u0026quot;)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.5 0.1 1 none FALSE TRUE 5 0.3 1 ## maxlen target ext ## 10 rules FALSE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 2 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[6 item(s), 8 transaction(s)] done [0.00s]. ## sorting and recoding items ... [4 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 done [0.00s]. ## writing ... [10 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. In our simple example, we already know that by using a support-threshold of 0.3, we will eliminate {meat} and {pear} from our analysis, since they have support values below 0.3.\nThe summary shows the following:\nsummary(rules) ## set of 10 rules ## ## rule length distribution (lhs + rhs):sizes ## 1 2 ## 4 6 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 1.0 2.0 1.6 2.0 2.0 ## ## summary of quality measures: ## support confidence lift count ## Min. :0.375 Min. :0.5000 Min. :1.000 Min. :3.0 ## 1st Qu.:0.375 1st Qu.:0.5000 1st Qu.:1.000 1st Qu.:3.0 ## Median :0.500 Median :0.5833 Median :1.000 Median :4.0 ## Mean :0.475 Mean :0.6417 Mean :1.067 Mean :3.8 ## 3rd Qu.:0.500 3rd Qu.:0.7500 3rd Qu.:1.000 3rd Qu.:4.0 ## Max. :0.750 Max. :1.0000 Max. :1.333 Max. :6.0 ## ## mining info: ## data ntransactions support confidence ## trans 8 0.3 0.5  Set of rules: 10. Rule length distribution (LHS + RHS): 4 rules with a length of 1 item; 6 rules with a length of 2 items. Summary of quality measures: min, max, median, mean and quantile values for support, confidence and lift. Mining info: number of transactions, support-threshold and confidence-threshold.  Inspect the 10 rules we obtained:\ninspect(rules) ## lhs rhs support confidence lift count ## [1] {} =\u0026gt; {apple} 0.500 0.5000000 1.000000 4 ## [2] {} =\u0026gt; {milk} 0.500 0.5000000 1.000000 4 ## [3] {} =\u0026gt; {rice} 0.500 0.5000000 1.000000 4 ## [4] {} =\u0026gt; {beer} 0.750 0.7500000 1.000000 6 ## [5] {apple} =\u0026gt; {beer} 0.375 0.7500000 1.000000 3 ## [6] {beer} =\u0026gt; {apple} 0.375 0.5000000 1.000000 3 ## [7] {milk} =\u0026gt; {beer} 0.375 0.7500000 1.000000 3 ## [8] {beer} =\u0026gt; {milk} 0.375 0.5000000 1.000000 3 ## [9] {rice} =\u0026gt; {beer} 0.500 1.0000000 1.333333 4 ## [10] {beer} =\u0026gt; {rice} 0.500 0.6666667 1.333333 4 The rules 1 to 4 with an empty LHS mean that no matter what other items are involved the item in the RHS will appear with the probability given by the rule’s confidence (which equals the support). If you want to avoid these rules then use the argument parameter=list(minlen=2) (stackoverflow).\n#Min Support 0.3, confidence as 0.5. rules \u0026lt;- apriori(trans, parameter = list(supp=0.3, conf=0.5, maxlen=10, minlen=2, target= \u0026quot;rules\u0026quot;)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.5 0.1 1 none FALSE TRUE 5 0.3 2 ## maxlen target ext ## 10 rules FALSE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 2 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[6 item(s), 8 transaction(s)] done [0.00s]. ## sorting and recoding items ... [4 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 done [0.00s]. ## writing ... [6 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. inspect(rules) ## lhs rhs support confidence lift count ## [1] {apple} =\u0026gt; {beer} 0.375 0.7500000 1.000000 3 ## [2] {beer} =\u0026gt; {apple} 0.375 0.5000000 1.000000 3 ## [3] {milk} =\u0026gt; {beer} 0.375 0.7500000 1.000000 3 ## [4] {beer} =\u0026gt; {milk} 0.375 0.5000000 1.000000 3 ## [5] {rice} =\u0026gt; {beer} 0.500 1.0000000 1.333333 4 ## [6] {beer} =\u0026gt; {rice} 0.500 0.6666667 1.333333 4 We can observe that rule 6 states that {beer -\u0026gt; rice} has a support of 50% and a confidence of 67%. This means this rule was found in 50% of all transactions. The confidence that rice (LHS) is purchased given beer (RHS) is purchased (P(rice|beer)) is 67%. In other words, 67% of the times a customer buys beer, rice is bought as well.\n 4.4 Set LHS and RHS If you want to analyze a specific rule, you can use the option appearance to set a LHS (if part) or RHS (then part) of the rule.\nFor example, to analyze what items customers buy before buying {beer}, we set rhs=beerand default=lhs:\nbeer_rules_rhs \u0026lt;- apriori(trans, parameter = list(supp=0.3, conf=0.5, maxlen=10, minlen=2), appearance = list(default=\u0026quot;lhs\u0026quot;, rhs=\u0026quot;beer\u0026quot;)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.5 0.1 1 none FALSE TRUE 5 0.3 2 ## maxlen target ext ## 10 rules FALSE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 2 ## ## set item appearances ...[1 item(s)] done [0.00s]. ## set transactions ...[6 item(s), 8 transaction(s)] done [0.00s]. ## sorting and recoding items ... [4 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 done [0.00s]. ## writing ... [3 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. Inspect the result:\ninspect(beer_rules_rhs) ## lhs rhs support confidence lift count ## [1] {apple} =\u0026gt; {beer} 0.375 0.75 1.000000 3 ## [2] {milk} =\u0026gt; {beer} 0.375 0.75 1.000000 3 ## [3] {rice} =\u0026gt; {beer} 0.500 1.00 1.333333 4 It is also possible to analyze what items customers buy after buying {beer}:\nbeer_rules_lhs \u0026lt;- apriori(trans, parameter = list(supp=0.3, conf=0.5, maxlen=10, minlen=2), appearance = list(lhs=\u0026quot;beer\u0026quot;, default=\u0026quot;rhs\u0026quot;)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.5 0.1 1 none FALSE TRUE 5 0.3 2 ## maxlen target ext ## 10 rules FALSE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 2 ## ## set item appearances ...[1 item(s)] done [0.00s]. ## set transactions ...[6 item(s), 8 transaction(s)] done [0.00s]. ## sorting and recoding items ... [4 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 done [0.00s]. ## writing ... [3 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. Inspect the result:\ninspect(beer_rules_lhs) ## lhs rhs support confidence lift count ## [1] {beer} =\u0026gt; {apple} 0.375 0.5000000 1.000000 3 ## [2] {beer} =\u0026gt; {milk} 0.375 0.5000000 1.000000 3 ## [3] {beer} =\u0026gt; {rice} 0.500 0.6666667 1.333333 4  4.5 Visualizing association rules Mining association rules often results in a very large number of found rules, leaving the analyst with the task to go through all the rules and discover interesting ones. Sifting manually through large sets of rules is time consuming and strenuous. Therefore, in addition to our calculations of associations, we can use the package arulesViz to visualize our results as:\n Scatter-plots, interactive scatter-plots and Individual rule representations.  For a detailed discussion of the different visualization techniques, review Hahsler \u0026amp; Karpienko (2017).\n 4.6 Scatter-Plot A scatter plot for association rules uses two interest measures, one on each of the axes. The default plot for association rules in arulesViz is a scatter plot using support and confidence on the axes. The measure defined by shading (default: lift) is visualized by the color of the points. A color key is provided to the right of the plot.\nTo visualize our association rules in a scatter plot, we use the function plot() of the arulesViz package. You can use the function as follows: plot(x, method, measure, shading, control, data, engine). For a detailed description, review the vignette of the package:\n x: an object of class “rules” or “itemsets”. method: a string with value “scatterplot”, “two-key plot”, “matrix”, “matrix3D”, “mo-saic”, “doubledecker”, “graph”, “paracoord” or “grouped”, “iplots” selecting the visualization method. measure: measure(s) of interestingness (e.g., “support”, “confidence”, “lift”, “order”) used in the visualization. shading: measure of interestingness used for the color of the points/arrows/nodes (e.g., “support”, “confidence”, “lift”). The default is “lift”. control: a list of control parameters for the plot. The available control parameters depend on the used visualization method. data: the dataset (class “transactions”) used to generate the rules/itemsets. Only “mo-saic” and “doubledecker” require the original data. engine: a string indicating the plotting engine used to render the plot. The “default” en- gine uses (mostly) grid, but some plots can produce interactive interactive grid visualizations using engine “interactive”, or HTML widgets using engine “html- widget”.  For a basic plot with default settings, just insert the object x (in our case rules). This visualization method draws a two dimensional scatter plot with different measures of interestingness (parameter “measure”) on the axes and a third measure (parameter “shading”) is represented by the color of the points.\nlibrary(arulesViz) plot(rules)  Figure 4.2: Scatter plot  The plot shows support on the x-axis and confidence on the y-axis. Lift ist shown as a color with different levels ranging from grey to red.\nWe could also use only “confidence” as a specific measure of interest:\nplot(rules, measure = \u0026quot;confidence\u0026quot;)  Figure 4.3: Scatter plot with confidence as measure of interest  There is a special value for shading called “order” which produces a two-key plot where the color of the points represents the length (order) of the rule if you select method = \"two-key plot. This is basically a scatterplot with shading = \"order\":\nplot(rules, method = \u0026quot;two-key plot\u0026quot;)  Figure 4.4: Two-key plot   4.7 Interactive scatter-plot Plot an interactive scatter plot for association rules using plotly:\nplot(rules, engine = \u0026quot;plotly\u0026quot;)   {\"x\":{\"visdat\":{\"2c2be2cdf0d\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"2c2be2cdf0d\",\"attrs\":{\"2c2be2cdf0d\":{\"x\":[0.373346867557848,0.377369146996643,0.375952479192056,0.372515940181911,0.500604235514766,0.501004825507989],\"y\":[0.750615238446146,0.500003982396383,0.749567694180398,0.500463832698907,0.99942207884702,0.665848194714662],\"hoverinfo\":\"text\",\"text\":[\"[1] {apple}\u0026nbsp;\u0026nbsp; = {beer} support: 0.375 confidence: 0.75 lift: 1\",\"[2] {beer}\u0026nbsp;\u0026nbsp; = {apple} support: 0.375 confidence: 0.5 lift: 1\",\"[3] {milk}\u0026nbsp;\u0026nbsp; = {beer} support: 0.375 confidence: 0.75 lift: 1\",\"[4] {beer}\u0026nbsp;\u0026nbsp; = {milk} support: 0.375 confidence: 0.5 lift: 1\",\"[5] {rice}\u0026nbsp;\u0026nbsp; = {beer} support: 0.5 confidence: 1 lift: 1.33\",\"[6] {beer}\u0026nbsp;\u0026nbsp; = {rice} support: 0.5 confidence: 0.667 lift: 1.33\"],\"mode\":\"markers\",\"marker\":[],\"color\":[1,1,1,1,1.33333333333333,1.33333333333333],\"colors\":[\"#EEEEEEFF\",\"#EE0000FF\"],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"support\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"confidence\"},\"hovermode\":\"closest\",\"showlegend\":false,\"legend\":{\"yanchor\":\"top\",\"y\":0.5}},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[0.373346867557848,0.377369146996643,0.375952479192056,0.372515940181911,0.500604235514766,0.501004825507989],\"y\":[0.750615238446146,0.500003982396383,0.749567694180398,0.500463832698907,0.99942207884702,0.665848194714662],\"hoverinfo\":[\"text\",\"text\",\"text\",\"text\",\"text\",\"text\"],\"text\":[\"[1] {apple}\u0026nbsp;\u0026nbsp; = {beer} support: 0.375 confidence: 0.75 lift: 1\",\"[2] {beer}\u0026nbsp;\u0026nbsp; = {apple} support: 0.375 confidence: 0.5 lift: 1\",\"[3] {milk}\u0026nbsp;\u0026nbsp; = {beer} support: 0.375 confidence: 0.75 lift: 1\",\"[4] {beer}\u0026nbsp;\u0026nbsp; = {milk} support: 0.375 confidence: 0.5 lift: 1\",\"[5] {rice}\u0026nbsp;\u0026nbsp; = {beer} support: 0.5 confidence: 1 lift: 1.33\",\"[6] {beer}\u0026nbsp;\u0026nbsp; = {rice} support: 0.5 confidence: 0.667 lift: 1.33\"],\"mode\":\"markers\",\"type\":\"scatter\",\"marker\":{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"cmin\":1,\"cmax\":1.33333333333333,\"colorscale\":[[\"0\",\"rgba(238,238,238,1)\"],[\"0.0416666666666665\",\"rgba(241,231,228,1)\"],[\"0.0833333333333331\",\"rgba(244,223,218,1)\"],[\"0.125\",\"rgba(247,216,208,1)\"],[\"0.166666666666667\",\"rgba(249,208,198,1)\"],[\"0.208333333333333\",\"rgba(251,201,188,1)\"],[\"0.25\",\"rgba(252,193,178,1)\"],[\"0.291666666666667\",\"rgba(253,186,168,1)\"],[\"0.333333333333334\",\"rgba(254,178,158,1)\"],[\"0.375\",\"rgba(255,170,149,1)\"],[\"0.416666666666667\",\"rgba(255,163,139,1)\"],[\"0.458333333333333\",\"rgba(255,155,130,1)\"],[\"0.5\",\"rgba(255,147,120,1)\"],[\"0.541666666666667\",\"rgba(255,139,111,1)\"],[\"0.583333333333333\",\"rgba(254,131,102,1)\"],[\"0.625\",\"rgba(253,123,92,1)\"],[\"0.666666666666666\",\"rgba(252,114,83,1)\"],[\"0.708333333333334\",\"rgba(251,105,74,1)\"],[\"0.75\",\"rgba(250,96,65,1)\"],[\"0.791666666666667\",\"rgba(248,86,56,1)\"],[\"0.833333333333333\",\"rgba(246,76,46,1)\"],[\"0.875\",\"rgba(245,65,37,1)\"],[\"0.916666666666667\",\"rgba(242,51,26,1)\"],[\"0.958333333333333\",\"rgba(240,34,14,1)\"],[\"1\",\"rgba(238,0,0,1)\"]],\"showscale\":false,\"color\":[1,1,1,1,1.33333333333333,1.33333333333333],\"line\":{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"cmin\":1,\"cmax\":1.33333333333333,\"colorscale\":[[\"0\",\"rgba(238,238,238,1)\"],[\"0.0416666666666665\",\"rgba(241,231,228,1)\"],[\"0.0833333333333331\",\"rgba(244,223,218,1)\"],[\"0.125\",\"rgba(247,216,208,1)\"],[\"0.166666666666667\",\"rgba(249,208,198,1)\"],[\"0.208333333333333\",\"rgba(251,201,188,1)\"],[\"0.25\",\"rgba(252,193,178,1)\"],[\"0.291666666666667\",\"rgba(253,186,168,1)\"],[\"0.333333333333334\",\"rgba(254,178,158,1)\"],[\"0.375\",\"rgba(255,170,149,1)\"],[\"0.416666666666667\",\"rgba(255,163,139,1)\"],[\"0.458333333333333\",\"rgba(255,155,130,1)\"],[\"0.5\",\"rgba(255,147,120,1)\"],[\"0.541666666666667\",\"rgba(255,139,111,1)\"],[\"0.583333333333333\",\"rgba(254,131,102,1)\"],[\"0.625\",\"rgba(253,123,92,1)\"],[\"0.666666666666666\",\"rgba(252,114,83,1)\"],[\"0.708333333333334\",\"rgba(251,105,74,1)\"],[\"0.75\",\"rgba(250,96,65,1)\"],[\"0.791666666666667\",\"rgba(248,86,56,1)\"],[\"0.833333333333333\",\"rgba(246,76,46,1)\"],[\"0.875\",\"rgba(245,65,37,1)\"],[\"0.916666666666667\",\"rgba(242,51,26,1)\"],[\"0.958333333333333\",\"rgba(240,34,14,1)\"],[\"1\",\"rgba(238,0,0,1)\"]],\"showscale\":false,\"color\":[1,1,1,1,1.33333333333333,1.33333333333333]}},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[0.372515940181911,0.501004825507989],\"y\":[0.500003982396383,0.99942207884702],\"type\":\"scatter\",\"mode\":\"markers\",\"opacity\":0,\"hoverinfo\":\"none\",\"showlegend\":false,\"marker\":{\"colorbar\":{\"title\":\"lift\",\"ticklen\":2,\"len\":0.5,\"lenmode\":\"fraction\",\"y\":1,\"yanchor\":\"top\"},\"cmin\":1,\"cmax\":1.33333333333333,\"colorscale\":[[\"0\",\"rgba(238,238,238,1)\"],[\"0.0416666666666665\",\"rgba(241,231,228,1)\"],[\"0.0833333333333331\",\"rgba(244,223,218,1)\"],[\"0.125\",\"rgba(247,216,208,1)\"],[\"0.166666666666667\",\"rgba(249,208,198,1)\"],[\"0.208333333333333\",\"rgba(251,201,188,1)\"],[\"0.25\",\"rgba(252,193,178,1)\"],[\"0.291666666666667\",\"rgba(253,186,168,1)\"],[\"0.333333333333334\",\"rgba(254,178,158,1)\"],[\"0.375\",\"rgba(255,170,149,1)\"],[\"0.416666666666667\",\"rgba(255,163,139,1)\"],[\"0.458333333333333\",\"rgba(255,155,130,1)\"],[\"0.5\",\"rgba(255,147,120,1)\"],[\"0.541666666666667\",\"rgba(255,139,111,1)\"],[\"0.583333333333333\",\"rgba(254,131,102,1)\"],[\"0.625\",\"rgba(253,123,92,1)\"],[\"0.666666666666666\",\"rgba(252,114,83,1)\"],[\"0.708333333333334\",\"rgba(251,105,74,1)\"],[\"0.75\",\"rgba(250,96,65,1)\"],[\"0.791666666666667\",\"rgba(248,86,56,1)\"],[\"0.833333333333333\",\"rgba(246,76,46,1)\"],[\"0.875\",\"rgba(245,65,37,1)\"],[\"0.916666666666667\",\"rgba(242,51,26,1)\"],[\"0.958333333333333\",\"rgba(240,34,14,1)\"],[\"1\",\"rgba(238,0,0,1)\"]],\"showscale\":true,\"color\":[1,1.33333333333333],\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} Figure 4.5: Interactive scatter-plot   4.8 Graph-based visualization Graph-based techniques concentrate on the relationship between individual items in the rule set. They represent the rules (or itemsets) as a graph with items as labeled vertices, and rules (or itemsets) represented as vertices connected to items using arrows.\nFor rules, the LHS items are connected with arrows pointing to the vertex representing the rule and the RHS has an arrow pointing to the item.\nSeveral engines are available. The default engine uses igraph (plot.igraph and tkplot for the interactive visualization). … arguments are passed on to the respective plotting function (use for color, etc.).\nThe network graph below shows associations between selected items. Larger circles imply higher support, while red circles imply higher lift. Graphs only work well with very few rules, why we only use a subset of 10 rules from our data:\nsubrules \u0026lt;- head(rules, n = 10, by = \u0026quot;confidence\u0026quot;) plot(subrules, method = \u0026quot;graph\u0026quot;, engine = \u0026quot;htmlwidget\u0026quot;)   {\"x\":{\"nodes\":{\"id\":[1,2,3,4,5,6,7,8,9,10],\"label\":[\"rice\",\"apple\",\"milk\",\"beer\",\"rule 1\",\"rule 2\",\"rule 3\",\"rule 4\",\"rule 5\",\"rule 6\"],\"group\":[\"item\",\"item\",\"item\",\"item\",\"rule\",\"rule\",\"rule\",\"rule\",\"rule\",\"rule\"],\"value\":[1,1,1,1,100,1,1,100,1,1],\"color\":[\"#CBD2FC\",\"#CBD2FC\",\"#CBD2FC\",\"#CBD2FC\",\"#EE1B1B\",\"#EEDCDC\",\"#EEDCDC\",\"#EE1B1B\",\"#EEDCDC\",\"#EEDCDC\"],\"title\":[\"rice\",\"apple\",\"milk\",\"beer\",\"[1]{rice}\u0026nbsp;\u0026nbsp; = {beer}support = 0.5confidence = 1lift = 1.33count = 4order = 2\",\"[2]{apple}\u0026nbsp;\u0026nbsp; = {beer}support = 0.375confidence = 0.75lift = 1count = 3order = 2\",\"[3]{milk}\u0026nbsp;\u0026nbsp; = {beer}support = 0.375confidence = 0.75lift = 1count = 3order = 2\",\"[4]{beer}\u0026nbsp;\u0026nbsp; = {rice}support = 0.5confidence = 0.667lift = 1.33count = 4order = 2\",\"[5]{beer}\u0026nbsp;\u0026nbsp; = {apple}support = 0.375confidence = 0.5lift = 1count = 3order = 2\",\"[6]{beer}\u0026nbsp;\u0026nbsp; = {milk}support = 0.375confidence = 0.5lift = 1count = 3order = 2\"],\"shape\":[\"box\",\"box\",\"box\",\"box\",\"circle\",\"circle\",\"circle\",\"circle\",\"circle\",\"circle\"],\"x\":[1,0.622025168237994,-1,0.209752925412518,0.841722780596659,0.660018052911227,-0.536587755114388,0.500202494577971,0.239327800764795,-0.460380004602411],\"y\":[1,-1,0.32187958176185,0.104628396727376,0.490537591371417,-0.469423503736561,0.0236247927126421,0.764327554234668,-0.611686548528727,0.439884095089312]},\"edges\":{\"from\":[1,2,3,4,4,4,5,6,7,8,9,10],\"to\":[5,6,7,8,9,10,4,4,4,1,2,3],\"arrows\":[\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\",\"to\"]},\"nodesToDataframe\":true,\"edgesToDataframe\":true,\"options\":{\"width\":\"100%\",\"height\":\"100%\",\"nodes\":{\"shape\":\"dot\",\"scaling\":{\"label\":{\"enabled\":true}},\"physics\":false},\"manipulation\":{\"enabled\":false},\"edges\":{\"smooth\":false},\"physics\":{\"stabilization\":false},\"interaction\":{\"hover\":true}},\"groups\":[\"item\",\"rule\"],\"width\":null,\"height\":null,\"idselection\":{\"enabled\":true,\"style\":\"width: 150px; height: 26px\",\"useLabels\":true,\"main\":\"Select by id\"},\"byselection\":{\"enabled\":false,\"style\":\"width: 150px; height: 26px\",\"multiple\":false,\"hideColor\":\"rgba(200,200,200,0.5)\",\"highlight\":false},\"main\":null,\"submain\":null,\"footer\":null,\"background\":\"rgba(0, 0, 0, 0)\",\"igraphlayout\":{\"type\":\"square\"},\"tooltipStay\":300,\"tooltipStyle\":\"position: fixed;visibility:hidden;padding: 5px;white-space: nowrap;font-family: verdana;font-size:14px;font-color:#000000;background-color: #f5f4ed;-moz-border-radius: 3px;-webkit-border-radius: 3px;border-radius: 3px;border: 1px solid #808074;box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);\",\"highlight\":{\"enabled\":true,\"hoverNearest\":true,\"degree\":1,\"algorithm\":\"all\",\"hideColor\":\"rgba(200,200,200,0.5)\",\"labelOnly\":true},\"collapse\":{\"enabled\":false,\"fit\":false,\"resetHighlight\":true,\"clusterOptions\":null,\"keepCoord\":true,\"labelSuffix\":\"(cluster)\"}},\"evals\":[],\"jsHooks\":[]} Figure 4.6: Graph-based visualization   4.9 Parallel coordinate plot Represents the rules (or itemsets) as a parallel coordinate plot (from LHS to RHS).\nplot(subrules, method=\u0026quot;paracoord\u0026quot;)  Figure 4.7: Parallel coordinate plot  The plot indicates that if a customer buys rice, he is likely to buy beer as well: {rice -\u0026gt; beer}. The same is true for the opposite direction: {beer -\u0026gt; rice}.\n  5 References Hahsler, M., \u0026amp; Karpienko, R. (2017). Visualizing association rules in hierarchical groups. Journal of Business Economics, 87(3), 317–335. https://doi.org/10.1007/s11573-016-0822-8\nHornik, K., Grün, B., \u0026amp; Hahsler, M. (2005). arules - A computational environment for mining association rules and frequent item sets. Journal of Statistical Software, 14(15), 1–25.\nLeskovec, J., Rajaraman, A., \u0026amp; Ullman, J. D. (2020). Mining of massive data sets. Cambridge university press.\nNg, A., \u0026amp; Soo, K. (2017). Numsense! Data Science for the Layman: No Math Added. Leanpub.\n ","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589414400,"objectID":"a9a78500bdde59e897fb58c92665ea31","permalink":"/post/2020-05-14-r-association-rule-mining/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/post/2020-05-14-r-association-rule-mining/","section":"post","summary":"Association Rule Mining in R","tags":["Unsupervised Learning","R"],"title":"Introduction to Association Rule Mining in R","type":"post"},{"authors":null,"categories":null,"content":"  HTML-presentation “First Steps in Markdown” (created in Markdown with the R Xaringan package).\n R Markdown Report in HTML\n  Markdown is one of the world’s most popular markup languages used in data science. Both R Markdown and Jupyter Notebooks use Markdown to provide an unified authoring framework for data science, combining code (Python, R, SQL,…), its results, and commentary in Markdown. The documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more.\nAccording to Wickham \u0026amp; Grolemund (2016), Markdown files are designed to be used in three ways:\n div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}  For communicating to decision makers, who want to focus on the conclusions, not the code behind the analysis.\n For collaborating with other data scientists, who are interested in both your conclusions, and how you reached them (i.e. the code).\n As an environment in which to do data science, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking.\n   \nLearn the most important basics of Markdown in this excellent interactive “60 Seconds Markdown Tutorial”.\nWickham, H., \u0026amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc.\n","date":1574071200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574071200,"objectID":"e2cc47f2de919961138e99b6e168e5aa","permalink":"/project/markdown-first-steps/","publishdate":"2019-11-18T10:00:00Z","relpermalink":"/project/markdown-first-steps/","section":"project","summary":"Introduction to R Markdown - Communicate Data Science Results in a Modern Way","tags":["Markdown","R Markdown"],"title":"Introduction to R Markdown","type":"project"},{"authors":["Jan Kirenz"],"categories":["Seminar"],"content":"Anhand von mehreren Fallstudien wird zunächst die Extraktion, Bearbeitung und Analyse von Daten in unterschiedlichen Datenbanken mit Hilfe von SQL eingehend behandelt. Der dabei gelernte SQL-Syntax – bspw.\n Datenimport, Verknüpfung von Tabellen, Gruppierung und Summierung, Berechnung statistischer Kennzahlen, - Datenexploration, Analyse von Zeitdaten, Textanalysen und bedingte Ausdrücke (Conditional Expressions)  kann auf eine Vielzahl von Datenbanken wie bspw.\n PostgreSQL, MySQL, Microsoft Azure SQL Datenbank, Google BigQuery und Oracle angewendet werden.  Im Rahmen der Datenanalyse mit R werden neben den zentralen Grundkenntnissen der Programmiersprache R insbesondere Kompetenzen im Umgang mit Datentransformationen („Data Wrangling“), der explorativen Datenanalyse und Visualisierung von Daten (bspw. mit Hilfe eines Dashboards) vermittelt. Zudem wird die Erstellung und Ausgabe (bspw. als HTML, PDF, Word, Excel, PPT,…) von Reports in R markdown behandelt.\n","date":1571907600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571907600,"objectID":"657b65f4bda3414ca3c7b8b97dae5480","permalink":"/talk/2019-programming-languages/","publishdate":"2019-08-17T00:00:00Z","relpermalink":"/talk/2019-programming-languages/","section":"talk","summary":"Introduction to SQL and R","tags":["SQL","R"],"title":"Programming Languages for Data Science","type":"talk"},{"authors":["Jan Kirenz"],"categories":["lecture"],"content":"In der Veranstaltung Web Analytics befassen wir uns mit der Erfassung, Analyse und Visualisierung von strukturierten und unstrukturierten Daten aus dem Internet - bspw. Daten aus sozialen Medien, Multi-Media-Plattformen, Microblogs, Foren und anderen Webpräsenzen.\nVon besonderem Interesse sind in diesem Zusammenhang \u0026ldquo;offene Daten\u0026rdquo; (Open Data), die im Internet zur freien Weiterverwendung frei zugänglich gemacht werden (insbesondere von Organisationen wie der OECD, Weltbank und von staatlichen Stellen).\nWir werden insbesondere mit der Programmiersprache R arbeiten und die folgenden Inhalte behandeln:\n Web-scraping Data Science Web-API\u0026rsquo;s Natural Language Processing bzw. Text Mining Programmierung von Dashboards  ","date":1571040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571040000,"objectID":"96c98820f8ebaba7884230fae6e9c23b","permalink":"/talk/2019-web-analytics/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/talk/2019-web-analytics/","section":"talk","summary":"Webscraping, API's, Network Analysis, NLP, Dashboards","tags":["DataScience"],"title":"Web Analytics","type":"talk"},{"authors":["Jan Kirenz"],"categories":["lecture"],"content":"\u0026lsquo;In der Veranstaltung Web Analytics \u0026amp; Big Data befassen wir uns mit der Erfassung und Analyse von strukturierten und unstrukturierten (großen) Daten - bspw. Daten aus sozialen Medien, Multi-Media-Plattformen, Microblogs, Foren, Webpräsenzen. Die dadurch gewonnenen Erkenntnisse sollen einen Beitrag zu dem systematischen Aufbau und der Pflege dauerhafter und profitabler Kundenbeziehungen leisten und beziehen sich insbesondere auf die Phasen der Kundengewinnung, Kundenbindung und Vermeidung der Kundenabwanderung innerhalb des Customer Lifecycle Managements.\u0026rsquo;\n","date":1570784400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570784400,"objectID":"a1a67e2a8f607672e1b8511789c96248","permalink":"/talk/2019-big-data-web-analytics/","publishdate":"2019-10-11T09:00:00Z","relpermalink":"/talk/2019-big-data-web-analytics/","section":"talk","summary":"Analyse von Big Data mit Python \u0026 R","tags":["DataScience"],"title":"Big Data and Web Analytics","type":"talk"},{"authors":["Jan Kirenz"],"categories":["R","TextMining"],"content":"   1 Introduction to Textmining in R 1.1 Installation of R packages 1.2 Data import  2 Data transformation 2.1 Tokenization 2.2 Stop words  3 Exploratory data analysis 3.1 Term frequency (tf) 3.2 Term frequency and inverse document frequency (tf-idf) 3.3 Tokenizing by n-gram 3.4 Network analysis  4 Classification with logistic regression 4.1 Train test split 4.2 Training data (sparse matrix) 4.3 Response variable 4.4 Logistic regression model 4.5 Model evaluation with test data    1 Introduction to Textmining in R This post demonstrates how various R packages can be used for text mining in R. In particular, we start with common text transformations, perform various data explorations with term frequency (tf) and inverse document frequency (idf) and build a supervised classifiaction model that learns the difference between texts of different authors.\nThe content of this tutorial is based on the excellent book “Textmining with R (2019)” from Julia Silge and David Robinson and the blog post “Text classification with tidy data principles (2018)” from Julia Silges.\n1.1 Installation of R packages If you like to install all packages at once, use the code below.\ninstall.packages(c(\u0026quot;dplyr\u0026quot;, \u0026quot;gutenbergr\u0026quot;, \u0026quot;stringr\u0026quot;, \u0026quot;tidytext\u0026quot;, \u0026quot;tidyr\u0026quot;, \u0026quot;stopwords\u0026quot;, \u0026quot;wordcloud\u0026quot;, \u0026quot;rsample\u0026quot;, \u0026quot;glmnet\u0026quot;, \u0026quot;doMC\u0026quot;, \u0026quot;forcats\u0026quot;, \u0026quot;broom\u0026quot;, \u0026quot;igraph\u0026quot;, \u0026quot;ggraph\u0026quot;))   1.2 Data import We can access the full texts of various books from “Project Gutenberg” via the gutenbergr package. We can look up certain authors or titles with a regular expression using the stringr package. All functions in stringr start with str_and take a vector of strings as the first argument. To learn more about stringr, visit the stringr documentation.\nlibrary(gutenbergr) library(stringr) doyle \u0026lt;- gutenberg_works(str_detect(author, \u0026quot;Doyle\u0026quot;))   gutenberg_id  title  author  gutenberg_author_id  language  gutenberg_bookshelf  rights  has_text      108  The Return of Sherlock Holmes  Doyle, Arthur Conan  69  en  Detective Fiction  Public domain in the USA.  TRUE    126  The Poison Belt  Doyle, Arthur Conan  69  en  Science Fiction  Public domain in the USA.  TRUE    139  The Lost World  Doyle, Arthur Conan  69  en  Science Fiction  Public domain in the USA.  TRUE    244  A Study in Scarlet  Doyle, Arthur Conan  69  en  Detective Fiction  Public domain in the USA.  TRUE     We obtain “Relativity: The Special and General Theory” by Albert Einstein (gutenberg_id: 30155) and “Experiments with Alternate Currents of High Potential and High Frequency” by Nikola Tesla (gutenberg_id: 13476) from gutenberg and add the column “author” to the result.\nlibrary(gutenbergr) books \u0026lt;- gutenberg_download(c(30155, 13476), meta_fields = \u0026quot;author\u0026quot;) Furthermore, we transfrom the data to a tibble (tibbles are a modern take on data frames), add the row number with the column name document to the tibble and drop the column gutenberg_id. We will use the information in column document to train a model that can take an individual line (row) and give us a probability that the text in this particular line comes from a certain author.\nlibrary(dplyr) books \u0026lt;- as_tibble(books) %\u0026gt;% mutate(document = row_number()) %\u0026gt;% select(-gutenberg_id)   text  author  document      EXPERIMENTS WITH ALTERNATE CURRENTS OF HIGH POTENTIAL AND HIGH FREQUENCY  Tesla, Nikola  1     Tesla, Nikola  2    A Lecture Delivered before the Institution of Electrical Engineers, London  Tesla, Nikola  3     Tesla, Nikola  4    by  Tesla, Nikola  5     Tesla, Nikola  6    NIKOLA TESLA  Tesla, Nikola  7     Tesla, Nikola  8       2 Data transformation 2.1 Tokenization First of all, we need to both break the text into individual tokens (a process called tokenization) and transform it to a tidy data structure (i.e. each variable must have its own column, each observation must have its own row and each value must have its own cell). To do this, we use tidytext’s unnest_tokens() function. We also remove the rarest words in that step, keeping only words in our dataset that occur more than 10 times.\nlibrary(dplyr) library(tidytext) tidy_books \u0026lt;- books %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% group_by(word) %\u0026gt;% filter(n() \u0026gt; 10) %\u0026gt;% ungroup()   author  document  word      Tesla, Nikola  1  experiments    Tesla, Nikola  1  with    Tesla, Nikola  1  alternate    Tesla, Nikola  1  currents    Tesla, Nikola  1  of    Tesla, Nikola  1  high    Tesla, Nikola  1  potential    Tesla, Nikola  1  and      2.2 Stop words Now that the data is in a tidy “one-word-per-row” format, we can manipulate it with packages like dplyr. Often in text analysis, we will want to remove stop words: Stop words are words that are not useful for an analysis, typically extremely common words such as “the”, “of”, “to”, and so forth. We can remove stop words in our data by using the stop words provided in the package stopwords with an anti_join() from the package dplyr.\nlibrary(stopwords) library(dplyr) library(tibble) stopword \u0026lt;- as_tibble(stopwords::stopwords(\u0026quot;en\u0026quot;)) stopword \u0026lt;- rename(stopword, word=value) tb \u0026lt;- anti_join(tidy_books, stopword, by = \u0026#39;word\u0026#39;)   author  document  word      Tesla, Nikola  1  experiments    Tesla, Nikola  1  alternate    Tesla, Nikola  1  currents    Tesla, Nikola  1  high    Tesla, Nikola  1  potential    Tesla, Nikola  1  high    Tesla, Nikola  1  frequency    Tesla, Nikola  3  lecture     The tidy data structure allows different types of exploratory data analysis (EDA), which we turn to next.\n  3 Exploratory data analysis 3.1 Term frequency (tf) An important question in text mining is how to quantify what a document is about. One measure of how important a word may be is its term frequency (tf), i.e. how frequently a word occurs in a document.\nWe can start by using dplyr to explore the most commonly used words.\nlibrary(dplyr) word_count \u0026lt;- count(tb, word, sort = TRUE)   word  n      one  239    body  230    may  224    can  194    relativity  193     Term frequency by author:\nlibrary(dplyr) author_count \u0026lt;- tb %\u0026gt;% count(author, word, sort = TRUE)   author  word  n      Einstein, Albert  relativity  193    Tesla, Nikola  may  184    Einstein, Albert  theory  181    Tesla, Nikola  bulb  171    Tesla, Nikola  coil  166    Tesla, Nikola  high  166    Einstein, Albert  body  156    Tesla, Nikola  one  156    Einstein, Albert  reference  150    Tesla, Nikola  tube  147     Plot terms with a frequency greater than 100:\nlibrary(dplyr) library(ggplot2) tb %\u0026gt;% count(author, word, sort = TRUE) %\u0026gt;% filter(n \u0026gt; 100) %\u0026gt;% mutate(word = reorder(word, n)) %\u0026gt;% ggplot(aes(word, n)) + geom_col(aes(fill=author)) + xlab(NULL) + scale_y_continuous(expand = c(0, 0)) + coord_flip() + theme_classic(base_size = 12) + labs(fill= \u0026quot;Author\u0026quot;, title=\u0026quot;Word frequency\u0026quot;, subtitle=\u0026quot;n \u0026gt; 100\u0026quot;)+ theme(plot.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;)) + scale_fill_brewer()  Plot top 20 terms by author:\nlibrary(ggplot2) tb %\u0026gt;% count(author, word, sort = TRUE) %\u0026gt;% group_by(author) %\u0026gt;% top_n(20) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(reorder_within(word, n, author), n, fill = author)) + geom_col(alpha = 0.8, show.legend = FALSE) + scale_x_reordered() + coord_flip() + facet_wrap(~author, scales = \u0026quot;free\u0026quot;) + scale_y_continuous(expand = c(0, 0)) + theme_classic(base_size = 12) + labs(fill= \u0026quot;Author\u0026quot;, title=\u0026quot;Most frequent words\u0026quot;, subtitle=\u0026quot;Top 20 words by book\u0026quot;, x= NULL, y= \u0026quot;Word Count\u0026quot;)+ theme(plot.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;)) + scale_fill_brewer()  You may notice expressions like “_k”, “co” in the Einstein text and “fig” in the Tesla text. Let’s remove these and other less meaningful words with a custom list of stop words and use anti_join() to remove them.\nnewstopwords \u0026lt;- tibble(word = c(\u0026quot;eq\u0026quot;, \u0026quot;co\u0026quot;, \u0026quot;rc\u0026quot;, \u0026quot;ac\u0026quot;, \u0026quot;ak\u0026quot;, \u0026quot;bn\u0026quot;, \u0026quot;fig\u0026quot;, \u0026quot;file\u0026quot;, \u0026quot;cg\u0026quot;, \u0026quot;cb\u0026quot;, \u0026quot;cm\u0026quot;, \u0026quot;ab\u0026quot;, \u0026quot;_k\u0026quot;, \u0026quot;_k_\u0026quot;, \u0026quot;_x\u0026quot;)) tb \u0026lt;- anti_join(tb, newstopwords, by = \u0026quot;word\u0026quot;) Now we plot the data again without the new stopwords:\nlibrary(ggplot2) tb %\u0026gt;% count(author, word, sort = TRUE) %\u0026gt;% group_by(author) %\u0026gt;% top_n(20) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(reorder_within(word, n, author), n, fill = author)) + geom_col(alpha = 0.8, show.legend = FALSE) + scale_x_reordered() + coord_flip() + facet_wrap(~author, scales = \u0026quot;free\u0026quot;) + scale_y_continuous(expand = c(0, 0)) + theme_classic(base_size = 12) + labs(fill= \u0026quot;Author\u0026quot;, title=\u0026quot;Most frequent words after removing stop words\u0026quot;, subtitle=\u0026quot;Top 20 words by book\u0026quot;, x= NULL, y= \u0026quot;Word Count\u0026quot;)+ theme(plot.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;)) + scale_fill_brewer()  You also may want to visualize the most frequent terms as a simple word cloud:\nlibrary(wordcloud) tb %\u0026gt;% count(word) %\u0026gt;% with(wordcloud(word, n, max.words = 15))  3.2 Term frequency and inverse document frequency (tf-idf) Term frequency is a useful measure to determine how frequently a word occurs in a document. There are words in a document, however, that occur many times but may not be important.\nAnother approach is to look at a term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. This can be combined with term frequency to calculate a term’s tf-idf (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used.\nThe inverse document frequency for any given term is defined as:\n\\[idf(\\text{term}) = \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)}\\]\nHence, term frequency and inverse document frequency allows us to find words that are characteristic for one document within a collection of documents. The tidytext package uses an implementation of tf-idf consistent with tidy data principles that enables us to see how different words are important in documents within a collection or corpus of documents.\nlibrary(forcats) plot_tb \u0026lt;- tb %\u0026gt;% count(author, word, sort = TRUE) %\u0026gt;% bind_tf_idf(word, author, n) %\u0026gt;% mutate(word = fct_reorder(word, tf_idf)) %\u0026gt;% mutate(author = factor(author, levels = c(\u0026quot;Tesla, Nikola\u0026quot;, \u0026quot;Einstein, Albert\u0026quot;))) plot_tb %\u0026gt;% group_by(author) %\u0026gt;% top_n(15, tf_idf) %\u0026gt;% ungroup() %\u0026gt;% mutate(word = reorder(word, tf_idf)) %\u0026gt;% ggplot(aes(word, tf_idf, fill = author)) + scale_y_continuous(expand = c(0, 0)) + geom_col(show.legend = FALSE) + labs(x = NULL, y = \u0026quot;tf-idf\u0026quot;) + facet_wrap(~author, ncol = 2, scales = \u0026quot;free\u0026quot;) + coord_flip() + theme_classic(base_size = 12) + labs(fill= \u0026quot;Author\u0026quot;, title=\u0026quot;Term frequency and inverse document frequency (tf-idf)\u0026quot;, subtitle=\u0026quot;Top 20 words by book\u0026quot;, x= NULL, y= \u0026quot;tf-idf\u0026quot;) + theme(plot.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;)) + scale_fill_brewer()  In particular, the bind_tf_idf function in the tidytext package takes a tidy text dataset as input with one row per token (term), per document. One column (word here) contains the terms/tokens, one column contains the documents (authors in this case), and the last necessary column contains the counts, how many times each document contains each term (n in this example).\ntf_idf \u0026lt;- tb %\u0026gt;% count(author, word, sort = TRUE) %\u0026gt;% bind_tf_idf(word, author, n)   author  word  n  tf  idf  tf_idf      Einstein, Albert  relativity  193  0.0177831  0.6931472  0.0123263    Tesla, Nikola  may  184  0.0139436  0.0000000  0.0000000    Einstein, Albert  theory  181  0.0166774  0.6931472  0.0115599    Tesla, Nikola  bulb  171  0.0129585  0.6931472  0.0089821    Tesla, Nikola  coil  166  0.0125796  0.6931472  0.0087195    Tesla, Nikola  high  166  0.0125796  0.0000000  0.0000000    Einstein, Albert  body  156  0.0143739  0.0000000  0.0000000    Tesla, Nikola  one  156  0.0118218  0.0000000  0.0000000    Einstein, Albert  reference  150  0.0138211  0.0000000  0.0000000    Tesla, Nikola  tube  147  0.0111397  0.0000000  0.0000000     Notice that idf and thus tf-idf are zero for extremely common words (like “may”). These are all words that appear in both documents, so the idf term (which will then be the natural log of 1) is zero. The inverse document frequency (and thus tf-idf) is very low (near zero) for words that occur in many of the documents in a collection; this is how this approach decreases the weight for common words. The inverse document frequency will be a higher number for words that occur in fewer of the documents in the collection.\n 3.3 Tokenizing by n-gram We’ve been using the unnest_tokens function to tokenize by word, or sometimes by sentence, which is useful for the kinds of frequency analyses we’ve been doing so far. But we can also use the function to tokenize into consecutive sequences of words, called n-grams. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them.\nlibrary(dplyr) library(tidytext) einstein_bigrams \u0026lt;- books %\u0026gt;% filter(author == \u0026quot;Einstein, Albert\u0026quot;) %\u0026gt;% unnest_tokens(bigram, text, token = \u0026quot;ngrams\u0026quot;, n = 2)   author  document  bigram      Einstein, Albert  3797  NA    Einstein, Albert  3798  NA    Einstein, Albert  3799  NA    Einstein, Albert  3800  NA    Einstein, Albert  3801  relativity the    Einstein, Albert  3801  the special    Einstein, Albert  3801  special and    Einstein, Albert  3801  and general    Einstein, Albert  3801  general theory    Einstein, Albert  3802  NA     We can examine the most common bigrams using dplyr’s count():\neinstein_bigrams_count \u0026lt;- einstein_bigrams %\u0026gt;% count(bigram, sort = TRUE)   bigram  n      NA  916    of the  613    to the  247    in the  197    of relativity  164    theory of  121    with the  119    on the  111    that the  110    of a  98     Now we use tidyr’s separate(), which splits a column into multiple columns based on a delimiter. This lets us separate it into two columns, “word1” and “word2”, at which point we can remove cases where either is a stop-word. This time, we use the stopwords from the package tidyr:\nlibrary(tidyr) # seperate words bigrams_separated \u0026lt;- einstein_bigrams %\u0026gt;% separate(bigram, c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;), sep = \u0026quot; \u0026quot;) # filter stop words and NA bigrams_filtered \u0026lt;- bigrams_separated %\u0026gt;% filter(!word1 %in% stop_words$word) %\u0026gt;% filter(!word2 %in% stop_words$word) %\u0026gt;% filter(!is.na(word1)) # new bigram counts: bigram_counts \u0026lt;- bigrams_filtered %\u0026gt;% count(word1, word2, sort = TRUE)   word1  word2  n      reference  body  56    gravitational  field  53    special  theory  35    ordinate  system  34    space  time  27    classical  mechanics  26    lorentz  transformation  23    measuring  rods  22    straight  line  17    rigid  body  16     This one-bigram-per-row format is helpful for exploratory analyses of the text. As a simple example, we might be interested in the most often mentioned “theory”:\nbigram_theory \u0026lt;- bigrams_filtered %\u0026gt;% filter(word2 == \u0026quot;theory\u0026quot;) %\u0026gt;% count(word1, sort = TRUE)   word1  n      special  35    lorentz  4    newton’s  4    _special  1    comprehensive  1    electrodynamic  1    electromagnetic  1     In other analyses you may be interested in the most common trigrams, which are consecutive sequences of 3 words. We can find this by setting n = 3:\ntrigram \u0026lt;- books %\u0026gt;% unnest_tokens(trigram, text, token = \u0026quot;ngrams\u0026quot;, n = 3) %\u0026gt;% separate(trigram, c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;, \u0026quot;word3\u0026quot;), sep = \u0026quot; \u0026quot;) %\u0026gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word, !word3 %in% stop_words$word, !is.na(word1)) %\u0026gt;% count(word1, word2, word3, sort = TRUE)   word1  word2  word3  n      _x_1  _x_2  _x_3  12    light  _in  vacuo_  10    reference  body  k  10    space  time  continuum  9    _x_2  _x_3  _x_4  8    reference  body  _k  8    disruptive  discharge  coil  6      3.4 Network analysis We may be interested in visualizing all of the relationships among words simultaneously, rather than just the top few at a time. As one common visualization, we can arrange the words into a network, or “graph.” Here we’ll be referring to a “graph” not in the sense of a visualization, but as a combination of connected nodes. A graph can be constructed from a tidy object since it has three variables:\n from: the node an edge is coming from to: the node an edge is going towards weight: A numeric value associated with each edge  The igraph package has many functions for manipulating and analyzing networks. One way to create an igraph object from tidy data is the graph_from_data_frame() function, which takes a data frame of edges with columns for “from”, “to”, and edge attributes (in this case n):\nlibrary(dplyr) library(igraph) # filter for only relatively common combinations bigram_graph \u0026lt;- bigram_counts %\u0026gt;% filter(n \u0026gt; 5) %\u0026gt;% graph_from_data_frame() We use the ggraph package to convert the igraph object into a ggraph with the ggraph function, after which we add layers to it, much like layers are added in ggplot2. For example, for a basic graph we need to add three layers: nodes, edges, and text:\nlibrary(ggraph) set.seed(123) ggraph(bigram_graph, layout = \u0026quot;fr\u0026quot;) + geom_edge_link() + geom_node_point() + geom_node_text(aes(label = name), vjust = 1, hjust = 1) Finally, we will change some settings to obtain to a better looking graph:\n We add the edge_alpha aesthetic to the link layer to make links transparent based on how common or rare the bigram is.\n We add directionality with an arrow, constructed using grid::arrow(), including an end_cap option that tells the arrow to end before touching the node.\n We tinker with the options to the node layer to make the nodes more attractive (larger, blue points).\n We add a theme that’s useful for plotting networks, theme_void().\n  library(ggraph) set.seed(123) a \u0026lt;- grid::arrow(type = \u0026quot;closed\u0026quot;, length = unit(.15, \u0026quot;inches\u0026quot;)) ggraph(bigram_graph, layout = \u0026quot;fr\u0026quot;) + geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(.07, \u0026#39;inches\u0026#39;)) + geom_node_point(color = \u0026quot;lightblue\u0026quot;, size = 5) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void()   4 Classification with logistic regression In the first part we will build a statistical learning model. In the second part we will want to test it and assess its quality. Without dividing the dataset we would test the model on the data which the algorithm have already seen, which is why we start by splitting the data.\n4.1 Train test split Let’s go back to the original books dataset (not the tidy_books dataset) because the lines of text are our individual observations.\nWe could use functions from the rsample package to generate resampled datasets, but the specific modeling approach we’re going to use will do that for us so we only need a simple train/test split.\nlibrary(rsample) books_split \u0026lt;- books %\u0026gt;% select(document) %\u0026gt;% initial_split(prop = 3/4) train_data \u0026lt;- training(books_split) test_data \u0026lt;- testing(books_split) Notice that we just select specific text rows (column document) for training and others for our test data (we set the proportion of data to be retained for modeling/analysis to 3/4) without selecting the actual text lines at this point.\n 4.2 Training data (sparse matrix) Now we want to transform our training data from a tidy data structure to a “sparse matrix” (these objects can be treated as though they were matrices, for example accessing particular rows and columns, but are stored in a more efficient format) to use for our classification algorithm.\nlibrary(tidytext) sparse_words \u0026lt;- tidy_books %\u0026gt;% count(document, word) %\u0026gt;% inner_join(train_data, by = \u0026quot;document\u0026quot;) %\u0026gt;% cast_sparse(document, word, n) dim(sparse_words) ## [1] 4782 892 We have over 4,700 training observations and almost 900 features. Text feature space handled in this way is very high dimensional, so we need to take that into account when considering our modeling approach.\nOne reason this overall approach is flexible is that you could at this point cbind() other columns, such as non-text numeric data, onto this sparse matrix. Then you can use this combination of text and non-text data as your predictors in the classifiaction algorithm, and the regularized regression algorithm we are going to use will find which are important for your problem space.\n 4.3 Response variable We also need to build a tibble with a response variable to associate each of the rownames() of the sparse matrix with an author, to use as the quantity we will predict in the model.\nword_rownames \u0026lt;- as.integer(rownames(sparse_words)) books_joined \u0026lt;- tibble(document = word_rownames) %\u0026gt;% left_join(books %\u0026gt;% select(document, author))   document  author      1  Tesla, Nikola    3  Tesla, Nikola    5  Tesla, Nikola    7  Tesla, Nikola    9  Tesla, Nikola    24  Tesla, Nikola    25  Tesla, Nikola      4.4 Logistic regression model Now it’s time to train our classification model. Let’s use the glmnet package to fit a logistic regression model with lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) regularization. This regression analysis method performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.\nGlmnet is a package that fits lasso models via penalized maximum likelihood. We do not cover the method and glmnet package in detail at this point, but if you want to learn more about glmnet and lasso regression, review the following resources:\n Introduction to glmnet glmnet documentation LASSO regression in Python  The package is very useful for text classification because the variable selection that lasso regularization performs can tell you which words are important for your prediction problem. The glmnet package also supports parallel processing, so we can train on multiple cores with cross-validation on the training set using cv.glmnet().\nlibrary(glmnet) library(doMC) registerDoMC(cores = 8) is_einstein \u0026lt;- books_joined$author == \u0026quot;Einstein, Albert\u0026quot; model \u0026lt;- cv.glmnet(sparse_words, is_einstein, family = \u0026quot;binomial\u0026quot;, parallel = TRUE, keep = TRUE) Let’s use the package broom (the broom package takes the messy output of built-in functions in R, such as lm, nls, or t.test, and turns them into tidy data frames) to check out the coefficients of the model, for the largest value of lambda with error within 1 standard error of the minimum (lambda.1se).\nlibrary(broom) coefs \u0026lt;- model$glmnet.fit %\u0026gt;% tidy() %\u0026gt;% filter(lambda == model$lambda.1se) Which coefficents are the largest in size, in each direction:\nlibrary(forcats) coefs %\u0026gt;% group_by(estimate \u0026gt; 0) %\u0026gt;% top_n(10, abs(estimate)) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate \u0026gt; 0)) + geom_col(alpha = 0.8, show.legend = FALSE) + coord_flip() + labs( x = NULL, title = \u0026quot;Coefficients that increase/decrease probability the most\u0026quot;, subtitle = \u0026quot;A document mentioning lecture or probably is unlikely to be written by Albert Einstein\u0026quot; ) + theme_classic(base_size = 12) + theme(plot.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;)) + scale_fill_brewer()   4.5 Model evaluation with test data Now we want to evaluate how well this model is doing using the test data that we held out and did not use for training the model. Let’s create a dataframe that tells us, for each document in the test set, the probability of being written by Albert Einstein.\nintercept \u0026lt;- coefs %\u0026gt;% filter(term == \u0026quot;(Intercept)\u0026quot;) %\u0026gt;% pull(estimate) classifications \u0026lt;- tidy_books %\u0026gt;% inner_join(test_data) %\u0026gt;% inner_join(coefs, by = c(\u0026quot;word\u0026quot; = \u0026quot;term\u0026quot;)) %\u0026gt;% group_by(document) %\u0026gt;% summarize(score = sum(estimate)) %\u0026gt;% mutate(probability = plogis(intercept + score))   document  score  probability      21  -1.3811800  0.2063129    26  -1.9929541  0.1235678    30  1.2522803  0.7834973    33  -1.8746267  0.1369635    52  -5.1987683  0.0056813    54  -2.8148527  0.0583613    56  0.2272565  0.5649167     Now let’s use the yardstick package (yardstick is a package to estimate how well models are working using tidy data principles) to calculate some model performance metrics. For example, what does the ROC curve (receiver operating characteristic curve - a graph showing the performance of a classification model at all classification thresholds) look like:\nlibrary(yardstick) comment_classes \u0026lt;- classifications %\u0026gt;% left_join(books %\u0026gt;% select(author, document), by = \u0026quot;document\u0026quot;) %\u0026gt;% mutate(author = as.factor(author)) comment_classes %\u0026gt;% roc_curve(author, probability) %\u0026gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_line( color = \u0026quot;midnightblue\u0026quot;, size = 1.5 ) + geom_abline( lty = 2, alpha = 0.5, color = \u0026quot;gray50\u0026quot;, size = 1.2 ) + labs( title = \u0026quot;ROC curve for text classification using regularized regression\u0026quot;, subtitle = \u0026quot;Predicting whether text was written by Albert Einstein or Nikola Tesla\u0026quot; ) + theme_classic(base_size = 12) + theme(plot.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;)) Let’s obtain the accuracy (AUC - the fraction of predictions that a classification model got right) on the test data:\nauc \u0026lt;- comment_classes %\u0026gt;% roc_auc(author, probability)   .metric  .estimator  .estimate      roc_auc  binary  0.9757987     Next we turn to the confusion matrix. Let’s make the following definitions:\n “Einstein, Albert” is a positive class. “Tesla, Nikola” is a negative class.    True Positive (TP):  False Positive (FP):     Reality: Text is from Einstein Reality: Text is from Tesla  Model: Text is from Einstein Model: Text is from Einstein      False Negative (FN):  True Negative (TN):     Reality: Text is from Einstein Reality: Text is from Tesla  Model: Text is from Tesla Model: Text is from Tesla    We can summarize our “einstein-text-prediction” model using a 2x2 confusion matrix that depicts all four possible outcomes:\n A true positive is an outcome where the model correctly predicts the positive class (Einstein). Similarly, a true negative is an outcome where the model correctly predicts the negative class (Tesla).\n A false positive is an outcome where the model incorrectly predicts the positive class. And a false negative is an outcome where the model incorrectly predicts the negative class.\n  Let’s use a probability of 0.5 as our threshold. That means all model predictions with a probability greater than 50% get labeld as beeing text from Einstein:\ncomment_classes %\u0026gt;% mutate(prediction = case_when( probability \u0026gt; 0.5 ~ \u0026quot;Einstein, Albert\u0026quot;, TRUE ~ \u0026quot;Tesla, Nikola\u0026quot;), prediction = as.factor(prediction)) %\u0026gt;% conf_mat(author, prediction) ## Truth ## Prediction Einstein, Albert Tesla, Nikola ## Einstein, Albert 628 58 ## Tesla, Nikola 70 784 Let’s take a closer look at these misclassifications: false negatives (FN) and false positives (FP). Which documents here were incorrectly predicted to be written by Albert Einstein, at the extreme probability end of greater than 80% (false positive)?\nFP\u0026lt;- comment_classes %\u0026gt;% filter(probability \u0026gt; .8, author == \u0026quot;Tesla, Nikola\u0026quot;) %\u0026gt;% sample_n(10) %\u0026gt;% inner_join(books %\u0026gt;% select(document, text)) %\u0026gt;% select(probability, text)   probability  text      0.8189629  through things. He is an omnivorous reader, who never forgets; and he    0.9012553  our sense of vision.    0.8094770  enormous distance without affecting greatly the character of the    0.9058630  experience of to-day enables us to see clearly why these coils under    0.8509898  discharger I have been able to maintain an oscillating motion without    0.8119290  little thought leads us to the conclusion that, could we but reach    0.9086652  disc, which could be seen from a considerable distance, such is the    0.9440000  obtainable at any point of the universe. This idea is not novel. Men    0.9069282  Leaving practicability out of consideration, this, then, would be the    0.8595897  plant, and on returning to Paris sought to carry out a number of ideas     These documents were incorrectly predicted to be written by Albert Einstein. However, they were written by Nikola Tesla.\nFinally, let’s take a look at the texts which are from Albert Einstein that the model did not correctly identify (false negative):\nFN \u0026lt;- comment_classes %\u0026gt;% filter(probability \u0026lt; .3, author == \u0026quot;Einstein, Albert\u0026quot;) %\u0026gt;% sample_n(10) %\u0026gt;% inner_join(books %\u0026gt;% select(document, text)) %\u0026gt;% select(probability, text)   probability  text      0.0969140  be arbitrary, although it was always tacitly made even before the    0.1989692  strings to the floor, otherwise the slightest impact against the floor    0.1994746  local variations of temperature, and with which we made acquaintance as    0.1932809  the conservation of energy (and of impulse).    0.0546119  me—and rightly so—and you declare: “I maintain my previous definition    0.0613870  permits of our answering it with a moderate degree of certainty, and in    0.2458622  treated in detail and with unsurpassable lucidity by Helmholtz and    0.1886392  gravitational potential, then the study of this displacement will    0.1570832  for the following reason. As a result of the more careful study of    0.0134175  throwing it. Then, disregarding the influence of the air resistance, I     We can conclude that the model did a very good job in predicting the authors of the texts. Furthermore, the texts of the misclassifications are quite short and we can imagine, that even a human reader who is familiar with the work of Einstein and Tesla would have difficulties to classify them correctly.\n  ","date":1568592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568592000,"objectID":"b904f4e1a912793d737185f8af52e2e2","permalink":"/post/2019-09-16-r-text-mining/","publishdate":"2019-09-16T00:00:00Z","relpermalink":"/post/2019-09-16-r-text-mining/","section":"post","summary":"Introduction to Text Mining in R with Tidytext","tags":["Statistics","R"],"title":"Text Mining in R","type":"post"},{"authors":null,"categories":null,"content":" Agenda Einführung  Einführung in Business Intelligence\n First steps in Markdown\n   SQL  Datenexploration (Selektieren, Ordnen und Filtern)\n Datentypen und Datentransformationen\n Gruppieren und Aggregieren\n Tabellen verbinden (Joins)\n Tabellen modifizieren\n Subqueries\n Microsoft Azure SQL Database\n  Literatur:\nDeBarros, A. (2018). Practical SQL: A Beginner’s Guide to Storytelling with Data. No Starch Press.\n R for Data Science  First Steps in R\n Introduction to Data Science with R\n Basic Analytics in R\n Data Exploration in R\n Correlation Analysis in R\n Natural Language Processing with R\n Advanced Programming in R\n Erstellung von interaktiven Tutorials in R.\n  Literatur:\n Wickham, H., \u0026amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc.\n Wickham, H. (2019). Advanced r. Chapman and Hall/CRC.\n Silge, J., \u0026amp; Robinson, D. (2017). Text mining with R: A tidy approach. “O’Reilly Media, Inc.”\n Xie, Y. (2019). Bookdown: Authoring Books and Technical Documents with R Markdown. Chapman and Hall/CRC.\n    ","date":1567504800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567504800,"objectID":"382835ef5e725bb0fa29f440760b980f","permalink":"/project/programming-languages/","publishdate":"2019-09-03T10:00:00Z","relpermalink":"/project/programming-languages/","section":"project","summary":"Unterlagen für Programming Languages for Data Science.","tags":["R","SQL","Markdown","DataScience"],"title":"Programming Languages for Data Science","type":"project"},{"authors":["Jan Kirenz"],"categories":["Tutorial"],"content":"A good place to start your journey into the basics of machine learning \u0026ndash;the science of getting computers to act without being explicitly programmed\u0026ndash; are the non-technical and easy to follow introductions to machine learning by Yufeng Guo from Google\u0026rsquo;s AI Adventures video-series:\n[](https://www.youtube.com/watch?v=HcqpanDadyQ\u0026quot;What is Machine Learning?\u0026quot;)\n[](https://www.youtube.com/watch?v=nKW8Ndu7Mjw\u0026quot;The 7 Steps of Machine Learning?\u0026quot;)\n  Google\u0026rsquo;s Machine Learning Crash Course  Google recently published a series of internal AI training resources originally developed for its engineers. The crash course provides a fast-paced and practical overview about the fundamental concepts of machine learning. Here you can learn and apply fundamental machine learning concepts, get real-world examples with the companion Kaggle competition, or visit Learn with Google AI to explore the full library of training resources.\n  Introduction to Machine Learning from Andrew Ng  In this excellent course from Andrew Ng, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself.\nThis course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include:\n (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI).   ","date":1565913600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565913600,"objectID":"389b0874c51ec9563bd2813804f2100f","permalink":"/project/intro-machine-learning/","publishdate":"2019-08-16T00:00:00Z","relpermalink":"/project/intro-machine-learning/","section":"project","summary":"Free Resources to Learn the Basics of Artificial Intelligence, Machine  Learning and Deep Learning","tags":["MachineLearning","DeepLearning","ArtificialIntelligence"],"title":"Learn the Basics of AI","type":"project"},{"authors":["Jan Kirenz"],"categories":["Statistics","Python"],"content":"  1 Social Network Analysis with NetworkX in Python 1.1 Social Network Basics 1.1.1 Symmetric Networks (undirected) 1.1.2 Asymmetric Networks (directed) 1.1.3 Weighted Networks  1.2 Clustering coefficient 1.3 Network Distance Measures 1.3.1 Degree 1.3.2 Distance 1.3.3 Breadth-first search 1.3.4 Eccentricity  1.4 Centrality measures 1.4.1 Degree Centrality 1.4.2 Eigenvector Centrality 1.4.3 Closeness Centrality 1.4.4 Betweenness Centrality  1.5 Facebook Case Study    1 Social Network Analysis with NetworkX in Python We use the module NetworkX in this tutorial. It is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\nIf you work with Anaconda, you can install the package as follows:\nconda install -c anaconda networkx Import modules:\nimport networkx as nx import matplotlib.pyplot as plt %matplotlib inline import warnings; warnings.simplefilter(\u0026#39;ignore\u0026#39;) 1.1 Social Network Basics Each network consists of:\n Nodes: The individuals whose network we are building. Edges: The connection between the nodes. It represents a relationship between the nodes of the network.  1.1.1 Symmetric Networks (undirected) The first network that we create is a group of people who work together. This is called a symmetric network because the relationship “working together” is a symmetric relationship: If A is related to B, B is also related to A.\nG_symmetric = nx.Graph() G_symmetric.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;Laura\u0026#39;) G_symmetric.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;Marc\u0026#39;) G_symmetric.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;John\u0026#39;) G_symmetric.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;Michelle\u0026#39;) G_symmetric.add_edge(\u0026#39;Laura\u0026#39;, \u0026#39;Michelle\u0026#39;) G_symmetric.add_edge(\u0026#39;Michelle\u0026#39;,\u0026#39;Marc\u0026#39;) G_symmetric.add_edge(\u0026#39;George\u0026#39;, \u0026#39;John\u0026#39;) G_symmetric.add_edge(\u0026#39;George\u0026#39;, \u0026#39;Steven\u0026#39;) print(nx.info(G_symmetric)) Name: Type: Graph Number of nodes: 6 Number of edges: 8 Average degree: 2.6667 Now we visualize the network with the draw_networkx() function.\nplt.figure(figsize=(5,5)) nx.draw_networkx(G_symmetric);  1.1.2 Asymmetric Networks (directed) What if the relationship between nodes is ‘child of’, then the relationship is no longer symmetric. This is the case if someone follows someone else on Twitter. Or in the case of hyperlinks.\nIf A is the child of B, then B is not a child of A. Such a network where the relationship is asymmetric (A is related to B, does not necessarily means that B is associated with A) is called an Asymmetric network.\nWe can build the asymmetric network in NetworkX using DiGraph method, which is short of Directional Graph.\nG_asymmetric = nx.DiGraph() G_asymmetric.add_edge(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;) G_asymmetric.add_edge(\u0026#39;A\u0026#39;,\u0026#39;D\u0026#39;) G_asymmetric.add_edge(\u0026#39;C\u0026#39;,\u0026#39;A\u0026#39;) G_asymmetric.add_edge(\u0026#39;D\u0026#39;,\u0026#39;E\u0026#39;) To make sure that all nodes are distinctly visible in the network, use the spring_layout() function, followed by the draw_networkx() function.\nnx.spring_layout(G_asymmetric) nx.draw_networkx(G_asymmetric)  1.1.3 Weighted Networks Till now we had networks without weights, but it is possible that networks are made with weights, for example, if in our initial network we consider the number of projects done together as a weight, we will get a weighted Network.\nLet us make one again of the employees, but this time we add weight to the network, each edge has a weight signifying the number of projects they have done together.\nG_weighted = nx.Graph() G_weighted.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;Laura\u0026#39;, weight=25) G_weighted.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;Marc\u0026#39;, weight=8) G_weighted.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;John\u0026#39;, weight=11) G_weighted.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;Michelle\u0026#39;,weight=1) G_weighted.add_edge(\u0026#39;Laura\u0026#39;, \u0026#39;Michelle\u0026#39;,weight=1) G_weighted.add_edge(\u0026#39;Michelle\u0026#39;,\u0026#39;Marc\u0026#39;, weight=1) G_weighted.add_edge(\u0026#39;George\u0026#39;, \u0026#39;John\u0026#39;, weight=8) G_weighted.add_edge(\u0026#39;George\u0026#39;, \u0026#39;Steven\u0026#39;, weight=4) elarge = [(u, v) for (u, v, d) in G_weighted.edges(data=True) if d[\u0026#39;weight\u0026#39;] \u0026gt; 8] esmall = [(u, v) for (u, v, d) in G_weighted.edges(data=True) if d[\u0026#39;weight\u0026#39;] \u0026lt;= 8] pos = nx.circular_layout(G_weighted) # positions for all nodes # nodes nx.draw_networkx_nodes(G_weighted, pos, node_size=700) # edges nx.draw_networkx_edges(G_weighted, pos, edgelist=elarge,width=6) nx.draw_networkx_edges(G_weighted, pos, edgelist=esmall,width=6, alpha=0.5, edge_color=\u0026#39;b\u0026#39;, style=\u0026#39;dashed\u0026#39;) # labels nx.draw_networkx_labels(G_weighted, pos, font_size=20, font_family=\u0026#39;sans-serif\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show();    1.2 Clustering coefficient It is observed that people who share connections in a social network tend to form associations. In other words, there is a tendency in a social network to form clusters.\nWe can determine the clusters of a node, local clustering coefficient, which is the fraction of pairs of the node’s friends (that is connections) that are connected with each other.\nTo determine the local clustering coefficient, we make use of nx.clustering(Graph, Node) function.\nIn the symmetric employee-network, you will find that Michelle has a local clustering coefficient of 0.67 and Laura has a local clustering coefficient of 1.\nThe average clustering coefficient (sum of all the local clustering coefficients divided by the number of nodes) for the symmetric employee-network is 0.867.\nnx.clustering(G_symmetric,\u0026#39;Michelle\u0026#39;) 0.6666666666666666 nx.clustering(G_symmetric,\u0026#39;Laura\u0026#39;) 1.0 nx.average_clustering(G_symmetric) 0.8277777777777778  1.3 Network Distance Measures 1.3.1 Degree Degree of a node defines the number of connections a node has. NetworkX has the function degree which we can use to determine the degree of a node in the network.\nnx.degree(G_symmetric, \u0026#39;Michelle\u0026#39;) 3 This will return a value of 3, as Michelle has worked with three employees in the network.\n 1.3.2 Distance We can also determine the shortest path between two nodes and its length in NetworkX using nx.shortest_path(Graph, Node1, Node2) and nx.shortest_path_length(Graph, Node1, Node2) functions respectively.\nnx.shortest_path(G_symmetric, \u0026#39;Michelle\u0026#39;, \u0026#39;John\u0026#39;) [\u0026#39;Michelle\u0026#39;, \u0026#39;Steven\u0026#39;, \u0026#39;John\u0026#39;] nx.shortest_path_length(G_symmetric, \u0026#39;Michelle\u0026#39;, \u0026#39;John\u0026#39;) 2  1.3.3 Breadth-first search We can find the distance of a node from every other node in the network using breadth-first search algorithm, starting from that node. networkX provides the function bfs_tree to do it.\nAnd so if you use M = nx.bfs_tree(G_symmetric, 'Michelle') and now draw this tree, we will get a network structure telling how we can reach other nodes of the network starting from Michelle .\nS = nx.bfs_tree(G_symmetric, \u0026#39;Steven\u0026#39;) nx.draw_networkx(S) M = nx.bfs_tree(G_symmetric, \u0026#39;Michelle\u0026#39;) nx.draw_networkx(M)  1.3.4 Eccentricity Eccentricity of a node A is defined as the largest distance between A and all other nodes.\nIt can be found using nx.eccentricity() function. In the symmetric employee-network, Michelle has an eccentricity of 2, and Steven has an eccentricity of 1 (he is connected to every other node).\nnx.eccentricity(G_symmetric,\u0026#39;Michelle\u0026#39;) 2 nx.eccentricity(G_symmetric,\u0026#39;Steven\u0026#39;) 1   1.4 Centrality measures Above we learned some of the network distance measures and they are useful in knowing how the information will spread through the network.\nIn this section, we will learn how to find the most important nodes (individuals) in the network. These parameters are called as centrality measures. Centrality Measures can help us in identifying popularity, most liked, and biggest influencers within the network.\n1.4.1 Degree Centrality The people most popular or more liked usually are the ones who have more friends.\nDegree centrality is a measure of the number of connections a particular node has in the network. It is based on the fact that important nodes have many connections. NetworkX has the function degree_centrality() to calculate the degree centrality of all the nodes of a network.\nnx.degree_centrality(G_symmetric) {\u0026#39;Steven\u0026#39;: 1.0, \u0026#39;Laura\u0026#39;: 0.4, \u0026#39;Marc\u0026#39;: 0.4, \u0026#39;John\u0026#39;: 0.4, \u0026#39;Michelle\u0026#39;: 0.6000000000000001, \u0026#39;George\u0026#39;: 0.4}  1.4.2 Eigenvector Centrality It is not just how many individuals one is connected too, but the type of people one is connected with that can decide the importance of a node.\nEigenvector centrality is a measure of how import a node is by accounting for the fact of how well it is connected to other important nodes.\nWe can use the eigenvector_centrality() function of NetworkX to calculate eigenvector centrality of all the nodes in a network.\nThe Google’s Pagerank algorithm is a variant of Eigenvector centrality algorithm.\nnx.eigenvector_centrality(G_symmetric) {\u0026#39;Steven\u0026#39;: 0.6006686104947806, \u0026#39;Laura\u0026#39;: 0.3545677660798074, \u0026#39;Marc\u0026#39;: 0.3545677660798074, \u0026#39;John\u0026#39;: 0.30844592433424667, \u0026#39;Michelle\u0026#39;: 0.4443904166426225, \u0026#39;George\u0026#39;: 0.30844592433424667}  1.4.3 Closeness Centrality Closeness Centrality is a measure where each node’s importance is determined by closeness to all other nodes.\nnx.closeness_centrality(G_symmetric) {\u0026#39;Steven\u0026#39;: 1.0, \u0026#39;Laura\u0026#39;: 0.625, \u0026#39;Marc\u0026#39;: 0.625, \u0026#39;John\u0026#39;: 0.625, \u0026#39;Michelle\u0026#39;: 0.7142857142857143, \u0026#39;George\u0026#39;: 0.625}  1.4.4 Betweenness Centrality The Betweenness Centrality is the centrality of control.\nIt represents the frequency at which a point occurs on the shortest paths that connected pair of points. It quantifies how many times a particular node comes in the shortest chosen path between two other nodes.\nThe nodes with high betweenness centrality play a significant role in the communication/information flow within the network.\nThe nodes with high betweenness centrality can have a strategic control and influence on others. An individual at such a strategic position can influence the whole group, by either withholding or coloring the information in transmission.\nNetworkx has the function betweenness_centrality() to measure it for the network. It has options to select if we want betweenness values to be normalized or not, weights to be included in centrality calculation or not, and to include the endpoints in the shortest path counts or not.\nnx.betweenness_centrality(G_symmetric) {\u0026#39;Steven\u0026#39;: 0.65, \u0026#39;Laura\u0026#39;: 0.0, \u0026#39;Marc\u0026#39;: 0.0, \u0026#39;John\u0026#39;: 0.0, \u0026#39;Michelle\u0026#39;: 0.05, \u0026#39;George\u0026#39;: 0.0} pos = nx.spring_layout(G_symmetric) betCent = nx.betweenness_centrality(G_symmetric, normalized=True, endpoints=True) node_color = [20000.0 * G_symmetric.degree(v) for v in G_symmetric] node_size = [v * 10000 for v in betCent.values()] plt.figure(figsize=(10,10)) nx.draw_networkx(G_symmetric, pos=pos, with_labels=True, node_color=node_color, node_size=node_size ) plt.axis(\u0026#39;off\u0026#39;); sorted(betCent, key=betCent.get, reverse=True)[:5] [\u0026#39;Steven\u0026#39;, \u0026#39;Michelle\u0026#39;, \u0026#39;Laura\u0026#39;, \u0026#39;Marc\u0026#39;, \u0026#39;John\u0026#39;]   1.5 Facebook Case Study This dataset consists of ‘circles’ (or ‘friends lists’) from Facebook. Facebook data was collected from survey participants using this Facebook app. The dataset includes node features (profiles), circles, and ego networks.\nFacebook data has been anonymized by replacing the Facebook-internal ids for each user with a new value. Also, while feature vectors from this dataset have been provided, the interpretation of those features has been obscured. For instance, where the original dataset may have contained a feature “political=Democratic Party”, the new data would simply contain “political=anonymized feature 1”. Thus, using the anonymized data it is possible to determine whether two users have the same political affiliations, but not what their individual political affiliations represent.\nSource: J. McAuley and J. Leskovec. Learning to Discover Social Circles in Ego Networks. NIPS, 2012\nLet us start with the Facebook data, for our analysis here we will use Facebook combined ego networks dataset, it contains the aggregated network of ten individuals’ Facebook friends list. You can download the required facebook_combined.txt file from the Stanford University site.\nWe read in the file and construct the Graph:\nDownload the file\nimport pandas as pd df = pd.read_csv(\u0026#39;/Users/jankirenz/Dropbox/Data/facebook_combined.txt\u0026#39;) df.info() \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 88233 entries, 0 to 88232 Data columns (total 1 columns): 0 1 88233 non-null object dtypes: object(1) memory usage: 689.4+ KB df.tail()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }      0 1       88228   4026 4030     88229   4027 4031     88230   4027 4032     88231   4027 4038     88232   4031 4038      G_fb = nx.read_edgelist(\u0026quot;/Users/jankirenz/Dropbox/Data/facebook_combined.txt\u0026quot;, create_using = nx.Graph(), nodetype=int) print(nx.info(G_fb)) Name: Type: Graph Number of nodes: 4039 Number of edges: 88234 Average degree: 43.6910\nThe network consists of 4,039 nodes, connected via 88,234 edges.\nplt.figure(figsize=(20,20)) nx.draw_networkx(G_fb); We can also visualize the network such that the node color varies with Degree and node size with Betweenness Centrality. The code to do this is:\npos = nx.spring_layout(G_fb) betCent = nx.betweenness_centrality(G_fb, normalized=True, endpoints=True) node_color = [20000.0 * G_fb.degree(v) for v in G_fb] node_size = [v * 10000 for v in betCent.values()] plt.figure(figsize=(20,20)) nx.draw_networkx(G_fb, pos=pos, with_labels=False, node_color=node_color, node_size=node_size ) plt.axis(\u0026#39;off\u0026#39;); You can also know the labels of the nodes with the highest betweenness centrality using:\nsorted(betCent, key=betCent.get, reverse=True)[:5] We can see that some nodes are common between Degree Centrality, which is a measure of degree, and Betweenness Centrality which controls the information flow.\nIt is natural that nodes that are more connected also lie on shortest paths between other nodes. The node 1912 is an important node as it is crucial according to all three centrality measures that we had considered.\nSources of examples:\n Datacamp; Aksakalli, C., McAuley, J. \u0026amp; Leskovec, J.    ","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"2393e2c84e8a07f97594f6d229d5f607","permalink":"/post/2019-08-13-network_analysis/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/post/2019-08-13-network_analysis/","section":"post","summary":"Introduction to Social Network Analysis with NetworkX","tags":["Statistics"],"title":"Social Network Analysis with Python","type":"post"},{"authors":null,"categories":null,"content":" NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\nMaterial:\n Introduction to network presentation slides Network analysis tasks with code templates  ","date":1565604000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565604000,"objectID":"38f963ba80811eb47af257387e025405","permalink":"/project/python-network-analysis-intro/","publishdate":"2019-08-12T10:00:00Z","relpermalink":"/project/python-network-analysis-intro/","section":"project","summary":"Network Analysis with the Python module NetworkX","tags":["Python","NetworkAnalysis"],"title":"Introduction to Network Analysis with Python","type":"project"},{"authors":["Jan Kirenz"],"categories":["Python","Statistics","Regression"],"content":"  1 Lasso Regression Basics 2 Implementation of Lasso regression 2.1 Standardization 2.2 Split data 2.3 Lasso regression 2.4 Lasso with different lambdas 2.5 Plot values as a function of lambda 2.6 Identify best lambda and coefficients 2.7 Cross Validation 2.8 Best Model    1 Lasso Regression Basics Lasso performs a so called L1 regularization (a process of introducing additional information in order to prevent overfitting), i.e. adds penalty equivalent to absolute value of the magnitude of coefficients.\nIn particular, the minimization objective does not only include the residual sum of squares (RSS) - like in the OLS regression setting - but also the sum of the absolute value of coefficients.\nThe residual sum of squares (RSS) is calculated as follows:\n\\[ RSS = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 \\]\nThis formula can be stated as:\n\\[ RSS = \\sum_{i=1}^{n} \\bigg(y_i - \\big( \\beta_{0} + \\sum_{j=1}^{p} \\beta_{j} x_{ij} \\big) \\bigg)^2 \\]\n n represents the number of distinct data points, or observations, in our sample. p denotes the number of variables that are available in the dataset. x_{ij} represents the value of the jth variable for the ith observation, where i = 1, 2, . . ., n and j = 1, 2, . . . , p.  In the lasso regression, the minimization objective becomes:\n\\[ \\sum_{i=1}^{n} \\bigg(y_i - \\big( \\beta_{0} + \\sum_{j=1}^{p} \\beta_{j} x_{ij} \\big) \\bigg)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\]\nwhich equals:\n\\[RSS + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\]\n\\(\\lambda\\) (lambda) provides a trade-off between balancing RSS and magnitude of coefficients.\n\\(\\lambda\\) can take various values:\n \\(\\lambda\\) = 0: Same coefficients as simple linear regression \\(\\lambda\\) = ∞: All coefficients zero (same logic as before) 0 \u0026lt; \\(\\lambda\\) \u0026lt; ∞: coefficients between 0 and that of simple linear regression   2 Implementation of Lasso regression Python set up:\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline plt.style.use(\u0026#39;ggplot\u0026#39;) import warnings; warnings.simplefilter(\u0026#39;ignore\u0026#39;) This notebook involves the use of the Lasso regression on the “Auto” dataset. In particular, we only use observations 1 to 200 for our analysis. Furthermore, you can drop the name variable.\nImport data:\ndf = pd.read_csv(\u0026quot;https://raw.githubusercontent.com/kirenz/datasets/master/Auto.csv\u0026quot;) Tidying data:\ndf = df.iloc[0:200] df = df.drop([\u0026#39;name\u0026#39;], axis=1) df.info() \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 8 columns): mpg 200 non-null float64 cylinders 200 non-null int64 displacement 200 non-null float64 horsepower 200 non-null object weight 200 non-null int64 acceleration 200 non-null float64 year 200 non-null int64 origin 200 non-null int64 dtypes: float64(3), int64(4), object(1) memory usage: 12.6+ KB df[\u0026#39;origin\u0026#39;] = pd.Categorical(df[\u0026#39;origin\u0026#39;]) df[\u0026#39;horsepower\u0026#39;] = pd.to_numeric(df[\u0026#39;horsepower\u0026#39;], errors=\u0026#39;coerce\u0026#39;) print(df.isnull().sum()) mpg 0 cylinders 0 displacement 0 horsepower 2 weight 0 acceleration 0 year 0 origin 0 dtype: int64 # drop missing cases df = df.dropna() We use scikit learn to fit a Lasso regression (see documentation) and follow a number of steps (note that scikit-learn uses \\(\\alpha\\) instead of \\(\\lambda\\) in their notation):\n2.1 Standardization Standardize the features with the module: from sklearn.preprocessing import StandardScaler\nIt is important to standardize the features by removing the mean and scaling to unit variance. The L1 (Lasso) and L2 (Ridge) regularizers of linear models assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\ndfs = df.astype(\u0026#39;int\u0026#39;) dfs.info() \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; Int64Index: 198 entries, 0 to 199 Data columns (total 8 columns): mpg 198 non-null int64 cylinders 198 non-null int64 displacement 198 non-null int64 horsepower 198 non-null int64 weight 198 non-null int64 acceleration 198 non-null int64 year 198 non-null int64 origin 198 non-null int64 dtypes: int64(8) memory usage: 13.9 KB dfs.columns Index([\u0026#39;mpg\u0026#39;, \u0026#39;cylinders\u0026#39;, \u0026#39;displacement\u0026#39;, \u0026#39;horsepower\u0026#39;, \u0026#39;weight\u0026#39;, \u0026#39;acceleration\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;origin\u0026#39;], dtype=\u0026#39;object\u0026#39;) from sklearn.preprocessing import StandardScaler scaler = StandardScaler() dfs[[\u0026#39;cylinders\u0026#39;, \u0026#39;displacement\u0026#39;, \u0026#39;horsepower\u0026#39;, \u0026#39;weight\u0026#39;, \u0026#39;acceleration\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;origin\u0026#39;]] = scaler.fit_transform(dfs[[\u0026#39;cylinders\u0026#39;, \u0026#39;displacement\u0026#39;, \u0026#39;horsepower\u0026#39;, \u0026#39;weight\u0026#39;, \u0026#39;acceleration\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;origin\u0026#39;]]) dfs.head(5)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }      mpg   cylinders   displacement   horsepower   weight   acceleration   year   origin       0   18   1.179744   0.726091   0.325216   0.346138   -0.955578   -1.516818   -0.629372     1   15   1.179744   1.100254   1.129264   0.548389   -1.305309   -1.516818   -0.629372     2   18   1.179744   0.821807   0.784672   0.273370   -1.305309   -1.516818   -0.629372     3   16   1.179744   0.699986   0.784672   0.270160   -0.955578   -1.516818   -0.629372     4   17   1.179744   0.682583   0.554944   0.287282   -1.655041   -1.516818   -0.629372       2.2 Split data Split the data set into train and test sets (use X_train, X_test, y_train, y_test), with the first 75% of the data for training and the remaining for testing. (module: from sklearn.model_selection import train_test_split)\nX = dfs.drop([\u0026#39;mpg\u0026#39;], axis=1) y = dfs[\u0026#39;mpg\u0026#39;] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)  2.3 Lasso regression Apply Lasso regression on the training set with the regularization parameter lambda = 0.5 (module: from sklearn.linear_model import Lasso) and print the \\(R^2\\)-score for the training and test set. Comment on your findings.\nfrom sklearn.linear_model import Lasso reg = Lasso(alpha=0.5) reg.fit(X_train, y_train) Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=‘cyclic’, tol=0.0001, warm_start=False)\nprint(\u0026#39;Lasso Regression: R^2 score on training set\u0026#39;, reg.score(X_train, y_train)*100) print(\u0026#39;Lasso Regression: R^2 score on test set\u0026#39;, reg.score(X_test, y_test)*100) Lasso Regression: R^2 score on training set 82.49741060950073 Lasso Regression: R^2 score on test set 85.49734440925533\n 2.4 Lasso with different lambdas Apply the Lasso regression on the training set with the following λ parameters: (0.001, 0.01, 0.1, 0.5, 1, 2, 10). Evaluate the R^2 score for all the models you obtain on both the train and test sets.\nlambdas = (0.001, 0.01, 0.1, 0.5, 1, 2, 10) l_num = 7 pred_num = X.shape[1] # prepare data for enumerate coeff_a = np.zeros((l_num, pred_num)) train_r_squared = np.zeros(l_num) test_r_squared = np.zeros(l_num) # enumerate through lambdas with index and i for ind, i in enumerate(lambdas): reg = Lasso(alpha = i) reg.fit(X_train, y_train) coeff_a[ind,:] = reg.coef_ train_r_squared[ind] = reg.score(X_train, y_train) test_r_squared[ind] = reg.score(X_test, y_test)  2.5 Plot values as a function of lambda Plot all values for both data sets (train and test \\(R^2\\)-values) as a function of λ. Comment on your findings.\n# Plotting plt.figure(figsize=(18, 8)) plt.plot(train_r_squared, \u0026#39;bo-\u0026#39;, label=r\u0026#39;$R^2$ Training set\u0026#39;, color=\u0026quot;darkblue\u0026quot;, alpha=0.6, linewidth=3) plt.plot(test_r_squared, \u0026#39;bo-\u0026#39;, label=r\u0026#39;$R^2$ Test set\u0026#39;, color=\u0026quot;darkred\u0026quot;, alpha=0.6, linewidth=3) plt.xlabel(\u0026#39;Lamda index\u0026#39;); plt.ylabel(r\u0026#39;$R^2$\u0026#39;) plt.xlim(0, 6) plt.title(r\u0026#39;Evaluate lasso regression with lamdas: 0 = 0.001, 1= 0.01, 2 = 0.1, 3 = 0.5, 4= 1, 5= 2, 6 = 10\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.grid()  2.6 Identify best lambda and coefficients Store your test data results in a DataFrame and indentify the lambda where the \\(R^2\\) has it’s maximum value in the test data. Fit a Lasso model with this lambda parameter (use the training data) and obtain the corresponding regression coefficients. Furthermore, obtain the mean squared error for the test data of this model (module: from sklearn.metrics import mean_squared_error)\ndf_lam = pd.DataFrame(test_r_squared*100, columns=[\u0026#39;R_squared\u0026#39;]) df_lam[\u0026#39;lambda\u0026#39;] = (lambdas) # returns the index of the row where column has maximum value. df_lam.loc[df_lam[\u0026#39;R_squared\u0026#39;].idxmax()] R_squared 88.105773 lambda 0.001000 Name: 0, dtype: float64\n# Coefficients of best model reg_best = Lasso(alpha = 0.1) reg_best.fit(X_train, y_train) reg_best.coef_ array([-0.35554113, -1.13104696, -0.00596296, -3.31741775, -0. , 0.37914648, 0.74902885])\nfrom sklearn.metrics import mean_squared_error mean_squared_error(y_test, reg_best.predict(X_test)) 3.586249592807347\n 2.7 Cross Validation Evaluate the performance of a Lasso regression for different regularization parameters λ using 5-fold cross validation on the training set (module: from sklearn.model_selection import cross_val_score) and plot the cross-validation (CV) \\(R^2\\) scores of the training and test data as a function of λ.\nUse the following lambda parameters: l_min = 0.05 l_max = 0.2 l_num = 20 lambdas = np.linspace(l_min,l_max, l_num)\nl_min = 0.05 l_max = 0.2 l_num = 20 lambdas = np.linspace(l_min,l_max, l_num) train_r_squared = np.zeros(l_num) test_r_squared = np.zeros(l_num) pred_num = X.shape[1] coeff_a = np.zeros((l_num, pred_num)) from sklearn.model_selection import cross_val_score for ind, i in enumerate(lambdas): reg = Lasso(alpha = i) reg.fit(X_train, y_train) results = cross_val_score(reg, X, y, cv=5, scoring=\u0026quot;r2\u0026quot;) train_r_squared[ind] = reg.score(X_train, y_train) test_r_squared[ind] = reg.score(X_test, y_test) # Plotting plt.figure(figsize=(18, 8)) plt.plot(train_r_squared, \u0026#39;bo-\u0026#39;, label=r\u0026#39;$R^2$ Training set\u0026#39;, color=\u0026quot;darkblue\u0026quot;, alpha=0.6, linewidth=3) plt.plot(test_r_squared, \u0026#39;bo-\u0026#39;, label=r\u0026#39;$R^2$ Test set\u0026#39;, color=\u0026quot;darkred\u0026quot;, alpha=0.6, linewidth=3) plt.xlabel(\u0026#39;Lamda value\u0026#39;); plt.ylabel(r\u0026#39;$R^2$\u0026#39;) plt.xlim(0, 19) plt.title(r\u0026#39;Evaluate 5-fold cv with different lamdas\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.grid()  2.8 Best Model Finally, store your test data results in a DataFrame and identify the lambda where the \\(R^2\\) has it’s maximum value in the test data. Fit a Lasso model with this lambda parameter (use the training data) and obtain the corresponding regression coefficients. Furthermore, obtain the mean squared error for the test data of this model (module: from sklearn.metrics import mean_squared_error)\ndf_lam = pd.DataFrame(test_r_squared*100, columns=[\u0026#39;R_squared\u0026#39;]) df_lam[\u0026#39;lambda\u0026#39;] = (lambdas) # returns the index of the row where column has maximum value. df_lam.loc[df_lam[\u0026#39;R_squared\u0026#39;].idxmax()] R_squared 87.897525 lambda 0.050000 Name: 0, dtype: float64\n# Best Model reg_best = Lasso(alpha = 0.144737) reg_best.fit(X_train, y_train) Lasso(alpha=0.144737, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=‘cyclic’, tol=0.0001, warm_start=False)\nfrom sklearn.metrics import mean_squared_error mean_squared_error(y_test, reg_best.predict(X_test)) 3.635187490993961\nreg_best.coef_ array([-0.34136411, -1.18223273, -0. , -3.27132984, 0. , 0.33262331, 0.71385488])\n  ","date":1565568000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565568000,"objectID":"ac43ad8d2960580a15c95be8929481ae","permalink":"/post/2019-08-12-python-lasso-regression-auto/","publishdate":"2019-08-12T00:00:00Z","relpermalink":"/post/2019-08-12-python-lasso-regression-auto/","section":"post","summary":"Implementation of Lasso Regression in Python","tags":["Statistics"],"title":"Lasso Regression with Python","type":"post"},{"authors":["Jan Kirenz"],"categories":["tutorial"],"content":"Correlation is a way of measuring the extent to which two variables are related. This means we need to analyze whether as one variable increases, the other\n (1) increases, (2) decreases or (3) stays the same.  This can be done by calculating the covariance or correlation of two variables.\nIn this Correlation Tutorial in R, we use a small dataset to illustrate the concepts of covariance and correlation. You may also download the Rmarkdown file and open it in RStudio.\nCheck your understanding with multiple choice tasks\n","date":1565499600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565499600,"objectID":"7352002437c59ef78e85aacacec73f5f","permalink":"/project/r-correlation-tutorial/","publishdate":"2019-08-11T05:00:00Z","relpermalink":"/project/r-correlation-tutorial/","section":"project","summary":"Introduction to correlation analysis with R.","tags":["Statistics","R","DataExploration"],"title":"Correlation Tutorial with R","type":"project"},{"authors":["Jan Kirenz"],"categories":["tutorial"],"content":"Lasso Regression In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces (Wikipedia).\n Lasso Regression with Python (Auto Data): Jupyter Notebook\n ","date":1565481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565481600,"objectID":"f470393a44e6222966c47805a878add9","permalink":"/project/r-lasso-regression/","publishdate":"2019-08-11T00:00:00Z","relpermalink":"/project/r-lasso-regression/","section":"project","summary":"Introduction to Lasso Regression with Python.","tags":["Statistics","Python","Regression"],"title":"Lasso Regression with Python","type":"project"},{"authors":["Jan Kirenz"],"categories":["tutorial"],"content":"Time Series Analysis with Python Time series analysis can be used in a multitude of business applications for forecasting a quantity into the future and explaining its historical patterns. Here are just a few examples of possible use cases:\n Explaining seasonal patterns in sales Predicting the expected number of incoming or churning customers Estimating the effect of a newly launched product on number of sold units   Introduction to Time Series Analysis with Python: Fit ARIMA and SARIMAX-Models with Statsmodel: Jupyter Notebook\n Introduction to Facebook\u0026rsquo;s time series analysis modul Prophet: Jupyter Notebook\n ","date":1565481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565481600,"objectID":"cdb80853bd478e55954960b7ce45945f","permalink":"/project/python-time-series/","publishdate":"2019-08-11T00:00:00Z","relpermalink":"/project/python-time-series/","section":"project","summary":"Time Series Analysis with Python. Introduction to ARIMA, SARIMAX and Facebook's Prophet","tags":["Statistics","Python","TimeSeries"],"title":"Time Series Analysis with Python","type":"project"},{"authors":["Jan Kirenz"],"categories":["Statistics","R","German"],"content":"  1 Deskriptive Statistik in R 1.1 Datenimport 1.2 Deskriptive Statistiken 1.2.1 Mittelwert 1.2.2 Standardabweichung 1.2.3 Getrimmter Mittelwert 1.2.4 Schiefe 1.2.5 Kurtosis 1.2.6 Standardfehler     1 Deskriptive Statistik in R In diesem Beitrag wird die Berechnung einfacher deskriptiver Statistiken und die Visualisierung von Verteilungen in R am Beispiel des Datensatzes “Advertising” behandelt.\n1.1 Datenimport  Datensatz: Advertising.csv Variablen: TV, radio, newspaper = jeweils Werbeausgaben in Dollar; sales = Produkte in Tausend Einheiten Abhängige Variable (dependent variable, response): sales Unabhängige Variablen (independent variables, predictors): TV, radio, newspaper, sales  Zunächts möchten wir uns einen Überblick über die Daten verschaffen. Dafür importieren wir die Daten und prüfen, ob die Skalenniveaus korrekt sind. Für die weiteren Berechnungen wird die Variable X1 nicht benötigt, weshalb wir diese löschen.\nlibrary(tidyverse) # Daten importieren Advertising \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/kirenz/datasets/master/advertising.csv\u0026quot;) # Überblick über die Daten verschaffen (Skalenniveaus prüfen) head(Advertising) ## # A tibble: 6 x 5 ## X1 TV radio newspaper sales ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 230. 37.8 69.2 22.1 ## 2 2 44.5 39.3 45.1 10.4 ## 3 3 17.2 45.9 69.3 9.3 ## 4 4 152. 41.3 58.5 18.5 ## 5 5 181. 10.8 58.4 12.9 ## 6 6 8.7 48.9 75 7.2 # Bereinigung der Daten Advertising$X1 \u0026lt;- NULL  1.2 Deskriptive Statistiken Ausgabe unterschiedlicher deskriptiver Statistiken:\nlibrary(psych) psych::describe(Advertising)  ## vars n mean sd median trimmed mad min max range ## TV 1 200 147.04 85.85 149.75 147.20 108.82 0.7 296.4 295.7 ## radio 2 200 23.26 14.85 22.90 23.00 19.79 0.0 49.6 49.6 ## newspaper 3 200 30.55 21.78 25.75 28.41 23.13 0.3 114.0 113.7 ## sales 4 200 14.02 5.22 12.90 13.78 4.82 1.6 27.0 25.4 ## skew kurtosis se ## TV -0.07 -1.24 6.07 ## radio 0.09 -1.28 1.05 ## newspaper 0.88 0.57 1.54 ## sales 0.40 -0.45 0.37  Hinweise zu den Kennzahlen:  vars: Nummer der Variable n: Anzahl der Beobachtungen mean: arithmetischer Mittelwert sd: empirische Standardabweichung median: Median trimmed: getrimmter Mittelwert mad: Mittlere absolute Abweichung vom Median min: kleinster Beobachtungswert max: größter Beobachtungswert range: Spannweite skew: Schiefe kurtosis: Wölbung se = Standardfehler   1.2.1 Mittelwert Bei der Berechnung des arithmetischen Mittelwerts in R sollte immer die Anweisung gegeben werden, fehlende Werte auszuschließen (na.rm = “remove values which are not available”). Ansonsten stoppt R bei fehlenden Werten die Berechnung und gibt eine Fehlermeldung aus.\nmean_sales \u0026lt;- mean(Advertising$sales, na.rm = TRUE) print(paste0(\u0026quot;Mittelwert der Variable Sales: \u0026quot;, mean_sales)) ## [1] \u0026quot;Mittelwert der Variable Sales: 14.0225\u0026quot;  1.2.2 Standardabweichung Die Standardabweichung ist ein häufig verwendetes Streuungsmaß und beschreibt die mittlere Abweichung der einzelnen Messwerte vom empirischen Mittelwert. Die Standardabweichung ist die positive Wurzel der empirischen Varianz. Die Varianz einer Stichprobe wird wie folgt berechnet: \\[s^{2} = \\frac{\\sum_{i=1}^{n} \\left(x_{i} - \\bar{x}\\right)^{2}} {n-1}\\]\nBerechnung der Standardabweichung: \\[s = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n} \\left(x_{i} - \\bar{x}\\right)^{2}} {n-1}}\\]\nvar_sales \u0026lt;- var(Advertising$sales, na.rm = TRUE) print(paste0(\u0026quot;Varianz der Variable Sales: \u0026quot;, round(var_sales, 2))) ## [1] \u0026quot;Varianz der Variable Sales: 27.22\u0026quot; sd_sales \u0026lt;- sd(Advertising$sales, na.rm = TRUE) print(paste0(\u0026quot;Standardabweichung der Variable Sales: \u0026quot;, round(sd_sales,2))) ## [1] \u0026quot;Standardabweichung der Variable Sales: 5.22\u0026quot;  1.2.3 Getrimmter Mittelwert Bei dem getrimmten Mittelwert wird ein bestimmer Anteil der größten und kleinsten Beobachtungen - hier oberhalb des 90% Quantils und unterhalb des 10 % Quantils - ignoriert. Damit sollen Ausreißer aus der Berechnung des Mittelwerts ausgeschlossen werden. Der getrimmte Mittelwert kann wie folgt in R berechnet werden:\nmean_trim_sales \u0026lt;- mean(Advertising$sales, trim = 0.1, na.rm = TRUE) print(paste0(\u0026quot;Getrimmter Mittelwert der Variable Sales: \u0026quot;, round(mean_trim_sales, 2))) ## [1] \u0026quot;Getrimmter Mittelwert der Variable Sales: 13.78\u0026quot;  1.2.4 Schiefe Die Schiefe ist eine statistische Kennzahl, die die Art und Stärke der Asymmetrie einer Wahrscheinlichkeitsverteilung beschreibt. Sie zeigt an, ob und wie stark die Verteilung nach rechts (positive Schiefe) oder nach links (negative Schiefe) geneigt ist. Jede nicht symmetrische Verteilung heißt schief.\nDarstellung der Verteilung in einem Histogramm:\nlibrary(ggplot2) # Vorlage für die Erstellung von plots in ggplot2 plot_1 \u0026lt;- theme_bw() + theme(axis.text.x = element_text(angle = 0, size = 8, family=\u0026quot;Arial\u0026quot;, colour=\u0026#39;black\u0026#39;), axis.text.y = element_text(angle = 0, size = 8, family=\u0026quot;Arial\u0026quot;, colour=\u0026#39;black\u0026#39;), axis.title = element_text(size=8, face=\u0026quot;bold\u0026quot;, family=\u0026quot;Arial\u0026quot;, colour=\u0026#39;black\u0026#39;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.title=element_text(hjust=0, size=10, family=\u0026quot;Arial\u0026quot;, face=\u0026quot;bold\u0026quot;, colour=\u0026#39;black\u0026#39;)) ggplot(Advertising, aes(sales)) + geom_histogram(binwidth = 2, color=\u0026quot;red\u0026quot;, alpha=.2) + scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + labs(title=\u0026quot;Histogramm für Sales\u0026quot;, x=\u0026quot;Sales\u0026quot;, y=\u0026quot;Anzahl\u0026quot;) + plot_1 Darstellung der Verteilung in einer Dichtefunktion:\nlibrary(ggplot2) ggplot(Advertising, aes(sales)) + geom_density(fill=\u0026quot;grey\u0026quot;,alpha=.2 ) + geom_vline(aes(xintercept=mean(sales, na.rm=TRUE)), color=\u0026quot;red\u0026quot;, linetype=\u0026quot;dotted\u0026quot;, size=0.6) + geom_vline(aes(xintercept=median(sales, na.rm=TRUE)), color=\u0026quot;red\u0026quot;, linetype=\u0026quot;dotted\u0026quot;, size=0.6) + geom_text(aes(x=median(sales), y=0.02), colour = \u0026quot;grey\u0026quot;, size =3, label=round(mean(Advertising$sales), digits=2), hjust=-1, family=\u0026quot;Arial\u0026quot;) + geom_text(aes(x=mean(sales), y=0.02), hjust=-0.7, colour = \u0026quot;grey\u0026quot;, size = 3, label=\u0026quot;Mittelwert\u0026quot;, family=\u0026quot;Arial\u0026quot;) + geom_text(aes(x=median(sales), y=0.005), colour = \u0026quot;grey\u0026quot;, size =3, label=round(median(Advertising$sales), digits=2), hjust=1 , family=\u0026quot;Arial\u0026quot;) + geom_text(aes(x=median(sales), y=0.01), colour = \u0026quot;grey\u0026quot;, size = 3, label=\u0026quot;Median\u0026quot;, hjust=1, family=\u0026quot;Arial\u0026quot;) + labs(x=\u0026quot;Produktabsatz (in Tausend Einheiten)\u0026quot;, y = \u0026quot;Dichte\u0026quot;, title = \u0026quot;Wahrscheinlichkeitsdichtefunktion\u0026quot;) + plot_1 In der Abbildung kann man erkennen, dass es sich um eine asymmetrische Verteilung handelt (d.h. es liegt eine Abweichung von der Normalverteilung vor). Konkret handelt es sich um eine rechtsschiefe Verteilung (Mittelwert \u0026gt; Median; Schiefe = + 0.40).\n 1.2.5 Kurtosis Die Abweichung des Verlaufs einer Verteilung vom Verlauf einer Normalverteilung wird Kurtosis (Wölbung) genannt. Sie gibt an, wie spitz die Kurve verläuft. Unterschieden wird zwischen positiver, spitz zulaufender (leptokurtische Verteilung) und negativer, flacher (platykurtische Verteilung) Kurtosis. Die Kurtosis zählt zu den zentralen Momenten einer Verteilung, mittels derer der Kurvenverlauf definiert wird. Eine Kurtosis mit Wert 0 ist normalgipflig (mesokurtisch), ein Wert größer 0 ist steilgipflig und ein Wert unter 0 ist flachgipflig.\n 1.2.6 Standardfehler Der Standardfehler ein Maß für die durchschnittliche Abweichung des geschätzten Parameterwertes vom wahren Parameterwert. Je kleiner der Standardfehler ist, desto genauer kann der unbekannte Parameter der Population mit Hilfe der Schätzfunktion geschätzt werden. Der Standardfehler hängt unter anderem von dem Stichprobenumfang und der Varianz ab. Allgemein gilt: Je größer der Stichprobenumfang, desto kleiner der Standardfehler; je kleiner die Varianz, desto kleiner der Standardfehler.\n   ","date":1564876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564876800,"objectID":"db44e21c6ccda2defbb07fd439afc990","permalink":"/post/2019-08-01-r-descriptive-statistics/","publishdate":"2019-08-04T00:00:00Z","relpermalink":"/post/2019-08-01-r-descriptive-statistics/","section":"post","summary":"Berechnung von Statistiken in R.","tags":["Statistics"],"title":"Deskriptive Statistik in R","type":"post"},{"authors":["Jan Kirenz"],"categories":["tutorial"],"content":"Linear Regression Tutorial in Python Linear regression is the fundamental starting point for all regression methods. In this Jupyter Notebook, we fit a regression model in Python and take a closer look at the following topics:\n Histogramms Boxplots Mean Standard deviation Mean squared error $R^2$ Pearson\u0026rsquo;s correlation coefficient F-Statistic Standard error Confidence interval  Download the linear regression tutorial Jupyter Notebook in GitHub and open the file in Jupyter Notebook.\n","date":1564876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564876800,"objectID":"2e29f9d735ed71ed3c94cf69433d617c","permalink":"/project/python-linear-regression/","publishdate":"2019-08-04T00:00:00Z","relpermalink":"/project/python-linear-regression/","section":"project","summary":"Introduction to essential concepts of linear regression with Python.","tags":["Statistics","Python","Regression","DataExploration"],"title":"Linear Regression Tutorial with Python","type":"project"},{"authors":["Jan Kirenz"],"categories":["course"],"content":"First Steps in Python Download the PDF Python overview to get an overview about Python and a list of helpful resources (you need to download the file in order to use the embedded links).\nFirst of all, install Anaconda - it\u0026rsquo;s a free and open-source distribution of the Python programming language that aims to simplify package management and deployment. It already contains Jupyter Notebook (see below) and other important data science modules.\n Install Anaconda (select the current version of Python 3). After installation, launch the Anaconda Navigator and start Jupyter Notebook or Jupyter Lab.  One important third-party tool for data science is Jupyter, an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Learn why Jupyter is data scientists\u0026rsquo; computational notebook of choice:\n  Perkel, J. M. (2018). Why Jupyter is data scientists\u0026rsquo; computational notebook of choice. Nature, 563(7729), p. 145.\n  Pandey, P. (2018). Bringing the best out of Jupyter Notebooks for Data Science. Towards Data Science\n  Pandey, P. (2019). Jupyter Lab: Evolution of the Jupyter Notebook. Towards Data Science\n   Now, let\u0026rsquo;s start with some code examples:\n  Import and save CSV-files with Pandas\n  Check for missing values\n  Change data type (level of measurment)\n  Descriptive statistics  Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud. With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser:\n  Overview of the data science programming process\n  Recommended reading: \u0026ldquo;A Whirlwind Tour of Python\u0026rdquo; is a free and fast-paced introduction to essential features of the Python language. The material is particularly designed for those who wish to use Python for data science and/or scientific programming:\n VanderPlas, J. (2016). Whirlwind Tour of Python. O\u0026rsquo;Reilly Media.  ","date":1564790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564790400,"objectID":"406212c849cb00a72f75fea461300409","permalink":"/project/python-first-steps/","publishdate":"2019-08-03T00:00:00Z","relpermalink":"/project/python-first-steps/","section":"project","summary":"Learn Data Science with Python","tags":["Python","DataScience"],"title":"First Steps in Python","type":"project"},{"authors":["Jan Kirenz"],"categories":["course"],"content":"First Steps in R Download the PDF R overview to get an overview about R and a list of helpful resources (you need to download the file in order to use the embedded links).\nInstalling R The first step is to install R. You can download and install R from the Comprehensive R Archive Network (CRAN).\nWindows:\n Open the Comprehensive R Archive Network. Click on “CRAN”. You’ll see a list of mirror sites, organized by country. Select a site near you. Click on “Windows” under “Download and Install R”. Click on “base”. Click on the link for downloading the latest version of R (an .exe file). When the download completes, double-click on the .exe file and answer the usual questions.  Mac:\n Open the Comprehensive R Archive Network. Click on “CRAN”. You’ll see a list of mirror sites, organized by country. Select a site near you. Click on “MacOS X”. Click on the .pkg file for the latest version of R, under “Files:”, to download it. When the download completes, double-click on the .pkg file and answer the usual questions.  Installing RStudio The next step is to install RStudio, a free and open-source integrated development environment (IDE) for R. You can use it for viewing and running R scripts.\n Go to RStudio Download Click the Download RStudio Desktop button. Select the installation file for your system. Run the installation file.  Learn R Basics First of all, you can take an online course to master the basics of R: Visit the interactive R-Course from DataCamp. With the knowledge gained in this courses, you will be ready to undertake your first very own data analysis.\nThere are also open and free resources and reference guides for R. Two examples are:\n Quick-R: a quick online reference for data input, basic statistics and plots R reference card (PDF) by Tom Short  Two key things you need to know about R is that you can get help for a function using help or ?, like this:\n?install.packages help(\u0026quot;install.packages\u0026quot;) and the hash character represents comments, so text following these characters is not interpreted:\n##This is just a comment Installing R Packages The first R command we will run is install.packages.\nAn R package is a collection of functions, data, and documentation that extends the capabilities of base R. Many of these functions are stored in CRAN. You can easily install packages from within RStudio if you know the name of the packages.\nAs an example, we are going to install the package dplyr which we use in our first data analysis examples:\ninstall.packages(\u0026quot;dplyr\u0026quot;) We can then load the package into our R sessions using the library function:\nlibrary(dplyr) From now on you will see that we sometimes load packages without installing them. This is because you only need to install a package once, but you need to reload it with the command library every time you start a new R session.\nIf you try to load a package and get an error, it probably means you need to install it first.\nReview the dplyr-documentation to get an overview about the different functionalities of this package.\n","date":1564790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564790400,"objectID":"0e669adf7ec92f519cbd427c62406a42","permalink":"/project/r-first-steps/","publishdate":"2019-08-03T00:00:00Z","relpermalink":"/project/r-first-steps/","section":"project","summary":"Learn Data Science with R","tags":["R","DataScience"],"title":"First Steps in R","type":"project"},{"authors":null,"categories":null,"content":"","date":1564704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564704000,"objectID":"c7297892dd13604a0e171a4f21c6c629","permalink":"/project/blogdown-book/","publishdate":"2019-08-02T00:00:00Z","relpermalink":"/project/blogdown-book/","section":"project","summary":"A practical guide for creating websites using the blogdown package in R","tags":["book","DataReporting","R"],"title":"Creating Websites with R Markdown","type":"project"},{"authors":null,"categories":null,"content":" The xaringan package is an R Markdown extension based on the JavaScript library remark.js to generate HTML5 presentations in different themes (Xie 2019).\nYou can learn more about the usage of the xaringan package from this excellent documentation, which is actually a set of slides generated from xaringan.\nXie, Y. (2019). Xaringan: Presentation Ninja. CRAN\n","date":1564653600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564653600,"objectID":"b19c0e15b29b8a6a0eab449da497b019","permalink":"/project/r-xaringan/","publishdate":"2019-08-01T10:00:00Z","relpermalink":"/project/r-xaringan/","section":"project","summary":"Xaringan - An R package for creating slideshows with R Markdown.","tags":["R","DataReporting","R Markdown"],"title":"Create Data Presentations with R Markdown","type":"project"},{"authors":["Jan Kirenz"],"categories":["Tutorial"],"content":"\u0026ldquo;R for Data Science\u0026rdquo; offers an excellent introduction into data science in R with a focus on the popular package collection tidyverse. See how the tidyverse makes data science faster, easier and more fun:\n Wickham, H., \u0026amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O\u0026rsquo;Reilly Media, Inc.  \u0026ldquo;An Introduction to Statistical Learning\u0026rdquo; provides an accessible overview of the field of statistical learning with applications in R. This book presents important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more:\n James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An introduction to statistical learning with applications in R (Corr. 7th printing 2017). New York: Springer.  \u0026ldquo;Statistical Thinking for the 21 Century\u0026rdquo; and \u0026ldquo;Modern Dive: Statistical Inference via Data Science\u0026rdquo; are both open-source digital textbooks which provide a great introduction into the fundamentals of modern quantitative methods which take advantage of today’s increased computing power to solve statistical problems with R:\n  Poldrack, R. A. (2019). Statistical Thinking for the 21 Century. http://thinkstats.org.\n  Ismay, C. \u0026amp; Kim, A. Y. (2019). Modern Dive: Statistical Inference via Data Science. https://moderndive.com\n  ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"d6af7dd0858ce76a023eff631f6738de","permalink":"/project/r-data-science-statistics/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/project/r-data-science-statistics/","section":"project","summary":"Free resources to learn data science and statistics with R.","tags":["Statistics","R","StatisticalLearning","DataExploration"],"title":"Free books to learn Data Science \u0026 Statistics with R","type":"project"},{"authors":["Jan Kirenz"],"categories":["blogdown","R"],"content":"  1 Introduction to Blogdown 2 GitHub 3 Terminal or GitHub Desktop 4 RStudio 5 Build your site in RStudio 5.1 Picking a theme 5.2 Update project options 5.3 Edit your configurations 5.4 Addins \u0026amp; workflow 5.5 Posting 5.5.1 Draft posts 5.5.2 New markdown posts 5.5.3 New R Markdown (.Rmd) posts 5.5.4 Adding images to a post   6 Deploy in Netlify 7 Going further 7.1 Custom CSS 7.2 Formspree 7.3 *.rbind.io domain names    1 Introduction to Blogdown The content below is taken from the excellent post “Up \u0026amp; Running with blogdown” from Alison Hill\nBefore you start, I recommend reading the following:\n blogdown: Creating Websites with R Markdown by Yihui Xie and Amber Thomas  Also note that I am a macOS user, and I use R, RStudio, Git (usually via GitHub), and terminal regularly, so I’m assuming familiarity here with all of these. If that is not the case, here are some places to get started:\n For Git: Happy Git with R by Jenny Bryan et al. For RStudio: DataCamp’s Working with the RStudio IDE (free) by Garrett Grolemund For Terminal: The Command Line Murder Mystery by Noah Veltman, and The UNIX Workbench by Sean Kross  I also have Xcode and Homebrew installed- you will probably need these to download Hugo. If you don’t have either but are on a mac, this link may help:\n How to install Xcode, Homebrew, Git, RVM, Ruby \u0026amp; Rails on Mac OS X  Introduction to static site generators and how domain names work:\n “Considering the cost and friendliness to beginners, I currently recommend Netlify.” “If you are not familiar with domain names or do not want to learn more about them, an option for your consideration is a free subdomain *.rbind.io offered by RStudio, Inc.”.   2 GitHub Go online to your GitHub account, and create a new repository (check to initialize with a README but don’t add .gitignore- this will be taken care of later). For naming your repo, consider your future deployment plan:\n If you are going to use Netlify to host the site, you can name this repository anything you want!  You can see some of the repo names used by members of the rbindorganization here.    If you want to host your site as a GitHub Page, you should name your repository yourgithubusername.github.io.   Screenshot above: Creating a new repository in GitHub\n Go to the main page of your new repository, and under the repository name, click the green Clone or download button.\n In the Clone with HTTPs section, click on the clipboard icon to copy the clone URL for your new repository. You’ll paste this text into terminal in the next section.\n   3 Terminal or GitHub Desktop Now you will clone your remote repository and create a local copy on your computer so you can sync between the two locations (using terminal or your alternative command line tool for a Windows machine). However, I recommend to use GitHub Desktop instead of the terminal for the cloning process. If you instead would like to use the terminal, this is how you proceed:\nUse cd to navigate into the directory where you want your repo to be\n Once there, type: git clone [paste]. So my command looked like this:\n  git clone https://github.com/apreshill/apreshill.git And this is what printed to the terminal window:\nCloning into \u0026#39;apreshill\u0026#39;... remote: Counting objects: 3, done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), done. Checking connectivity... done. Close terminal, you are done in there.   4 RStudio Install blogdown from your RStudio console. If you already have devtools installed like I did, you can just use the second line below:  if (!requireNamespace(\u0026quot;devtools\u0026quot;)) install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;rstudio/blogdown\u0026quot;) Install Hugo using the blogdown package helper function:  blogdown::install_hugo() # or library(blogdown) install_hugo() Use the top menu buttons in RStudio to select File -\u0026gt; New Project -\u0026gt; Existing Directory, then browse to the directory on your computer where your GitHub repo is and click on the Create Project button.  Screenshot above: Creating a new project in an existing directory in RStudio\n Now you should be “in” your project in RStudio. If you are using git for version control, edit your *gitignore file. This file should be viewable in your file viewer pane in RStudio. Below is what it should look like: the first four lines will automatically be in this file if you have set up your RStudio Project, but if you plan to use Netlify to deploy, you need to add the public/ line (read about here.)  .Rproj.user .Rhistory .RData .Ruserdata blogdown .DS_Store # if a windows user, Thumbs.db instead public/ # if using Netlify  5 Build your site in RStudio Now you can finally build your site using the blogdown::new_site() function. But first you should at least think about themes…\n5.1 Picking a theme There are over 90 Hugo themes. Here you can find an overview of some of the themes. Whatever theme you choose, you’ll need to pick one of 3 ways to make your new site:\nIf you are happy with the default theme, which is the lithium theme, you can use:  blogdown::new_site() # default theme is lithium If you want a theme other than the default, you can specify the theme at the same time as you call the new_site function:  # for example, create a new site with the academic theme blogdown::new_site(theme = \u0026quot;gcushen/hugo-academic\u0026quot;, theme_example = TRUE) If instead you want to add the theme later, you can do this:  library(blogdown) new_site() # default theme is lithium # need to stop serving so can use the console again install_theme(\u0026quot;gcushen/hugo-academic\u0026quot;, theme_example = TRUE, update_config = TRUE)  Now is a good time to re-read about blogdown::serve_site()and how LiveReloadworks(and how it blocks your R console by default)    5.2 Update project options In your project in RStudio, go to the top menu bar of RStudio and select Tools -\u0026gt; Project Options and update following Yihui and Amber’s instructions.\n 5.3 Edit your configurations Relevant reading:\n blogdown book chapter on configuration You can also view Alison Hill’s config.toml file in GitHub  Now, edit the baseurl in your config.toml file. The URL should always end with a / trailing slash. At this point, you probably haven’t deployed your site yet, so to view it locally you can use the Serve Site add-in, or run the blogdown::serve_site function. Both of these baseurls worked for me when viewing locally:\nbaseurl = \u0026quot;https://example.com/\u0026quot; baseurl = \u0026quot;/\u0026quot;  Make sure that the baseurl =listed ends with a trailing slash /!   Go ahead and edit all the other elements in the config.toml file now as you please- this is how you personalize your site.\n 5.4 Addins \u0026amp; workflow Relevant reading:\n blogdown book chapter on the RStudio IDE  Addins: use them- you won’t need the blogdown library loaded in the console if you use the Addins. The workflow in RStudio at this point (again, just viewing locally because we haven’t deployed yet) works best like this:\nOpen the RStudio project for the site Use the Serve Site add-in (only once due to LiveReload) View site in the RStudio viewer pane, and open in a new browser window while you work Select existing files to edit using the file pane in RStudio After making changes, click the save button (don’t knit!)- the console will reload, the viewer pane will update, and if you hit refresh in the browser your local view will also be updated When happy with changes, add/commit/push changes to GitHub  Having blogdown::serve_site running locally with LiveReload is especially useful as you can immediately see if you have made any mistakes.\nThe above workflow is only for editing existing files or posts, but not for creating new posts. For that, read on…\n 5.5 Posting Relevant reading:\n blogdown book chapter on RStudio IDE blogdown book chapter on output formats: on .md versus .Rmd posts  Bottom line:\nUse the New Post addin. But, you need the console to do this, so you have to stop blogdown::serve_site by clicking on the red Stop button first. The Addin is a Shiny interface that runs this code in your console: blogdown:::new_post_addin(). So, your console needs to be unblocked for it to run. You also need to be “in” your RStudio project or it won’t work.\n5.5.1 Draft posts Relevant reading:\n blogdown book chapter on building a website for local preview  Whether you do a markdown or R Markdown post (see below), you should know that in the YAML front matter of your new file, you can add draft: TRUE and you will be able to preview your post using blogdown::serve_site(), but conveniently your post will not show up on your deployed site until you set it to false. Because this is a function built into Hugo, all posts (draft or not) will still end up in your GitHub repo though.\n 5.5.2 New markdown posts Pick one of 2 methods:\nUse the New Post addin and with the radio button at the bottom select Format: Markdown (recommended) Use the console to author a new .md post:  blogdown::new_post() blogdown::new_post(ext = \u0026#39;.md\u0026#39;) # md is the default! Here are the ?new_post arguments:\nnew_post(title, kind = \u0026quot;\u0026quot;, open = interactive(), author = getOption(\u0026quot;blogdown.author\u0026quot;), categories = NULL, tags = NULL, date = Sys.Date(), file = NULL, slug = NULL, title_case = getOption(\u0026quot;blogdown.title_case\u0026quot;), subdir = getOption(\u0026quot;blogdown.subdir\u0026quot;, \u0026quot;post\u0026quot;), ext = getOption(\u0026quot;blogdown.ext\u0026quot;, \u0026quot;.md\u0026quot;))  Remember to use the Serve Siteaddin again so that you can immediately view your changes with every save using LiveReload.    5.5.3 New R Markdown (.Rmd) posts Again, you have your choice of one of 2 methods:\nUse the New Post addin and with the radio button at the bottom select Format: R Markdown (.Rmd) (recommended) Use the console to author a new .Rmd post:  blogdown::new_post(ext = \u0026#39;.Rmd\u0026#39;) # md is the default! After you edit your .Rmd post, in addition to saving the changes in your .Rmd file, you must use blogdown::serve_site- this is how the output html file needs to be generated.\n Do notknit your .Rmdposts- use blogdown::serve_siteinstead. If you happen to hit the knit button, just Serve Siteagain to rewrite the .htmlfile.   Ultimately, your YAML front matter looks something like this; note that some but not all features of rmarkdown::html_document are supported in blogdown:\n--- title: \u0026quot;My Post\u0026quot; author: \u0026quot;John Doe\u0026quot; date: \u0026quot;2017-02-14\u0026quot; output: blogdown::html_page: toc: true toc_depth: 1 number_sections: true fig_width: 6 ---  Remember to use the Serve Siteaddin again so that you can immediately view your changes with every save using LiveReloadand your .htmlfile is properly output.    5.5.4 Adding images to a post If you want to include an image that is not a figure created from an R chunk, the recommended method is to:\nAdd the image to your /static/img/ folder, then Reference the image using the relative file path as follows:  ![my-image](/img/my-image.png)    6 Deploy in Netlify Deploying in Netlify through GitHub is smooth. Here are some beginner instructions, but Netlify is so easy, I recommend that you skip dragging your public folder in and instead automate the process through GitHub.\nWhen you are ready to deploy, commit your changes and push to GitHub, then go online to Netlify. Click on the Sign Up button and sign up using your existing GitHub account (no need to create another account) Log in, and select: New site from Git -\u0026gt; Continuous Deployment: GitHub. From there, Netlify will allow you to select from your existing GitHub repositories. You’ll pick the repo you’ve been working from with blogdown, then you’ll configure your build. This involves specifying two important things: the build command and the publish directory (this should be public).\n More about the build command from Netlify: “For Hugo hosting, hugo will build and deploy with the version 0.17 of hugo. You can specify a specific hugo release like this: hugo_0.15. Currently 0.13, 0.14, 0.15, 0.16, 0.17, 0.18 and 0.19 are supported. For version 0.20 and above, you’ll need to create a Build environment variable called HUGO_VERSION and set it to the version of your choice.” I opted for the former, and specified hugo_0.19.   You can check your hugo version in terminal using the command hugo version. This is what my output looked like, so I could run version 0.20 if I wanted to through Netlify, but I went with 0.19 and it works just fine.\n$ hugo version Hugo Static Site Generator v0.20.7 darwin/amd64 BuildDate: 2017-05-08T18:37:40-07:00 Screenshot above: Basic build settings in Netlify\n Netlify will deploy your site and assign you a random subdomain name of the form random-word-12345.netlify.com. You should know that you can change this; e.g. to mynewsite.netlify.com.\n Anytime you change your subdomain name, you need to update the baseurlin your config.tomlfile (e.g., baseurl = “https://mynewsite.netlify.com/”).   At this point, you should be up and running with blogdown, GitHub, and Netlify, but here are some ideas if you want to go further…\n 7 Going further 7.1 Custom CSS Every Hugo theme is structured a little differently, but if you are interested, you can check out Alison Hill’s custom css to see how she customized the academic theme, which provides a way to link to a custom CSS file in the config.toml file:\n # Link custom CSS and JS assets # (relative to /static/css and /static/js respectively) custom_css = [\u0026quot;blue.css\u0026quot;]  7.2 Formspree Alison Hill used Formspree to make a contact form, which is an online service (managed on GitHub) that allows you to add an HTML form to your static site. No registration, just use the form and confirm your email address once. She added the following code into the contact widget:\n\u0026lt;form action=\u0026quot;https://formspree.io/your@email.com\u0026quot; method=\u0026quot;POST\u0026quot;\u0026gt; \u0026lt;label for=\u0026quot;name\u0026quot;\u0026gt;Your name: \u0026lt;/label\u0026gt; \u0026lt;input type=\u0026quot;text\u0026quot; name=\u0026quot;name\u0026quot; required=\u0026quot;required\u0026quot; placeholder=\u0026quot;here\u0026quot;\u0026gt;\u0026lt;br\u0026gt; \u0026lt;label for=\u0026quot;email\u0026quot;\u0026gt;Your email: \u0026lt;/label\u0026gt; \u0026lt;input type=\u0026quot;email\u0026quot; name=\u0026quot;_replyto\u0026quot; required=\u0026quot;required\u0026quot; placeholder=\u0026quot;here\u0026quot;\u0026gt;\u0026lt;br\u0026gt; \u0026lt;label for=\u0026quot;message\u0026quot;\u0026gt;Your message:\u0026lt;/label\u0026gt;\u0026lt;br\u0026gt; \u0026lt;textarea rows=\u0026quot;4\u0026quot; name=\u0026quot;message\u0026quot; id=\u0026quot;message\u0026quot; required=\u0026quot;required\u0026quot; class=\u0026quot;form-control\u0026quot; placeholder=\u0026quot;I can\u0026#39;t wait to read this!\u0026quot;\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;input type=\u0026quot;hidden\u0026quot; name=\u0026quot;_next\u0026quot; value=\u0026quot;/html/thanks.html\u0026quot; /\u0026gt; \u0026lt;input type=\u0026quot;submit\u0026quot; value=\u0026quot;Send\u0026quot; name=\u0026quot;submit\u0026quot; class=\u0026quot;btn btn-primary btn-outline\u0026quot;\u0026gt; \u0026lt;input type=\u0026quot;hidden\u0026quot; name=\u0026quot;_subject\u0026quot; value=\u0026quot;Website message\u0026quot; /\u0026gt; \u0026lt;input type=\u0026quot;text\u0026quot; name=\u0026quot;_gotcha\u0026quot; style=\u0026quot;display:none\u0026quot; /\u0026gt; \u0026lt;/form\u0026gt;  7.3 *.rbind.io domain names You may want a different domain name than the one provided by Netlify. Alison opted for a free subdomain *.rbind.io offered by RStudio. To do the same, head over to the rbind/support GitHub page and open a new issue. All you need to do is let them know what your Netlify subdomain name is (*.netlify.com), and what you want your subdomain name to be (*.rbind.io). The rbind support team will help you take it from there!\n Again, you will need to update the baseurlin your config.tomlfile to reflect your new rbind subdomain name (so Alison’s is baseurl = “https://alison.rbind.io/”).   That’s it!\n  ","date":1563580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563580800,"objectID":"7d6af479259dc4296cafa4db445c9d2c","permalink":"/post/2019-07-20-up-and-running-with-blogdown/","publishdate":"2019-07-20T00:00:00Z","relpermalink":"/post/2019-07-20-up-and-running-with-blogdown/","section":"post","summary":"A guide to create and publish a website with RStudio, blogdown, Hugo, GitHub and Netlify","tags":["blogdown"],"title":"Create and publish a Website with R and Hugo","type":"post"},{"authors":["Jan Kirenz"],"categories":["seminar"],"content":"A hands-on data science and data engineering seminar, where students learn how to scrape websites, use API\u0026rsquo;s, use natural language processing (NLP), perform a network analysis and build dashboards with Python and R.\n","date":1557997200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557997200,"objectID":"b8933cf40b9ce90f29d7b3ae5685c037","permalink":"/talk/2019-web-analytics-social-media/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/talk/2019-web-analytics-social-media/","section":"talk","summary":"Webscraping, API's, Network Analysis, NLP, Dashboards","tags":["DataScience"],"title":"Web Analytics \u0026 Social Media Analytics","type":"talk"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"About / Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"Tutorials and resources for data science and data engineering.","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"322dbaccf72a6d71f827fdb2866be935","permalink":"/teaching/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/teaching/","section":"","summary":"Upcoming and recent seminars \u0026 lectures","tags":null,"title":"Seminars \u0026 Lectures","type":"widget_page"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"}]