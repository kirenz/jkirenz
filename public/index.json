[{"authors":["jan"],"categories":null,"content":"I\u0026rsquo;m a Professor at HdM Stuttgart, where I help students and organizations to learn and use data science, statistics, and machine learning with Python and R programming to extract meaningful information from data. Please feel free to contact me if you are interested in data science seminars or data science consulting.\n","date":1571907600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1571907600,"objectID":"2124dc84130bde003e7c6b380610af0e","permalink":"/authors/jan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jan/","section":"authors","summary":"I\u0026rsquo;m a Professor at HdM Stuttgart, where I help students and organizations to learn and use data science, statistics, and machine learning with Python and R programming to extract meaningful information from data. Please feel free to contact me if you are interested in data science seminars or data science consulting.","tags":null,"title":"Jan Kirenz","type":"authors"},{"authors":["jan"],"categories":null,"content":"I\u0026rsquo;m a Professor at HdM Stuttgart, where I help students and organizations to learn and use data science, statistics, and machine learning with Python and R programming to extract meaningful information from data.\n","date":1564617600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1564617600,"objectID":"e23cb71100d51c11123b1b2597de799d","permalink":"/authors/alison/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alison/","section":"authors","summary":"I\u0026rsquo;m a Professor at HdM Stuttgart, where I help students and organizations to learn and use data science, statistics, and machine learning with Python and R programming to extract meaningful information from data.","tags":null,"title":"Jan Kirenz","type":"authors"},{"authors":["Jan Kirenz"],"categories":["Seminar"],"content":"Anhand von mehreren Fallstudien wird zunächst die Extraktion, Bearbeitung und Analyse von Daten in unterschiedlichen Datenbanken mit Hilfe von SQL eingehend behandelt. Der dabei gelernte SQL-Syntax – bspw.\n Datenimport, Verknüpfung von Tabellen, Gruppierung und Summierung, Berechnung statistischer Kennzahlen, - Datenexploration, Analyse von Zeitdaten, Textanalysen und bedingte Ausdrücke (Conditional Expressions)  kann auf eine Vielzahl von Datenbanken wie bspw.\n PostgreSQL, MySQL, Microsoft Azure SQL Datenbank, Google BigQuery und Oracle angewendet werden.  Im Rahmen der Datenanalyse mit R werden neben den zentralen Grundkenntnissen der Programmiersprache R insbesondere Kompetenzen im Umgang mit Datentransformationen („Data Wrangling“), der explorativen Datenanalyse und Visualisierung von Daten (bspw. mit Hilfe eines Dashboards) vermittelt. Zudem wird die Erstellung und Ausgabe (bspw. als HTML, PDF, Word, Excel, PPT,…) von Reports in R markdown behandelt.\n","date":1571907600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571907600,"objectID":"657b65f4bda3414ca3c7b8b97dae5480","permalink":"/talk/2019-programming-languages/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/talk/2019-programming-languages/","section":"talk","summary":"Introduction to SQL and R","tags":["SQL","R"],"title":"Programming Languages for Data Science","type":"talk"},{"authors":["Jan Kirenz"],"categories":["lecture"],"content":"In der Veranstaltung befassen wir uns mit der Erfassung, Analyse und Visualisierung von strukturierten und unstrukturierten Daten - bspw. Daten aus sozialen Medien, Multi-Media-Plattformen, Microblogs, Foren, Webpräsenzen.\nDie Erfassung von Daten auf Webpräsenz wird mit Hilfe von Tracking-Methoden (bspw. mit JavaScript und Tools wie Google Tag Manager, Google Analytics, Matomo) sowie Methoden des Web-Scarping und der Nutzung von Programmierschnittstellen (Application Programming Interface, API) behandelt.\nIm Rahmen der Datenanalyse und Datenvisualisierung (Programmierung von Dashboards) werden insbesondere Open Source-Technologien (insbesondere Python und R) genutzt.\n","date":1571040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571040000,"objectID":"96c98820f8ebaba7884230fae6e9c23b","permalink":"/talk/2019-web-analytics/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/talk/2019-web-analytics/","section":"talk","summary":"Webscraping, API's, Network Analysis, NLP, Dashboards","tags":["DataScience"],"title":"Web Analytics","type":"talk"},{"authors":["Jan Kirenz"],"categories":["lecture"],"content":"\u0026lsquo;In der Veranstaltung Web Analytics \u0026amp; Big Data befassen wir uns mit der Erfassung und Analyse von strukturierten und unstrukturierten (großen) Daten - bspw. Daten aus sozialen Medien, Multi-Media-Plattformen, Microblogs, Foren, Webpräsenzen. Die dadurch gewonnenen Erkenntnisse sollen einen Beitrag zu dem systematischen Aufbau und der Pflege dauerhafter und profitabler Kundenbeziehungen leisten und beziehen sich insbesondere auf die Phasen der Kundengewinnung, Kundenbindung und Vermeidung der Kundenabwanderung innerhalb des Customer Lifecycle Managements.\u0026rsquo;\n","date":1570784400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570784400,"objectID":"a1a67e2a8f607672e1b8511789c96248","permalink":"/talk/2019-big-data-web-analytics/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/talk/2019-big-data-web-analytics/","section":"talk","summary":"Analyse von Big Data mit Python \u0026 R","tags":["DataScience"],"title":"Big Data and Web Analytics","type":"talk"},{"authors":["Jan Kirenz"],"categories":["Seminar"],"content":"","date":1569801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564651201,"objectID":"aa08020547fb1b9f1ceafedee17a5ce8","permalink":"/talk/2019-applied-statistics/","publishdate":"2019-09-30T00:00:00Z","relpermalink":"/talk/2019-applied-statistics/","section":"talk","summary":"Statistical Learning in Python \u0026 R","tags":["Statistics","StatisticalLearning","Python","R"],"title":"Applied Statistics","type":"talk"},{"authors":["Jan Kirenz"],"categories":["Statistics","Python"],"content":"  1 Social Network Analysis with NetworkX in Python 1.1 Social Network Basics 1.1.1 Symmetric Networks (undirected) 1.1.2 Asymmetric Networks (directed) 1.1.3 Weighted Networks  1.2 Clustering coefficient 1.3 Network Distance Measures 1.3.1 Degree 1.3.2 Distance 1.3.3 Breadth-first search 1.3.4 Eccentricity  1.4 Centrality measures 1.4.1 Degree Centrality 1.4.2 Eigenvector Centrality 1.4.3 Closeness Centrality 1.4.4 Betweenness Centrality  1.5 Facebook Case Study    1 Social Network Analysis with NetworkX in Python We use the module NetworkX in this tutorial. It is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\nIf you work with Anaconda, you can install the package as follows:\nconda install -c anaconda networkx Import modules:\nimport networkx as nx import matplotlib.pyplot as plt %matplotlib inline import warnings; warnings.simplefilter(\u0026#39;ignore\u0026#39;) 1.1 Social Network Basics Each network consists of:\n Nodes: The individuals whose network we are building. Edges: The connection between the nodes. It represents a relationship between the nodes of the network.  1.1.1 Symmetric Networks (undirected) The first network that we create is a group of people who work together. This is called a symmetric network because the relationship “working together” is a symmetric relationship: If A is related to B, B is also related to A.\nG_symmetric = nx.Graph() G_symmetric.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;Laura\u0026#39;) G_symmetric.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;Marc\u0026#39;) G_symmetric.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;John\u0026#39;) G_symmetric.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;Michelle\u0026#39;) G_symmetric.add_edge(\u0026#39;Laura\u0026#39;, \u0026#39;Michelle\u0026#39;) G_symmetric.add_edge(\u0026#39;Michelle\u0026#39;,\u0026#39;Marc\u0026#39;) G_symmetric.add_edge(\u0026#39;George\u0026#39;, \u0026#39;John\u0026#39;) G_symmetric.add_edge(\u0026#39;George\u0026#39;, \u0026#39;Steven\u0026#39;) print(nx.info(G_symmetric)) Name: Type: Graph Number of nodes: 6 Number of edges: 8 Average degree: 2.6667 Now we visualize the network with the draw_networkx() function.\nplt.figure(figsize=(5,5)) nx.draw_networkx(G_symmetric);  1.1.2 Asymmetric Networks (directed) What if the relationship between nodes is ‘child of’, then the relationship is no longer symmetric. This is the case if someone follows someone else on Twitter. Or in the case of hyperlinks.\nIf A is the child of B, then B is not a child of A. Such a network where the relationship is asymmetric (A is related to B, does not necessarily means that B is associated with A) is called an Asymmetric network.\nWe can build the asymmetric network in NetworkX using DiGraph method, which is short of Directional Graph.\nG_asymmetric = nx.DiGraph() G_asymmetric.add_edge(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;) G_asymmetric.add_edge(\u0026#39;A\u0026#39;,\u0026#39;D\u0026#39;) G_asymmetric.add_edge(\u0026#39;C\u0026#39;,\u0026#39;A\u0026#39;) G_asymmetric.add_edge(\u0026#39;D\u0026#39;,\u0026#39;E\u0026#39;) To make sure that all nodes are distinctly visible in the network, use the spring_layout() function, followed by the draw_networkx() function.\nnx.spring_layout(G_asymmetric) nx.draw_networkx(G_asymmetric)  1.1.3 Weighted Networks Till now we had networks without weights, but it is possible that networks are made with weights, for example, if in our initial network we consider the number of projects done together as a weight, we will get a weighted Network.\nLet us make one again of the employees, but this time we add weight to the network, each edge has a weight signifying the number of projects they have done together.\nG_weighted = nx.Graph() G_weighted.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;Laura\u0026#39;, weight=25) G_weighted.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;Marc\u0026#39;, weight=8) G_weighted.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;John\u0026#39;, weight=11) G_weighted.add_edge(\u0026#39;Steven\u0026#39;, \u0026#39;Michelle\u0026#39;,weight=1) G_weighted.add_edge(\u0026#39;Laura\u0026#39;, \u0026#39;Michelle\u0026#39;,weight=1) G_weighted.add_edge(\u0026#39;Michelle\u0026#39;,\u0026#39;Marc\u0026#39;, weight=1) G_weighted.add_edge(\u0026#39;George\u0026#39;, \u0026#39;John\u0026#39;, weight=8) G_weighted.add_edge(\u0026#39;George\u0026#39;, \u0026#39;Steven\u0026#39;, weight=4) elarge = [(u, v) for (u, v, d) in G_weighted.edges(data=True) if d[\u0026#39;weight\u0026#39;] \u0026gt; 8] esmall = [(u, v) for (u, v, d) in G_weighted.edges(data=True) if d[\u0026#39;weight\u0026#39;] \u0026lt;= 8] pos = nx.circular_layout(G_weighted) # positions for all nodes # nodes nx.draw_networkx_nodes(G_weighted, pos, node_size=700) # edges nx.draw_networkx_edges(G_weighted, pos, edgelist=elarge,width=6) nx.draw_networkx_edges(G_weighted, pos, edgelist=esmall,width=6, alpha=0.5, edge_color=\u0026#39;b\u0026#39;, style=\u0026#39;dashed\u0026#39;) # labels nx.draw_networkx_labels(G_weighted, pos, font_size=20, font_family=\u0026#39;sans-serif\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show();    1.2 Clustering coefficient It is observed that people who share connections in a social network tend to form associations. In other words, there is a tendency in a social network to form clusters.\nWe can determine the clusters of a node, local clustering coefficient, which is the fraction of pairs of the node’s friends (that is connections) that are connected with each other.\nTo determine the local clustering coefficient, we make use of nx.clustering(Graph, Node) function.\nIn the symmetric employee-network, you will find that Michelle has a local clustering coefficient of 0.67 and Laura has a local clustering coefficient of 1.\nThe average clustering coefficient (sum of all the local clustering coefficients divided by the number of nodes) for the symmetric employee-network is 0.867.\nnx.clustering(G_symmetric,\u0026#39;Michelle\u0026#39;) 0.6666666666666666 nx.clustering(G_symmetric,\u0026#39;Laura\u0026#39;) 1.0 nx.average_clustering(G_symmetric) 0.8277777777777778  1.3 Network Distance Measures 1.3.1 Degree Degree of a node defines the number of connections a node has. NetworkX has the function degree which we can use to determine the degree of a node in the network.\nnx.degree(G_symmetric, \u0026#39;Michelle\u0026#39;) 3 This will return a value of 3, as Michelle has worked with three employees in the network.\n 1.3.2 Distance We can also determine the shortest path between two nodes and its length in NetworkX using nx.shortest_path(Graph, Node1, Node2) and nx.shortest_path_length(Graph, Node1, Node2) functions respectively.\nnx.shortest_path(G_symmetric, \u0026#39;Michelle\u0026#39;, \u0026#39;John\u0026#39;) [\u0026#39;Michelle\u0026#39;, \u0026#39;Steven\u0026#39;, \u0026#39;John\u0026#39;] nx.shortest_path_length(G_symmetric, \u0026#39;Michelle\u0026#39;, \u0026#39;John\u0026#39;) 2  1.3.3 Breadth-first search We can find the distance of a node from every other node in the network using breadth-first search algorithm, starting from that node. networkX provides the function bfs_tree to do it.\nAnd so if you use M = nx.bfs_tree(G_symmetric, 'Michelle') and now draw this tree, we will get a network structure telling how we can reach other nodes of the network starting from Michelle .\nS = nx.bfs_tree(G_symmetric, \u0026#39;Steven\u0026#39;) nx.draw_networkx(S) M = nx.bfs_tree(G_symmetric, \u0026#39;Michelle\u0026#39;) nx.draw_networkx(M)  1.3.4 Eccentricity Eccentricity of a node A is defined as the largest distance between A and all other nodes.\nIt can be found using nx.eccentricity() function. In the symmetric employee-network, Michelle has an eccentricity of 2, and Steven has an eccentricity of 1 (he is connected to every other node).\nnx.eccentricity(G_symmetric,\u0026#39;Michelle\u0026#39;) 2 nx.eccentricity(G_symmetric,\u0026#39;Steven\u0026#39;) 1   1.4 Centrality measures Above we learned some of the network distance measures and they are useful in knowing how the information will spread through the network.\nIn this section, we will learn how to find the most important nodes (individuals) in the network. These parameters are called as centrality measures. Centrality Measures can help us in identifying popularity, most liked, and biggest influencers within the network.\n1.4.1 Degree Centrality The people most popular or more liked usually are the ones who have more friends.\nDegree centrality is a measure of the number of connections a particular node has in the network. It is based on the fact that important nodes have many connections. NetworkX has the function degree_centrality() to calculate the degree centrality of all the nodes of a network.\nnx.degree_centrality(G_symmetric) {\u0026#39;Steven\u0026#39;: 1.0, \u0026#39;Laura\u0026#39;: 0.4, \u0026#39;Marc\u0026#39;: 0.4, \u0026#39;John\u0026#39;: 0.4, \u0026#39;Michelle\u0026#39;: 0.6000000000000001, \u0026#39;George\u0026#39;: 0.4}  1.4.2 Eigenvector Centrality It is not just how many individuals one is connected too, but the type of people one is connected with that can decide the importance of a node.\nEigenvector centrality is a measure of how import a node is by accounting for the fact of how well it is connected to other important nodes.\nWe can use the eigenvector_centrality() function of NetworkX to calculate eigenvector centrality of all the nodes in a network.\nThe Google’s Pagerank algorithm is a variant of Eigenvector centrality algorithm.\nnx.eigenvector_centrality(G_symmetric) {\u0026#39;Steven\u0026#39;: 0.6006686104947806, \u0026#39;Laura\u0026#39;: 0.3545677660798074, \u0026#39;Marc\u0026#39;: 0.3545677660798074, \u0026#39;John\u0026#39;: 0.30844592433424667, \u0026#39;Michelle\u0026#39;: 0.4443904166426225, \u0026#39;George\u0026#39;: 0.30844592433424667}  1.4.3 Closeness Centrality Closeness Centrality is a measure where each node’s importance is determined by closeness to all other nodes.\nnx.closeness_centrality(G_symmetric) {\u0026#39;Steven\u0026#39;: 1.0, \u0026#39;Laura\u0026#39;: 0.625, \u0026#39;Marc\u0026#39;: 0.625, \u0026#39;John\u0026#39;: 0.625, \u0026#39;Michelle\u0026#39;: 0.7142857142857143, \u0026#39;George\u0026#39;: 0.625}  1.4.4 Betweenness Centrality The Betweenness Centrality is the centrality of control.\nIt represents the frequency at which a point occurs on the shortest paths that connected pair of points. It quantifies how many times a particular node comes in the shortest chosen path between two other nodes.\nThe nodes with high betweenness centrality play a significant role in the communication/information flow within the network.\nThe nodes with high betweenness centrality can have a strategic control and influence on others. An individual at such a strategic position can influence the whole group, by either withholding or coloring the information in transmission.\nNetworkx has the function betweenness_centrality() to measure it for the network. It has options to select if we want betweenness values to be normalized or not, weights to be included in centrality calculation or not, and to include the endpoints in the shortest path counts or not.\nnx.betweenness_centrality(G_symmetric) {\u0026#39;Steven\u0026#39;: 0.65, \u0026#39;Laura\u0026#39;: 0.0, \u0026#39;Marc\u0026#39;: 0.0, \u0026#39;John\u0026#39;: 0.0, \u0026#39;Michelle\u0026#39;: 0.05, \u0026#39;George\u0026#39;: 0.0} pos = nx.spring_layout(G_symmetric) betCent = nx.betweenness_centrality(G_symmetric, normalized=True, endpoints=True) node_color = [20000.0 * G_symmetric.degree(v) for v in G_symmetric] node_size = [v * 10000 for v in betCent.values()] plt.figure(figsize=(10,10)) nx.draw_networkx(G_symmetric, pos=pos, with_labels=True, node_color=node_color, node_size=node_size ) plt.axis(\u0026#39;off\u0026#39;); sorted(betCent, key=betCent.get, reverse=True)[:5] [\u0026#39;Steven\u0026#39;, \u0026#39;Michelle\u0026#39;, \u0026#39;Laura\u0026#39;, \u0026#39;Marc\u0026#39;, \u0026#39;John\u0026#39;]   1.5 Facebook Case Study This dataset consists of ‘circles’ (or ‘friends lists’) from Facebook. Facebook data was collected from survey participants using this Facebook app. The dataset includes node features (profiles), circles, and ego networks.\nFacebook data has been anonymized by replacing the Facebook-internal ids for each user with a new value. Also, while feature vectors from this dataset have been provided, the interpretation of those features has been obscured. For instance, where the original dataset may have contained a feature “political=Democratic Party”, the new data would simply contain “political=anonymized feature 1”. Thus, using the anonymized data it is possible to determine whether two users have the same political affiliations, but not what their individual political affiliations represent.\nSource: J. McAuley and J. Leskovec. Learning to Discover Social Circles in Ego Networks. NIPS, 2012\nLet us start with the Facebook data, for our analysis here we will use Facebook combined ego networks dataset, it contains the aggregated network of ten individuals’ Facebook friends list. You can download the required facebook_combined.txt file from the Stanford University site.\nWe read in the file and construct the Graph:\nDownload the file\nimport pandas as pd df = pd.read_csv(\u0026#39;/Users/jankirenz/Dropbox/Data/facebook_combined.txt\u0026#39;) df.info() \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 88233 entries, 0 to 88232 Data columns (total 1 columns): 0 1 88233 non-null object dtypes: object(1) memory usage: 689.4+ KB df.tail()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }      0 1       88228   4026 4030     88229   4027 4031     88230   4027 4032     88231   4027 4038     88232   4031 4038      G_fb = nx.read_edgelist(\u0026quot;/Users/jankirenz/Dropbox/Data/facebook_combined.txt\u0026quot;, create_using = nx.Graph(), nodetype=int) print(nx.info(G_fb)) Name: Type: Graph Number of nodes: 4039 Number of edges: 88234 Average degree: 43.6910\nThe network consists of 4,039 nodes, connected via 88,234 edges.\nplt.figure(figsize=(20,20)) nx.draw_networkx(G_fb); We can also visualize the network such that the node color varies with Degree and node size with Betweenness Centrality. The code to do this is:\npos = nx.spring_layout(G_fb) betCent = nx.betweenness_centrality(G_fb, normalized=True, endpoints=True) node_color = [20000.0 * G_fb.degree(v) for v in G_fb] node_size = [v * 10000 for v in betCent.values()] plt.figure(figsize=(20,20)) nx.draw_networkx(G_fb, pos=pos, with_labels=False, node_color=node_color, node_size=node_size ) plt.axis(\u0026#39;off\u0026#39;); You can also know the labels of the nodes with the highest betweenness centrality using:\nsorted(betCent, key=betCent.get, reverse=True)[:5] We can see that some nodes are common between Degree Centrality, which is a measure of degree, and Betweenness Centrality which controls the information flow.\nIt is natural that nodes that are more connected also lie on shortest paths between other nodes. The node 1912 is an important node as it is crucial according to all three centrality measures that we had considered.\nSources of examples:\n Datacamp; Aksakalli, C., McAuley, J. \u0026amp; Leskovec, J.    ","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"2393e2c84e8a07f97594f6d229d5f607","permalink":"/post/2019-08-13-network_analysis/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/post/2019-08-13-network_analysis/","section":"post","summary":"Introduction to Social Network Analysis with NetworkX","tags":["Statistics"],"title":"Social Network Analysis with Python","type":"post"},{"authors":null,"categories":null,"content":" NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.\nMaterial:\n Introduction to network presentation slides Network analysis tasks with code templates  ","date":1565604000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565604000,"objectID":"38f963ba80811eb47af257387e025405","permalink":"/project/python-network-analysis-intro/","publishdate":"2019-08-12T10:00:00Z","relpermalink":"/project/python-network-analysis-intro/","section":"project","summary":"Network Analysis with the Python module NetworkX","tags":["Python","NetworkAnalysis"],"title":"Introduction to Network Analysis with Python","type":"project"},{"authors":["Jan Kirenz"],"categories":["Python","Statistics","Regression"],"content":"  1 Lasso Regression Basics 2 Implementation of Lasso regression 2.1 Standardization 2.2 Split data 2.3 Lasso regression 2.4 Lasso with different lambdas 2.5 Plot values as a function of lambda 2.6 Identify best lambda and coefficients 2.7 Cross Validation 2.8 Best Model    1 Lasso Regression Basics Lasso performs a so called L1 regularization (a process of introducing additional information in order to prevent overfitting), i.e. adds penalty equivalent to absolute value of the magnitude of coefficients.\nIn particular, the minimization objective does not only include the residual sum of squares (RSS) - like in the OLS regression setting - but also the sum of the absolute value of coefficients.\nThe residual sum of squares (RSS) is calculated as follows:\n\\[ RSS = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 \\]\nThis formula can be stated as:\n\\[ RSS = \\sum_{i=1}^{n} \\bigg(y_i - \\big( \\beta_{0} + \\sum_{j=1}^{p} \\beta_{j} x_{ij} \\big) \\bigg)^2 \\]\n n represents the number of distinct data points, or observations, in our sample. p denotes the number of variables that are available in the dataset. x_{ij} represents the value of the jth variable for the ith observation, where i = 1, 2, . . ., n and j = 1, 2, . . . , p.  In the lasso regression, the minimization objective becomes:\n\\[ \\sum_{i=1}^{n} \\bigg(y_i - \\big( \\beta_{0} + \\sum_{j=1}^{p} \\beta_{j} x_{ij} \\big) \\bigg)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\]\nwhich equals:\n\\[RSS + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\]\n\\(\\lambda\\) (lambda) provides a trade-off between balancing RSS and magnitude of coefficients.\n\\(\\lambda\\) can take various values:\n \\(\\lambda\\) = 0: Same coefficients as simple linear regression \\(\\lambda\\) = ∞: All coefficients zero (same logic as before) 0 \u0026lt; \\(\\lambda\\) \u0026lt; ∞: coefficients between 0 and that of simple linear regression   2 Implementation of Lasso regression Python set up:\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline plt.style.use(\u0026#39;ggplot\u0026#39;) import warnings; warnings.simplefilter(\u0026#39;ignore\u0026#39;) This notebook involves the use of the Lasso regression on the “Auto” dataset. In particular, we only use observations 1 to 200 for our analysis. Furthermore, you can drop the name variable.\nImport data:\ndf = pd.read_csv(\u0026quot;https://raw.githubusercontent.com/kirenz/datasets/master/Auto.csv\u0026quot;) Tidying data:\ndf = df.iloc[0:200] df = df.drop([\u0026#39;name\u0026#39;], axis=1) df.info() \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 200 entries, 0 to 199 Data columns (total 8 columns): mpg 200 non-null float64 cylinders 200 non-null int64 displacement 200 non-null float64 horsepower 200 non-null object weight 200 non-null int64 acceleration 200 non-null float64 year 200 non-null int64 origin 200 non-null int64 dtypes: float64(3), int64(4), object(1) memory usage: 12.6+ KB df[\u0026#39;origin\u0026#39;] = pd.Categorical(df[\u0026#39;origin\u0026#39;]) df[\u0026#39;horsepower\u0026#39;] = pd.to_numeric(df[\u0026#39;horsepower\u0026#39;], errors=\u0026#39;coerce\u0026#39;) print(df.isnull().sum()) mpg 0 cylinders 0 displacement 0 horsepower 2 weight 0 acceleration 0 year 0 origin 0 dtype: int64 # drop missing cases df = df.dropna() We use scikit learn to fit a Lasso regression (see documentation) and follow a number of steps (note that scikit-learn uses \\(\\alpha\\) instead of \\(\\lambda\\) in their notation):\n2.1 Standardization Standardize the features with the module: from sklearn.preprocessing import StandardScaler\nIt is important to standardize the features by removing the mean and scaling to unit variance. The L1 (Lasso) and L2 (Ridge) regularizers of linear models assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\ndfs = df.astype(\u0026#39;int\u0026#39;) dfs.info() \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; Int64Index: 198 entries, 0 to 199 Data columns (total 8 columns): mpg 198 non-null int64 cylinders 198 non-null int64 displacement 198 non-null int64 horsepower 198 non-null int64 weight 198 non-null int64 acceleration 198 non-null int64 year 198 non-null int64 origin 198 non-null int64 dtypes: int64(8) memory usage: 13.9 KB dfs.columns Index([\u0026#39;mpg\u0026#39;, \u0026#39;cylinders\u0026#39;, \u0026#39;displacement\u0026#39;, \u0026#39;horsepower\u0026#39;, \u0026#39;weight\u0026#39;, \u0026#39;acceleration\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;origin\u0026#39;], dtype=\u0026#39;object\u0026#39;) from sklearn.preprocessing import StandardScaler scaler = StandardScaler() dfs[[\u0026#39;cylinders\u0026#39;, \u0026#39;displacement\u0026#39;, \u0026#39;horsepower\u0026#39;, \u0026#39;weight\u0026#39;, \u0026#39;acceleration\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;origin\u0026#39;]] = scaler.fit_transform(dfs[[\u0026#39;cylinders\u0026#39;, \u0026#39;displacement\u0026#39;, \u0026#39;horsepower\u0026#39;, \u0026#39;weight\u0026#39;, \u0026#39;acceleration\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;origin\u0026#39;]]) dfs.head(5)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }      mpg   cylinders   displacement   horsepower   weight   acceleration   year   origin       0   18   1.179744   0.726091   0.325216   0.346138   -0.955578   -1.516818   -0.629372     1   15   1.179744   1.100254   1.129264   0.548389   -1.305309   -1.516818   -0.629372     2   18   1.179744   0.821807   0.784672   0.273370   -1.305309   -1.516818   -0.629372     3   16   1.179744   0.699986   0.784672   0.270160   -0.955578   -1.516818   -0.629372     4   17   1.179744   0.682583   0.554944   0.287282   -1.655041   -1.516818   -0.629372       2.2 Split data Split the data set into train and test sets (use X_train, X_test, y_train, y_test), with the first 75% of the data for training and the remaining for testing. (module: from sklearn.model_selection import train_test_split)\nX = dfs.drop([\u0026#39;mpg\u0026#39;], axis=1) y = dfs[\u0026#39;mpg\u0026#39;] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)  2.3 Lasso regression Apply Lasso regression on the training set with the regularization parameter lambda = 0.5 (module: from sklearn.linear_model import Lasso) and print the \\(R^2\\)-score for the training and test set. Comment on your findings.\nfrom sklearn.linear_model import Lasso reg = Lasso(alpha=0.5) reg.fit(X_train, y_train) Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=‘cyclic’, tol=0.0001, warm_start=False)\nprint(\u0026#39;Lasso Regression: R^2 score on training set\u0026#39;, reg.score(X_train, y_train)*100) print(\u0026#39;Lasso Regression: R^2 score on test set\u0026#39;, reg.score(X_test, y_test)*100) Lasso Regression: R^2 score on training set 82.49741060950073 Lasso Regression: R^2 score on test set 85.49734440925533\n 2.4 Lasso with different lambdas Apply the Lasso regression on the training set with the following λ parameters: (0.001, 0.01, 0.1, 0.5, 1, 2, 10). Evaluate the R^2 score for all the models you obtain on both the train and test sets.\nlambdas = (0.001, 0.01, 0.1, 0.5, 1, 2, 10) l_num = 7 pred_num = X.shape[1] # prepare data for enumerate coeff_a = np.zeros((l_num, pred_num)) train_r_squared = np.zeros(l_num) test_r_squared = np.zeros(l_num) # enumerate through lambdas with index and i for ind, i in enumerate(lambdas): reg = Lasso(alpha = i) reg.fit(X_train, y_train) coeff_a[ind,:] = reg.coef_ train_r_squared[ind] = reg.score(X_train, y_train) test_r_squared[ind] = reg.score(X_test, y_test)  2.5 Plot values as a function of lambda Plot all values for both data sets (train and test \\(R^2\\)-values) as a function of λ. Comment on your findings.\n# Plotting plt.figure(figsize=(18, 8)) plt.plot(train_r_squared, \u0026#39;bo-\u0026#39;, label=r\u0026#39;$R^2$ Training set\u0026#39;, color=\u0026quot;darkblue\u0026quot;, alpha=0.6, linewidth=3) plt.plot(test_r_squared, \u0026#39;bo-\u0026#39;, label=r\u0026#39;$R^2$ Test set\u0026#39;, color=\u0026quot;darkred\u0026quot;, alpha=0.6, linewidth=3) plt.xlabel(\u0026#39;Lamda index\u0026#39;); plt.ylabel(r\u0026#39;$R^2$\u0026#39;) plt.xlim(0, 6) plt.title(r\u0026#39;Evaluate lasso regression with lamdas: 0 = 0.001, 1= 0.01, 2 = 0.1, 3 = 0.5, 4= 1, 5= 2, 6 = 10\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.grid()  2.6 Identify best lambda and coefficients Store your test data results in a DataFrame and indentify the lambda where the \\(R^2\\) has it’s maximum value in the test data. Fit a Lasso model with this lambda parameter (use the training data) and obtain the corresponding regression coefficients. Furthermore, obtain the mean squared error for the test data of this model (module: from sklearn.metrics import mean_squared_error)\ndf_lam = pd.DataFrame(test_r_squared*100, columns=[\u0026#39;R_squared\u0026#39;]) df_lam[\u0026#39;lambda\u0026#39;] = (lambdas) # returns the index of the row where column has maximum value. df_lam.loc[df_lam[\u0026#39;R_squared\u0026#39;].idxmax()] R_squared 88.105773 lambda 0.001000 Name: 0, dtype: float64\n# Coefficients of best model reg_best = Lasso(alpha = 0.1) reg_best.fit(X_train, y_train) reg_best.coef_ array([-0.35554113, -1.13104696, -0.00596296, -3.31741775, -0. , 0.37914648, 0.74902885])\nfrom sklearn.metrics import mean_squared_error mean_squared_error(y_test, reg_best.predict(X_test)) 3.586249592807347\n 2.7 Cross Validation Evaluate the performance of a Lasso regression for different regularization parameters λ using 5-fold cross validation on the training set (module: from sklearn.model_selection import cross_val_score) and plot the cross-validation (CV) \\(R^2\\) scores of the training and test data as a function of λ.\nUse the following lambda parameters: l_min = 0.05 l_max = 0.2 l_num = 20 lambdas = np.linspace(l_min,l_max, l_num)\nl_min = 0.05 l_max = 0.2 l_num = 20 lambdas = np.linspace(l_min,l_max, l_num) train_r_squared = np.zeros(l_num) test_r_squared = np.zeros(l_num) pred_num = X.shape[1] coeff_a = np.zeros((l_num, pred_num)) from sklearn.model_selection import cross_val_score for ind, i in enumerate(lambdas): reg = Lasso(alpha = i) reg.fit(X_train, y_train) results = cross_val_score(reg, X, y, cv=5, scoring=\u0026quot;r2\u0026quot;) train_r_squared[ind] = reg.score(X_train, y_train) test_r_squared[ind] = reg.score(X_test, y_test) # Plotting plt.figure(figsize=(18, 8)) plt.plot(train_r_squared, \u0026#39;bo-\u0026#39;, label=r\u0026#39;$R^2$ Training set\u0026#39;, color=\u0026quot;darkblue\u0026quot;, alpha=0.6, linewidth=3) plt.plot(test_r_squared, \u0026#39;bo-\u0026#39;, label=r\u0026#39;$R^2$ Test set\u0026#39;, color=\u0026quot;darkred\u0026quot;, alpha=0.6, linewidth=3) plt.xlabel(\u0026#39;Lamda value\u0026#39;); plt.ylabel(r\u0026#39;$R^2$\u0026#39;) plt.xlim(0, 19) plt.title(r\u0026#39;Evaluate 5-fold cv with different lamdas\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.grid()  2.8 Best Model Finally, store your test data results in a DataFrame and identify the lambda where the \\(R^2\\) has it’s maximum value in the test data. Fit a Lasso model with this lambda parameter (use the training data) and obtain the corresponding regression coefficients. Furthermore, obtain the mean squared error for the test data of this model (module: from sklearn.metrics import mean_squared_error)\ndf_lam = pd.DataFrame(test_r_squared*100, columns=[\u0026#39;R_squared\u0026#39;]) df_lam[\u0026#39;lambda\u0026#39;] = (lambdas) # returns the index of the row where column has maximum value. df_lam.loc[df_lam[\u0026#39;R_squared\u0026#39;].idxmax()] R_squared 87.897525 lambda 0.050000 Name: 0, dtype: float64\n# Best Model reg_best = Lasso(alpha = 0.144737) reg_best.fit(X_train, y_train) Lasso(alpha=0.144737, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=‘cyclic’, tol=0.0001, warm_start=False)\nfrom sklearn.metrics import mean_squared_error mean_squared_error(y_test, reg_best.predict(X_test)) 3.635187490993961\nreg_best.coef_ array([-0.34136411, -1.18223273, -0. , -3.27132984, 0. , 0.33262331, 0.71385488])\n  ","date":1565568000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565568000,"objectID":"ac43ad8d2960580a15c95be8929481ae","permalink":"/post/2019-08-12-python-lasso-regression-auto/","publishdate":"2019-08-12T00:00:00Z","relpermalink":"/post/2019-08-12-python-lasso-regression-auto/","section":"post","summary":"Implementation of Lasso Regression in Python","tags":["Statistics"],"title":"Lasso Regression with Python","type":"post"},{"authors":["Jan Kirenz"],"categories":["tutorial"],"content":"Correlation is a way of measuring the extent to which two variables are related. This means we need to analyze whether as one variable increases, the other\n (1) increases, (2) decreases or (3) stays the same.  This can be done by calculating the covariance or correlation of two variables.\nIn this Correlation Tutorial in R, we use a small dataset to illustrate the concepts of covariance and correlation. You may also download the Rmarkdown file and open it in RStudio.\nCheck your understanding with multiple choice tasks\n","date":1565499600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565499600,"objectID":"7352002437c59ef78e85aacacec73f5f","permalink":"/project/r-correlation-tutorial/","publishdate":"2019-08-11T05:00:00Z","relpermalink":"/project/r-correlation-tutorial/","section":"project","summary":"Introduction to correlation analysis with R.","tags":["Statistics","R","DataExploration"],"title":"Correlation Tutorial with R","type":"project"},{"authors":["Jan Kirenz"],"categories":["tutorial"],"content":" Lasso Regression In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces (Wikipedia).\nLasso Regression with Python (Auto Data): Jupyter Notebook\n","date":1565481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565481600,"objectID":"f470393a44e6222966c47805a878add9","permalink":"/project/r-lasso-regression/","publishdate":"2019-08-11T00:00:00Z","relpermalink":"/project/r-lasso-regression/","section":"project","summary":"Introduction to Lasso Regression with Python.","tags":["Statistics","Python","Regression"],"title":"Lasso Regression with Python","type":"project"},{"authors":["Jan Kirenz"],"categories":["tutorial"],"content":" Time Series Analysis with Python Time series analysis can be used in a multitude of business applications for forecasting a quantity into the future and explaining its historical patterns. Here are just a few examples of possible use cases:\n Explaining seasonal patterns in sales Predicting the expected number of incoming or churning customers Estimating the effect of a newly launched product on number of sold units  Introduction to Time Series Analysis with Python: Fit ARIMA and SARIMAX-Models with Statsmodel: Jupyter Notebook\nIntroduction to Facebook\u0026rsquo;s time series analysis modul Prophet: Jupyter Notebook\n","date":1565481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565481600,"objectID":"cdb80853bd478e55954960b7ce45945f","permalink":"/project/python-time-series/","publishdate":"2019-08-11T00:00:00Z","relpermalink":"/project/python-time-series/","section":"project","summary":"Introduction to ARIMA, SARIMAX \u0026 Facebook's Prophet","tags":["Statistics","Python","TimeSeries"],"title":"Time Series Analysis with Python","type":"project"},{"authors":["Jan Kirenz"],"categories":["Statistics","R","German"],"content":"  1 Deskriptive Statistik in R 1.1 Datenimport 1.2 Deskriptive Statistiken 1.2.1 Mittelwert 1.2.2 Standardabweichung 1.2.3 Getrimmter Mittelwert 1.2.4 Schiefe 1.2.5 Kurtosis 1.2.6 Standardfehler     1 Deskriptive Statistik in R In diesem Beitrag wird die Berechnung einfacher deskriptiver Statistiken und die Visualisierung von Verteilungen in R am Beispiel des Datensatzes “Advertising” behandelt.\n1.1 Datenimport  Datensatz: Advertising.csv Variablen: TV, radio, newspaper = jeweils Werbeausgaben in Dollar; sales = Produkte in Tausend Einheiten Abhängige Variable (dependent variable, response): sales Unabhängige Variablen (independent variables, predictors): TV, radio, newspaper, sales  Zunächts möchten wir uns einen Überblick über die Daten verschaffen. Dafür importieren wir die Daten und prüfen, ob die Skalenniveaus korrekt sind. Für die weiteren Berechnungen wird die Variable X1 nicht benötigt, weshalb wir diese löschen.\nlibrary(tidyverse) # Daten importieren Advertising \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/kirenz/datasets/master/advertising.csv\u0026quot;) # Überblick über die Daten verschaffen (Skalenniveaus prüfen) head(Advertising) ## # A tibble: 6 x 5 ## X1 TV radio newspaper sales ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 230. 37.8 69.2 22.1 ## 2 2 44.5 39.3 45.1 10.4 ## 3 3 17.2 45.9 69.3 9.3 ## 4 4 152. 41.3 58.5 18.5 ## 5 5 181. 10.8 58.4 12.9 ## 6 6 8.7 48.9 75 7.2 # Bereinigung der Daten Advertising$X1 \u0026lt;- NULL  1.2 Deskriptive Statistiken Ausgabe unterschiedlicher deskriptiver Statistiken:\nlibrary(psych) psych::describe(Advertising)  ## vars n mean sd median trimmed mad min max range ## TV 1 200 147.04 85.85 149.75 147.20 108.82 0.7 296.4 295.7 ## radio 2 200 23.26 14.85 22.90 23.00 19.79 0.0 49.6 49.6 ## newspaper 3 200 30.55 21.78 25.75 28.41 23.13 0.3 114.0 113.7 ## sales 4 200 14.02 5.22 12.90 13.78 4.82 1.6 27.0 25.4 ## skew kurtosis se ## TV -0.07 -1.24 6.07 ## radio 0.09 -1.28 1.05 ## newspaper 0.88 0.57 1.54 ## sales 0.40 -0.45 0.37  Hinweise zu den Kennzahlen:  vars: Nummer der Variable n: Anzahl der Beobachtungen mean: arithmetischer Mittelwert sd: empirische Standardabweichung median: Median trimmed: getrimmter Mittelwert mad: Mittlere absolute Abweichung vom Median min: kleinster Beobachtungswert max: größter Beobachtungswert range: Spannweite skew: Schiefe kurtosis: Wölbung se = Standardfehler   1.2.1 Mittelwert Bei der Berechnung des arithmetischen Mittelwerts in R sollte immer die Anweisung gegeben werden, fehlende Werte auszuschließen (na.rm = “remove values which are not available”). Ansonsten stoppt R bei fehlenden Werten die Berechnung und gibt eine Fehlermeldung aus.\nmean_sales \u0026lt;- mean(Advertising$sales, na.rm = TRUE) print(paste0(\u0026quot;Mittelwert der Variable Sales: \u0026quot;, mean_sales)) ## [1] \u0026quot;Mittelwert der Variable Sales: 14.0225\u0026quot;  1.2.2 Standardabweichung Die Standardabweichung ist ein häufig verwendetes Streuungsmaß und beschreibt die mittlere Abweichung der einzelnen Messwerte vom empirischen Mittelwert. Die Standardabweichung ist die positive Wurzel der empirischen Varianz. Die Varianz einer Stichprobe wird wie folgt berechnet: \\[s^{2} = \\frac{\\sum_{i=1}^{n} \\left(x_{i} - \\bar{x}\\right)^{2}} {n-1}\\]\nBerechnung der Standardabweichung: \\[s = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n} \\left(x_{i} - \\bar{x}\\right)^{2}} {n-1}}\\]\nvar_sales \u0026lt;- var(Advertising$sales, na.rm = TRUE) print(paste0(\u0026quot;Varianz der Variable Sales: \u0026quot;, round(var_sales, 2))) ## [1] \u0026quot;Varianz der Variable Sales: 27.22\u0026quot; sd_sales \u0026lt;- sd(Advertising$sales, na.rm = TRUE) print(paste0(\u0026quot;Standardabweichung der Variable Sales: \u0026quot;, round(sd_sales,2))) ## [1] \u0026quot;Standardabweichung der Variable Sales: 5.22\u0026quot;  1.2.3 Getrimmter Mittelwert Bei dem getrimmten Mittelwert wird ein bestimmer Anteil der größten und kleinsten Beobachtungen - hier oberhalb des 90% Quantils und unterhalb des 10 % Quantils - ignoriert. Damit sollen Ausreißer aus der Berechnung des Mittelwerts ausgeschlossen werden. Der getrimmte Mittelwert kann wie folgt in R berechnet werden:\nmean_trim_sales \u0026lt;- mean(Advertising$sales, trim = 0.1, na.rm = TRUE) print(paste0(\u0026quot;Getrimmter Mittelwert der Variable Sales: \u0026quot;, round(mean_trim_sales, 2))) ## [1] \u0026quot;Getrimmter Mittelwert der Variable Sales: 13.78\u0026quot;  1.2.4 Schiefe Die Schiefe ist eine statistische Kennzahl, die die Art und Stärke der Asymmetrie einer Wahrscheinlichkeitsverteilung beschreibt. Sie zeigt an, ob und wie stark die Verteilung nach rechts (positive Schiefe) oder nach links (negative Schiefe) geneigt ist. Jede nicht symmetrische Verteilung heißt schief.\nDarstellung der Verteilung in einem Histogramm:\nlibrary(ggplot2) # Vorlage für die Erstellung von plots in ggplot2 plot_1 \u0026lt;- theme_bw() + theme(axis.text.x = element_text(angle = 0, size = 8, family=\u0026quot;Arial\u0026quot;, colour=\u0026#39;black\u0026#39;), axis.text.y = element_text(angle = 0, size = 8, family=\u0026quot;Arial\u0026quot;, colour=\u0026#39;black\u0026#39;), axis.title = element_text(size=8, face=\u0026quot;bold\u0026quot;, family=\u0026quot;Arial\u0026quot;, colour=\u0026#39;black\u0026#39;), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.title=element_text(hjust=0, size=10, family=\u0026quot;Arial\u0026quot;, face=\u0026quot;bold\u0026quot;, colour=\u0026#39;black\u0026#39;)) ggplot(Advertising, aes(sales)) + geom_histogram(binwidth = 2, color=\u0026quot;red\u0026quot;, alpha=.2) + scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + labs(title=\u0026quot;Histogramm für Sales\u0026quot;, x=\u0026quot;Sales\u0026quot;, y=\u0026quot;Anzahl\u0026quot;) + plot_1 Darstellung der Verteilung in einer Dichtefunktion:\nlibrary(ggplot2) ggplot(Advertising, aes(sales)) + geom_density(fill=\u0026quot;grey\u0026quot;,alpha=.2 ) + geom_vline(aes(xintercept=mean(sales, na.rm=TRUE)), color=\u0026quot;red\u0026quot;, linetype=\u0026quot;dotted\u0026quot;, size=0.6) + geom_vline(aes(xintercept=median(sales, na.rm=TRUE)), color=\u0026quot;red\u0026quot;, linetype=\u0026quot;dotted\u0026quot;, size=0.6) + geom_text(aes(x=median(sales), y=0.02), colour = \u0026quot;grey\u0026quot;, size =3, label=round(mean(Advertising$sales), digits=2), hjust=-1, family=\u0026quot;Arial\u0026quot;) + geom_text(aes(x=mean(sales), y=0.02), hjust=-0.7, colour = \u0026quot;grey\u0026quot;, size = 3, label=\u0026quot;Mittelwert\u0026quot;, family=\u0026quot;Arial\u0026quot;) + geom_text(aes(x=median(sales), y=0.005), colour = \u0026quot;grey\u0026quot;, size =3, label=round(median(Advertising$sales), digits=2), hjust=1 , family=\u0026quot;Arial\u0026quot;) + geom_text(aes(x=median(sales), y=0.01), colour = \u0026quot;grey\u0026quot;, size = 3, label=\u0026quot;Median\u0026quot;, hjust=1, family=\u0026quot;Arial\u0026quot;) + labs(x=\u0026quot;Produktabsatz (in Tausend Einheiten)\u0026quot;, y = \u0026quot;Dichte\u0026quot;, title = \u0026quot;Wahrscheinlichkeitsdichtefunktion\u0026quot;) + plot_1 In der Abbildung kann man erkennen, dass es sich um eine asymmetrische Verteilung handelt (d.h. es liegt eine Abweichung von der Normalverteilung vor). Konkret handelt es sich um eine rechtsschiefe Verteilung (Mittelwert \u0026gt; Median; Schiefe = + 0.40).\n 1.2.5 Kurtosis Die Abweichung des Verlaufs einer Verteilung vom Verlauf einer Normalverteilung wird Kurtosis (Wölbung) genannt. Sie gibt an, wie spitz die Kurve verläuft. Unterschieden wird zwischen positiver, spitz zulaufender (leptokurtische Verteilung) und negativer, flacher (platykurtische Verteilung) Kurtosis. Die Kurtosis zählt zu den zentralen Momenten einer Verteilung, mittels derer der Kurvenverlauf definiert wird. Eine Kurtosis mit Wert 0 ist normalgipflig (mesokurtisch), ein Wert größer 0 ist steilgipflig und ein Wert unter 0 ist flachgipflig.\n 1.2.6 Standardfehler Der Standardfehler ein Maß für die durchschnittliche Abweichung des geschätzten Parameterwertes vom wahren Parameterwert. Je kleiner der Standardfehler ist, desto genauer kann der unbekannte Parameter der Population mit Hilfe der Schätzfunktion geschätzt werden. Der Standardfehler hängt unter anderem von dem Stichprobenumfang und der Varianz ab. Allgemein gilt: Je größer der Stichprobenumfang, desto kleiner der Standardfehler; je kleiner die Varianz, desto kleiner der Standardfehler.\n   ","date":1564876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564876800,"objectID":"db44e21c6ccda2defbb07fd439afc990","permalink":"/post/2019-08-01-r-descriptive-statistics/","publishdate":"2019-08-04T00:00:00Z","relpermalink":"/post/2019-08-01-r-descriptive-statistics/","section":"post","summary":"Berechnung von einfachen Statistiken in R.","tags":["Statistics"],"title":"Deskriptive Statistik in R","type":"post"},{"authors":["Jan Kirenz"],"categories":["tutorial"],"content":" Linear Regression Tutorial in Python Linear regression is the fundamental starting point for all regression methods. In this Jupyter Notebook, we fit a regression model in Python and take a closer look at the following topics:\n Histogramms Boxplots Mean Standard deviation Mean squared error $R^2$ Pearson\u0026rsquo;s correlation coefficient F-Statistic Standard error Confidence interval  Download the linear regression tutorial Jupyter Notebook in GitHub and open the file in Jupyter Notebook.\n","date":1564876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564876800,"objectID":"2e29f9d735ed71ed3c94cf69433d617c","permalink":"/project/python-linear-regression/","publishdate":"2019-08-04T00:00:00Z","relpermalink":"/project/python-linear-regression/","section":"project","summary":"Introduction to essential concepts of linear regression with Python.","tags":["Statistics","Python","Regression","DataExploration"],"title":"Linear Regression Tutorial with Python","type":"project"},{"authors":["Jan Kirenz"],"categories":["course"],"content":" First Steps in Python Download the PDF Python overview to get an overview about Python and a list of helpful resources (you need to download the file in order to use the embedded links).\nFirst of all, install Anaconda - it\u0026rsquo;s a free and open-source distribution of the Python programming language that aims to simplify package management and deployment. It already contains Jupyter Notebook (see below) and other important data science modules.\n Install Anaconda (select the current version of Python 3). After installation, launch the Anaconda Navigator and start Jupyter Notebook or Jupyter Lab.  One important third-party tool for data science is Jupyter, an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Learn why Jupyter is data scientists\u0026rsquo; computational notebook of choice:\n Perkel, J. M. (2018). Why Jupyter is data scientists\u0026rsquo; computational notebook of choice. Nature, 563(7729), p. 145.\n Pandey, P. (2018). Bringing the best out of Jupyter Notebooks for Data Science. Towards Data Science\n Pandey, P. (2019). Jupyter Lab: Evolution of the Jupyter Notebook. Towards Data Science\n  Now, let\u0026rsquo;s start with some code examples:\n1) Import and save CSV-files with Pandas\n2) Check for missing values\n3) Change data type (level of measurment)\n4) Descriptive statistics  Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud. With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser:\n5) Overview of the data science programming process\nRecommended reading: \u0026ldquo;A Whirlwind Tour of Python\u0026rdquo; is a free and fast-paced introduction to essential features of the Python language. The material is particularly designed for those who wish to use Python for data science and/or scientific programming:\n VanderPlas, J. (2016). Whirlwind Tour of Python. O\u0026rsquo;Reilly Media.  ","date":1564790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564790400,"objectID":"406212c849cb00a72f75fea461300409","permalink":"/project/python-first-steps/","publishdate":"2019-08-03T00:00:00Z","relpermalink":"/project/python-first-steps/","section":"project","summary":"Learn Data Science with Python","tags":["Python","DataScience"],"title":"First Steps in Python","type":"project"},{"authors":["Jan Kirenz"],"categories":["course"],"content":" First Steps in R Download the PDF R overview to get an overview about R and a list of helpful resources (you need to download the file in order to use the embedded links).\nInstalling R The first step is to install R. You can download and install R from the Comprehensive R Archive Network (CRAN).\nWindows: - Open the Comprehensive R Archive Network. - Click on “CRAN”. You’ll see a list of mirror sites, organized by country. - Select a site near you. - Click on “Windows” under “Download and Install R”. - Click on “base”. - Click on the link for downloading the latest version of R (an .exe file). - When the download completes, double-click on the .exe file and answer the usual questions.\nMac: - Open the Comprehensive R Archive Network. - Click on “CRAN”. - You’ll see a list of mirror sites, organized by country. - Select a site near you. - Click on “MacOS X”. - Click on the .pkg file for the latest version of R, under “Files:”, to download it. - When the download completes, double-click on the .pkg file and answer the usual questions.\nInstalling RStudio The next step is to install RStudio, a free and open-source integrated development environment (IDE) for R. You can use it for viewing and running R scripts.\n Go to RStudio Download Click the Download RStudio Desktop button. Select the installation file for your system. Run the installation file.  Learn R Basics First of all, you can take an online course to master the basics of R: Visit the interactive R-Course from DataCamp. With the knowledge gained in this courses, you will be ready to undertake your first very own data analysis.\nThere are also open and free resources and reference guides for R. Two examples are:\n Quick-R: a quick online reference for data input, basic statistics and plots R reference card (PDF) by Tom Short  Two key things you need to know about R is that you can get help for a function using help or ?, like this:\n?install.packages help(\u0026quot;install.packages\u0026quot;)  and the hash character represents comments, so text following these characters is not interpreted:\n##This is just a comment  Installing R Packages The first R command we will run is install.packages.\nAn R package is a collection of functions, data, and documentation that extends the capabilities of base R. Many of these functions are stored in CRAN. You can easily install packages from within RStudio if you know the name of the packages.\nAs an example, we are going to install the package dplyr which we use in our first data analysis examples:\ninstall.packages(\u0026quot;dplyr\u0026quot;)  We can then load the package into our R sessions using the library function:\nlibrary(dplyr)  From now on you will see that we sometimes load packages without installing them. This is because you only need to install a package once, but you need to reload it with the command library every time you start a new R session.\nIf you try to load a package and get an error, it probably means you need to install it first.\nReview the dplyr-documentation to get an overview about the different functionalities of this package.\n","date":1564790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564790400,"objectID":"0e669adf7ec92f519cbd427c62406a42","permalink":"/project/r-first-steps/","publishdate":"2019-08-03T00:00:00Z","relpermalink":"/project/r-first-steps/","section":"project","summary":"Learn Data Science with R","tags":["R","DataScience"],"title":"First Steps in R","type":"project"},{"authors":null,"categories":null,"content":"","date":1564704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564704000,"objectID":"c7297892dd13604a0e171a4f21c6c629","permalink":"/project/blogdown-book/","publishdate":"2019-08-02T00:00:00Z","relpermalink":"/project/blogdown-book/","section":"project","summary":"A practical guide for creating websites using the blogdown package in R","tags":["book","DataReporting","R"],"title":"Creating Websites with R Markdown","type":"project"},{"authors":null,"categories":null,"content":" The xaringan package is an R Markdown extension based on the JavaScript library remark.js to generate HTML5 presentations in different themes (Xie 2019).\nYou can learn more about the usage of the xaringan package from this excellent documentation, which is actually a set of slides generated from xaringan.\nXie, Y. (2019). Xaringan: Presentation Ninja. CRAN\n","date":1564653600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564653600,"objectID":"b19c0e15b29b8a6a0eab449da497b019","permalink":"/project/r-xaringan/","publishdate":"2019-08-01T10:00:00Z","relpermalink":"/project/r-xaringan/","section":"project","summary":"Xaringan - An R package for creating slideshows with R Markdown.","tags":["R","DataReporting","R Markdown"],"title":"Create Data Presentations with R Markdown","type":"project"},{"authors":["Jan Kirenz"],"categories":["Tutorial"],"content":"\u0026ldquo;R for Data Science\u0026rdquo; offers an excellent introduction into data science in R with a focus on the popular package collection tidyverse. See how the tidyverse makes data science faster, easier and more fun:\n Wickham, H., \u0026amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O\u0026rsquo;Reilly Media, Inc.  \u0026ldquo;An Introduction to Statistical Learning\u0026rdquo; provides an accessible overview of the field of statistical learning with applications in R. This book presents important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more:\n James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An introduction to statistical learning with applications in R (Corr. 7th printing 2017). New York: Springer.  \u0026ldquo;Statistical Thinking for the 21 Century\u0026rdquo; and \u0026ldquo;Modern Dive: Statistical Inference via Data Science\u0026rdquo; are both open-source digital textbooks which provide a great introduction into the fundamentals of modern quantitative methods which take advantage of today’s increased computing power to solve statistical problems with R:\n Poldrack, R. A. (2019). Statistical Thinking for the 21 Century. http://thinkstats.org.\n Ismay, C. \u0026amp; Kim, A. Y. (2019). Modern Dive: Statistical Inference via Data Science. https://moderndive.com\n  ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"06dd35ea1206f06323fcdbd727d9fb2a","permalink":"/project/r-data-science-stats/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/project/r-data-science-stats/","section":"project","summary":"Free resources to learn data science and statistics with R.","tags":["Statistics","R","StatisticalLearning","DataExploration"],"title":"Essential books for Data Science \u0026 Statistics with R","type":"project"},{"authors":["Jan Kirenz"],"categories":["blogdown","R"],"content":"  1 Read up on blogdown 2 In GitHub 3 In terminal 4 In RStudio 5 Build your site in RStudio 6 Deploy in Netlify 7 Going further   1 Read up on blogdown The content below is taken from the excellent post “Up \u0026amp; Running with blogdown” from Alison Hill\nBefore you start, I recommend reading the following:\n blogdown: Creating Websites with R Markdown by Yihui Xie and Amber Thomas  Also note that I am a macOS user, and I use R, RStudio, Git (usually via GitHub), and terminal regularly, so I’m assuming familiarity here with all of these. If that is not the case, here are some places to get started:\n For Git: Happy Git with R by Jenny Bryan et al. For RStudio: DataCamp’s Working with the RStudio IDE (free) by Garrett Grolemund For Terminal: The Command Line Murder Mystery by Noah Veltman, and The UNIX Workbench by Sean Kross  I also have Xcode and Homebrew installed- you will probably need these to download Hugo. If you don’t have either but are on a mac, this link may help:\n How to install Xcode, Homebrew, Git, RVM, Ruby \u0026amp; Rails on Mac OS X  Introduction to static site generators and how domain names work:\n “Considering the cost and friendliness to beginners, I currently recommend Netlify.” “If you are not familiar with domain names or do not want to learn more about them, an option for your consideration is a free subdomain *.rbind.io offered by RStudio, Inc.”.   2 In GitHub Go online to your GitHub account, and create a new repository (check to initialize with a README but don’t add .gitignore- this will be taken care of later). For naming your repo, consider your future deployment plan:\n If you are going to use Netlify to host the site, you can name this repository anything you want!  You can see some of the repo names used by members of the rbind organization here.    If you want to host your site as a GitHub Page, you should name your repository yourgithubusername.github.io.   Screenshot above: Creating a new repository in GitHub\n Go to the main page of your new repository, and under the repository name, click the green Clone or download button.\n In the Clone with HTTPs section, click on the clipboard icon to copy the clone URL for your new repository. You’ll paste this text into terminal in the next section.\n   3 In terminal Now you will clone your remote repository and create a local copy on your computer so you can sync between the two locations (using terminal or your alternative command line tool for a Windows machine). However, I recommend to use GitHub Desktop instead of the terminal for the cloning process. If you instead would like to use the terminal, this is how you proceed:\nUse cd to navigate into the directory where you want your repo to be\n Once there, type: git clone [paste]. So my command looked like this:\n  git clone https://github.com/apreshill/apreshill.git And this is what printed to the terminal window:\nCloning into \u0026#39;apreshill\u0026#39;... remote: Counting objects: 3, done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), done. Checking connectivity... done. Close terminal, you are done in there.   4 In RStudio Install blogdown from your RStudio console. If you already have devtools installed like I did, you can just use the second line below:  if (!requireNamespace(\u0026quot;devtools\u0026quot;)) install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;rstudio/blogdown\u0026quot;) Install Hugo using the blogdown package helper function:  blogdown::install_hugo() # or library(blogdown) install_hugo() Use the top menu buttons in RStudio to select File -\u0026gt; New Project -\u0026gt; Existing Directory, then browse to the directory on your computer where your GitHub repo is and click on the Create Project button.  Screenshot above: Creating a new project in an existing directory in RStudio\n Now you should be “in” your project in RStudio. If you are using git for version control, edit your *gitignore file. This file should be viewable in your file viewer pane in RStudio. Below is what it should look like: the first four lines will automatically be in this file if you have set up your RStudio Project, but if you plan to use Netlify to deploy, you need to add the public/ line (read about here.)  .Rproj.user .Rhistory .RData .Ruserdata blogdown .DS_Store # if a windows user, Thumbs.db instead public/ # if using Netlify  5 Build your site in RStudio Now you can finally build your site using the blogdown::new_site() function. But first you should at least think about themes…\n5.1 Picking a theme There are over 90 Hugo themes. Here you can find an overview of some of the themes. Whatever theme you choose, you’ll need to pick one of 3 ways to make your new site:\nIf you are happy with the default theme, which is the lithium theme, you can use:  blogdown::new_site() # default theme is lithium If you want a theme other than the default, you can specify the theme at the same time as you call the new_site function:  # for example, create a new site with the academic theme blogdown::new_site(theme = \u0026quot;gcushen/hugo-academic\u0026quot;, theme_example = TRUE) If instead you want to add the theme later, you can do this:  library(blogdown) new_site() # default theme is lithium # need to stop serving so can use the console again install_theme(\u0026quot;gcushen/hugo-academic\u0026quot;, theme_example = TRUE, update_config = TRUE)  Now is a good time to re-read about blogdown::serve_site() and how LiveReload works (and how it blocks your R console by default)    5.2 Update project options In your project in RStudio, go to the top menu bar of RStudio and select Tools -\u0026gt; Project Options and update following Yihui and Amber’s instructions.\n 5.3 Edit your configurations Relevant reading:\n blogdown book chapter on configuration You can also view Alison Hill’s config.toml file in GitHub  Now, edit the baseurl in your config.toml file. The URL should always end with a / trailing slash. At this point, you probably haven’t deployed your site yet, so to view it locally you can use the Serve Site add-in, or run the blogdown::serve_site function. Both of these baseurls worked for me when viewing locally:\nbaseurl = \u0026quot;https://example.com/\u0026quot; baseurl = \u0026quot;/\u0026quot;  Make sure that the baseurl = listed ends with a trailing slash /!   Go ahead and edit all the other elements in the config.toml file now as you please- this is how you personalize your site.\n 5.4 Addins \u0026amp; workflow Relevant reading:\n blogdown book chapter on the RStudio IDE  Addins: use them- you won’t need the blogdown library loaded in the console if you use the Addins. The workflow in RStudio at this point (again, just viewing locally because we haven’t deployed yet) works best like this:\nOpen the RStudio project for the site Use the Serve Site add-in (only once due to LiveReload) View site in the RStudio viewer pane, and open in a new browser window while you work Select existing files to edit using the file pane in RStudio After making changes, click the save button (don’t knit!)- the console will reload, the viewer pane will update, and if you hit refresh in the browser your local view will also be updated When happy with changes, add/commit/push changes to GitHub  Having blogdown::serve_site running locally with LiveReload is especially useful as you can immediately see if you have made any mistakes.\nThe above workflow is only for editing existing files or posts, but not for creating new posts. For that, read on…\n 5.5 Posting Relevant reading:\n blogdown book chapter on RStudio IDE blogdown book chapter on output formats: on .md versus .Rmd posts  Bottom line:\nUse the New Post addin. But, you need the console to do this, so you have to stop blogdown::serve_site by clicking on the red Stop button first. The Addin is a Shiny interface that runs this code in your console: blogdown:::new_post_addin(). So, your console needs to be unblocked for it to run. You also need to be “in” your RStudio project or it won’t work.\n5.5.1 Draft posts Relevant reading:\n blogdown book chapter on building a website for local preview  Whether you do a markdown or R Markdown post (see below), you should know that in the YAML front matter of your new file, you can add draft: TRUE and you will be able to preview your post using blogdown::serve_site(), but conveniently your post will not show up on your deployed site until you set it to false. Because this is a function built into Hugo, all posts (draft or not) will still end up in your GitHub repo though.\n 5.5.2 New markdown posts Pick one of 2 methods:\nUse the New Post addin and with the radio button at the bottom select Format: Markdown (recommended) Use the console to author a new .md post:  blogdown::new_post() blogdown::new_post(ext = \u0026#39;.md\u0026#39;) # md is the default! Here are the ?new_post arguments:\nnew_post(title, kind = \u0026quot;\u0026quot;, open = interactive(), author = getOption(\u0026quot;blogdown.author\u0026quot;), categories = NULL, tags = NULL, date = Sys.Date(), file = NULL, slug = NULL, title_case = getOption(\u0026quot;blogdown.title_case\u0026quot;), subdir = getOption(\u0026quot;blogdown.subdir\u0026quot;, \u0026quot;post\u0026quot;), ext = getOption(\u0026quot;blogdown.ext\u0026quot;, \u0026quot;.md\u0026quot;))  Remember to use the Serve Site addin again so that you can immediately view your changes with every save using LiveReload.    5.5.3 New R Markdown (.Rmd) posts Again, you have your choice of one of 2 methods:\nUse the New Post addin and with the radio button at the bottom select Format: R Markdown (.Rmd) (recommended) Use the console to author a new .Rmd post:  blogdown::new_post(ext = \u0026#39;.Rmd\u0026#39;) # md is the default! After you edit your .Rmd post, in addition to saving the changes in your .Rmd file, you must use blogdown::serve_site- this is how the output html file needs to be generated.\n Do not knit your .Rmd posts- use blogdown::serve_site instead. If you happen to hit the knit button, just Serve Site again to rewrite the .html file.   Ultimately, your YAML front matter looks something like this; note that some but not all features of rmarkdown::html_document are supported in blogdown:\n--- title: \u0026quot;My Post\u0026quot; author: \u0026quot;John Doe\u0026quot; date: \u0026quot;2017-02-14\u0026quot; output: blogdown::html_page: toc: true toc_depth: 1 number_sections: true fig_width: 6 ---  Remember to use the Serve Site addin again so that you can immediately view your changes with every save using LiveReload and your .html file is properly output.    5.5.4 Adding images to a post If you want to include an image that is not a figure created from an R chunk, the recommended method is to:\nAdd the image to your /static/img/ folder, then Reference the image using the relative file path as follows:  ![my-image](/img/my-image.png)    6 Deploy in Netlify Deploying in Netlify through GitHub is smooth. Here are some beginner instructions, but Netlify is so easy, I recommend that you skip dragging your public folder in and instead automate the process through GitHub.\nWhen you are ready to deploy, commit your changes and push to GitHub, then go online to Netlify. Click on the Sign Up button and sign up using your existing GitHub account (no need to create another account) Log in, and select: New site from Git -\u0026gt; Continuous Deployment: GitHub. From there, Netlify will allow you to select from your existing GitHub repositories. You’ll pick the repo you’ve been working from with blogdown, then you’ll configure your build. This involves specifying two important things: the build command and the publish directory (this should be public).\n More about the build command from Netlify: “For Hugo hosting, hugo will build and deploy with the version 0.17 of hugo. You can specify a specific hugo release like this: hugo_0.15. Currently 0.13, 0.14, 0.15, 0.16, 0.17, 0.18 and 0.19 are supported. For version 0.20 and above, you’ll need to create a Build environment variable called HUGO_VERSION and set it to the version of your choice.” I opted for the former, and specified hugo_0.19.   You can check your hugo version in terminal using the command hugo version. This is what my output looked like, so I could run version 0.20 if I wanted to through Netlify, but I went with 0.19 and it works just fine.\n$ hugo version Hugo Static Site Generator v0.20.7 darwin/amd64 BuildDate: 2017-05-08T18:37:40-07:00 Screenshot above: Basic build settings in Netlify\n Netlify will deploy your site and assign you a random subdomain name of the form random-word-12345.netlify.com. You should know that you can change this; e.g. to mynewsite.netlify.com.\n Anytime you change your subdomain name, you need to update the baseurl in your config.toml file (e.g., baseurl = “https://mynewsite.netlify.com/”).   At this point, you should be up and running with blogdown, GitHub, and Netlify, but here are some ideas if you want to go further…\n 7 Going further 7.1 Custom CSS Every Hugo theme is structured a little differently, but if you are interested, you can check out Alison Hill’s custom css to see how she customized the academic theme, which provides a way to link to a custom CSS file in the config.toml file:\n # Link custom CSS and JS assets # (relative to /static/css and /static/js respectively) custom_css = [\u0026quot;blue.css\u0026quot;]  7.2 Formspree Alison Hill used Formspree to make a contact form, which is an online service (managed on GitHub) that allows you to add an HTML form to your static site. No registration, just use the form and confirm your email address once. She added the following code into the contact widget:\n\u0026lt;form action=\u0026quot;https://formspree.io/your@email.com\u0026quot; method=\u0026quot;POST\u0026quot;\u0026gt; \u0026lt;label for=\u0026quot;name\u0026quot;\u0026gt;Your name: \u0026lt;/label\u0026gt; \u0026lt;input type=\u0026quot;text\u0026quot; name=\u0026quot;name\u0026quot; required=\u0026quot;required\u0026quot; placeholder=\u0026quot;here\u0026quot;\u0026gt;\u0026lt;br\u0026gt; \u0026lt;label for=\u0026quot;email\u0026quot;\u0026gt;Your email: \u0026lt;/label\u0026gt; \u0026lt;input type=\u0026quot;email\u0026quot; name=\u0026quot;_replyto\u0026quot; required=\u0026quot;required\u0026quot; placeholder=\u0026quot;here\u0026quot;\u0026gt;\u0026lt;br\u0026gt; \u0026lt;label for=\u0026quot;message\u0026quot;\u0026gt;Your message:\u0026lt;/label\u0026gt;\u0026lt;br\u0026gt; \u0026lt;textarea rows=\u0026quot;4\u0026quot; name=\u0026quot;message\u0026quot; id=\u0026quot;message\u0026quot; required=\u0026quot;required\u0026quot; class=\u0026quot;form-control\u0026quot; placeholder=\u0026quot;I can\u0026#39;t wait to read this!\u0026quot;\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;input type=\u0026quot;hidden\u0026quot; name=\u0026quot;_next\u0026quot; value=\u0026quot;/html/thanks.html\u0026quot; /\u0026gt; \u0026lt;input type=\u0026quot;submit\u0026quot; value=\u0026quot;Send\u0026quot; name=\u0026quot;submit\u0026quot; class=\u0026quot;btn btn-primary btn-outline\u0026quot;\u0026gt; \u0026lt;input type=\u0026quot;hidden\u0026quot; name=\u0026quot;_subject\u0026quot; value=\u0026quot;Website message\u0026quot; /\u0026gt; \u0026lt;input type=\u0026quot;text\u0026quot; name=\u0026quot;_gotcha\u0026quot; style=\u0026quot;display:none\u0026quot; /\u0026gt; \u0026lt;/form\u0026gt;  7.3 *.rbind.io domain names You may want a different domain name than the one provided by Netlify. Alison opted for a free subdomain *.rbind.io offered by RStudio. To do the same, head over to the rbind/support GitHub page and open a new issue. All you need to do is let them know what your Netlify subdomain name is (*.netlify.com), and what you want your subdomain name to be (*.rbind.io). The rbind support team will help you take it from there!\n Again, you will need to update the baseurl in your config.toml file to reflect your new rbind subdomain name (so Alison’s is baseurl = “https://alison.rbind.io/”).   That’s all for now.\n  ","date":1563580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563580800,"objectID":"7d6af479259dc4296cafa4db445c9d2c","permalink":"/post/2019-07-20-up-and-running-with-blogdown/","publishdate":"2019-07-20T00:00:00Z","relpermalink":"/post/2019-07-20-up-and-running-with-blogdown/","section":"post","summary":"A guide to getting up and running with blogdown, GitHub, and Netlify","tags":["blogdown"],"title":"Up \u0026 Running with blogdown","type":"post"},{"authors":["Jan Kirenz"],"categories":["seminar"],"content":"A hands-on data science and data engineering seminar, where students learn how to scrape websites, use API\u0026rsquo;s, use natural language processing (NLP), perform a network analysis and build dashboards with Python and R.\n","date":1557997200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557997200,"objectID":"b8933cf40b9ce90f29d7b3ae5685c037","permalink":"/talk/2019-web-analytics-social-media/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/talk/2019-web-analytics-social-media/","section":"talk","summary":"Webscraping, API's, Network Analysis, NLP, Dashboards","tags":["DataScience"],"title":"Web Analytics \u0026 Social Media Analytics","type":"talk"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"About / Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"Tutorials and resources for data science and data engineering.","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"322dbaccf72a6d71f827fdb2866be935","permalink":"/teaching/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/teaching/","section":"","summary":"Upcoming and recent seminars \u0026 lectures","tags":null,"title":"Seminars \u0026 Lectures","type":"widget_page"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n  \n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"}]