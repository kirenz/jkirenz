<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on Jan Kirenz</title>
    <link>/tags/statistics/</link>
    <description>Recent content in Statistics on Jan Kirenz</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Jan Kirenz, {year}</copyright>
    <lastBuildDate>Mon, 30 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Applied Statistics</title>
      <link>/talk/2019-applied-statistics/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/talk/2019-applied-statistics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lasso Regression with Python</title>
      <link>/post/2019-08-12-python-lasso-regression-auto/python-lasso-regression/</link>
      <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-08-12-python-lasso-regression-auto/python-lasso-regression/</guid>
      <description>

&lt;p&gt;Lasso performs a so called &lt;code&gt;L1 regularization&lt;/code&gt; (a process of introducing additional information in order to prevent overfitting), i.e. adds penalty equivalent to absolute value of the magnitude of coefficients.&lt;/p&gt;

&lt;p&gt;In particular, the minimization objective does not only include the residual sum of squares (RSS) but also the sum of the absolute value of coefficients.&lt;/p&gt;

&lt;p&gt;The residual sum of squares (RSS) is calculated as follows:&lt;/p&gt;

&lt;p&gt;$$ RSS = \sum_{i=1}^{n} (y - \hat{y})^2 $$&lt;/p&gt;

&lt;p&gt;Hence, the minimization objective becomes: RSS + α (sum of absolute value of coefficients), where α (alpha) provides a trade-off between balancing RSS and magnitude of coefficients.&lt;/p&gt;

&lt;p&gt;α can take various values:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;α = 0: Same coefficients as simple linear regression&lt;/li&gt;
&lt;li&gt;α = ∞: All coefficients zero (same logic as before)&lt;/li&gt;
&lt;li&gt;0 &amp;lt; α &amp;lt; ∞: coefficients between 0 and that of simple linear regression&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;data-preparation&#34;&gt;Data preparation&lt;/h2&gt;

&lt;p&gt;This tutorial involves the use of the Lasso regression on the &amp;ldquo;Auto&amp;rdquo; dataset. In particular, we only use observations 1 to 200 for our analysis. Furthermore, you can drop the &lt;code&gt;name&lt;/code&gt; variable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
plt.style.use(&#39;ggplot&#39;)
import warnings; warnings.simplefilter(&#39;ignore&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&amp;quot;https://raw.githubusercontent.com/kirenz/datasets/master/Auto.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Tidying data&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.iloc[0:200]
df = df.drop([&#39;name&#39;], axis=1)
df.info()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 200 entries, 0 to 199
Data columns (total 8 columns):
mpg             200 non-null float64
cylinders       200 non-null int64
displacement    200 non-null float64
horsepower      200 non-null object
weight          200 non-null int64
acceleration    200 non-null float64
year            200 non-null int64
origin          200 non-null int64
dtypes: float64(3), int64(4), object(1)
memory usage: 12.6+ KB
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;origin&#39;] = pd.Categorical(df[&#39;origin&#39;])
df[&#39;horsepower&#39;] = pd.to_numeric(df[&#39;horsepower&#39;], errors=&#39;coerce&#39;)
print(df.isnull().sum())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;mpg             0
cylinders       0
displacement    0
horsepower      2
weight          0
acceleration    0
year            0
origin          0
dtype: int64
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# drop missing cases
df = df.dropna()
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;implementation-of-lasso-regression&#34;&gt;Implementation of Lasso regression&lt;/h1&gt;

&lt;p&gt;We use scikit learn to fit a Lasso regression &lt;a href=&#34;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34; target=&#34;_blank&#34;&gt;(see documentation)&lt;/a&gt; and follow a number of steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(1.1) &lt;strong&gt;Standardize&lt;/strong&gt; the features (module: &lt;code&gt;from sklearn.preprocessing import StandardScaler&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Hint: It is important to standardize the features by removing the mean and scaling to unit variance. The L1 (Lasso) and L2 (Ridge) regularizers of linear models assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;(1.2) &lt;strong&gt;Split the data set&lt;/strong&gt; into train and test sets (use &lt;code&gt;X_train&lt;/code&gt;, &lt;code&gt;X_test&lt;/code&gt;, &lt;code&gt;y_train&lt;/code&gt;, &lt;code&gt;y_test&lt;/code&gt;), with the first 75% of the data for training and the remaining for testing. (module: &lt;code&gt;from sklearn.model_selection import train_test_split&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;(1.3) Apply &lt;strong&gt;Lasso regression&lt;/strong&gt; on the training set with the regularization parameter &lt;strong&gt;lambda = 0.5&lt;/strong&gt; (module: &lt;code&gt;from sklearn.linear_model import Lasso&lt;/code&gt;) and print the $R^2$-score for the training and test set. Comment on your findings.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;(1.4) Apply the &lt;strong&gt;Lasso regression&lt;/strong&gt; on the training set with the following &lt;strong&gt;λ parameters: (0.001, 0.01, 0.1, 0.5, 1, 2, 10)&lt;/strong&gt;. Evaluate the R^2 score for all the models you obtain on both the train and test sets.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;(1.5) &lt;strong&gt;Plot&lt;/strong&gt; all values for both data sets (train and test $R^2$-values) as a function of λ. Comment on your findings.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;(1.6) Store your test data results in a DataFrame and indentify the lambda where the $R^2$ has it&amp;rsquo;s &lt;strong&gt;maximum value&lt;/strong&gt; in the &lt;strong&gt;test data&lt;/strong&gt;. Fit a Lasso model with this lambda parameter (use the training data) and obtain the corresponding &lt;strong&gt;regression coefficients&lt;/strong&gt;. Furthermore, obtain the &lt;strong&gt;mean squared error&lt;/strong&gt; for the test data of this model (module: &lt;code&gt;from sklearn.metrics import mean_squared_error&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;(1.7) Evaluate the performance of a &lt;strong&gt;Lasso regression&lt;/strong&gt; for different regularization parameters λ using &lt;strong&gt;5-fold cross validation&lt;/strong&gt; on the training set (module: &lt;code&gt;from sklearn.model_selection import cross_val_score&lt;/code&gt;) and plot the cross-validation (CV) $R^2$ scores of the training and test data as a function of λ.&lt;/p&gt;

&lt;p&gt;Use the following lambda parameters:
l_min = 0.05
l_max = 0.2
l_num = 20
lambdas = np.linspace(l_min,l_max, l_num)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;(1.8) Finally, store your test data results in a DataFrame and identify the lambda where the $R^2$ has it&amp;rsquo;s &lt;strong&gt;maximum value&lt;/strong&gt; in the &lt;strong&gt;test data&lt;/strong&gt;. Fit a Lasso model with this lambda parameter (use the training data) and obtain the corresponding &lt;strong&gt;regression coefficients&lt;/strong&gt;. Furthermore, obtain the &lt;strong&gt;mean squared error&lt;/strong&gt; for the test data of this model (module: &lt;code&gt;from sklearn.metrics import mean_squared_error&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;1-1-standardization&#34;&gt;1.1 Standardization&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dfs = df.astype(&#39;int&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dfs.info()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
Int64Index: 198 entries, 0 to 199
Data columns (total 8 columns):
mpg             198 non-null int64
cylinders       198 non-null int64
displacement    198 non-null int64
horsepower      198 non-null int64
weight          198 non-null int64
acceleration    198 non-null int64
year            198 non-null int64
origin          198 non-null int64
dtypes: int64(8)
memory usage: 13.9 KB
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dfs.columns
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Index([&#39;mpg&#39;, &#39;cylinders&#39;, &#39;displacement&#39;, &#39;horsepower&#39;, &#39;weight&#39;,
       &#39;acceleration&#39;, &#39;year&#39;, &#39;origin&#39;],
      dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
dfs[[&#39;cylinders&#39;, &#39;displacement&#39;, &#39;horsepower&#39;,
     &#39;weight&#39;, &#39;acceleration&#39;, &#39;year&#39;, &#39;origin&#39;]] = scaler.fit_transform(dfs[[&#39;cylinders&#39;,
                                                                              &#39;displacement&#39;,
                                                                              &#39;horsepower&#39;,
                                                                              &#39;weight&#39;,
                                                                              &#39;acceleration&#39;,
                                                                              &#39;year&#39;, &#39;origin&#39;]])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dfs.head(5)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;mpg&lt;/th&gt;
      &lt;th&gt;cylinders&lt;/th&gt;
      &lt;th&gt;displacement&lt;/th&gt;
      &lt;th&gt;horsepower&lt;/th&gt;
      &lt;th&gt;weight&lt;/th&gt;
      &lt;th&gt;acceleration&lt;/th&gt;
      &lt;th&gt;year&lt;/th&gt;
      &lt;th&gt;origin&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;1.179744&lt;/td&gt;
      &lt;td&gt;0.726091&lt;/td&gt;
      &lt;td&gt;0.325216&lt;/td&gt;
      &lt;td&gt;0.346138&lt;/td&gt;
      &lt;td&gt;-0.955578&lt;/td&gt;
      &lt;td&gt;-1.516818&lt;/td&gt;
      &lt;td&gt;-0.629372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;1.179744&lt;/td&gt;
      &lt;td&gt;1.100254&lt;/td&gt;
      &lt;td&gt;1.129264&lt;/td&gt;
      &lt;td&gt;0.548389&lt;/td&gt;
      &lt;td&gt;-1.305309&lt;/td&gt;
      &lt;td&gt;-1.516818&lt;/td&gt;
      &lt;td&gt;-0.629372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;1.179744&lt;/td&gt;
      &lt;td&gt;0.821807&lt;/td&gt;
      &lt;td&gt;0.784672&lt;/td&gt;
      &lt;td&gt;0.273370&lt;/td&gt;
      &lt;td&gt;-1.305309&lt;/td&gt;
      &lt;td&gt;-1.516818&lt;/td&gt;
      &lt;td&gt;-0.629372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;1.179744&lt;/td&gt;
      &lt;td&gt;0.699986&lt;/td&gt;
      &lt;td&gt;0.784672&lt;/td&gt;
      &lt;td&gt;0.270160&lt;/td&gt;
      &lt;td&gt;-0.955578&lt;/td&gt;
      &lt;td&gt;-1.516818&lt;/td&gt;
      &lt;td&gt;-0.629372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;1.179744&lt;/td&gt;
      &lt;td&gt;0.682583&lt;/td&gt;
      &lt;td&gt;0.554944&lt;/td&gt;
      &lt;td&gt;0.287282&lt;/td&gt;
      &lt;td&gt;-1.655041&lt;/td&gt;
      &lt;td&gt;-1.516818&lt;/td&gt;
      &lt;td&gt;-0.629372&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&#34;1-2-split-data&#34;&gt;1.2 Split data&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X = dfs.drop([&#39;mpg&#39;], axis=1)
y = dfs[&#39;mpg&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-3-lasso&#34;&gt;1.3 Lasso&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import Lasso

reg = Lasso(alpha=0.5)
reg.fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False, random_state=None,
   selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&#39;Lasso Regression: R^2 score on training set&#39;, reg.score(X_train, y_train)*100)
print(&#39;Lasso Regression: R^2 score on test set&#39;, reg.score(X_test, y_test)*100)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Lasso Regression: R^2 score on training set 82.49741060950073
Lasso Regression: R^2 score on test set 85.49734440925533
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-4-lasso-with-different-lambdas&#34;&gt;1.4 Lasso with different lambdas&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lambdas = (0.001, 0.01, 0.1, 0.5, 1, 2, 10)
l_num = 7
pred_num = X.shape[1]

# prepare data for enumerate
coeff_a = np.zeros((l_num, pred_num))
train_r_squared = np.zeros(l_num)
test_r_squared = np.zeros(l_num)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# enumerate through lambdas with index and i
for ind, i in enumerate(lambdas):    
    reg = Lasso(alpha = i)
    reg.fit(X_train, y_train)

    coeff_a[ind,:] = reg.coef_
    train_r_squared[ind] = reg.score(X_train, y_train)
    test_r_squared[ind] = reg.score(X_test, y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-5-plot&#34;&gt;1.5 Plot&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plotting
plt.figure(figsize=(18, 8))
plt.plot(train_r_squared, &#39;bo-&#39;, label=r&#39;$R^2$ Training set&#39;, color=&amp;quot;darkblue&amp;quot;, alpha=0.6, linewidth=3)
plt.plot(test_r_squared, &#39;bo-&#39;, label=r&#39;$R^2$ Test set&#39;, color=&amp;quot;darkred&amp;quot;, alpha=0.6, linewidth=3)
plt.xlabel(&#39;Lamda index&#39;); plt.ylabel(r&#39;$R^2$&#39;)
plt.xlim(0, 6)
plt.title(r&#39;Evaluate lasso regression with lamdas: 0 = 0.001, 1= 0.01, 2 = 0.1, 3 = 0.5, 4= 1, 5= 2, 6 = 10&#39;)
plt.legend(loc=&#39;best&#39;)
plt.grid()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post/2019-08-12-python-lasso-regression-auto/output_27_0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;1-6-identify-best-lambda-and-coefficients&#34;&gt;1.6 Identify best lambda and coefficients&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_lam = pd.DataFrame(test_r_squared*100, columns=[&#39;R_squared&#39;])
df_lam[&#39;lambda&#39;] = (lambdas)
# returns the index of the row where column has maximum value.
df_lam.loc[df_lam[&#39;R_squared&#39;].idxmax()]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;R_squared    88.105773
lambda        0.001000
Name: 0, dtype: float64
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Coefficients of best model
reg_best = Lasso(alpha = 0.1)
reg_best.fit(X_train, y_train)
reg_best.coef_
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([-0.35554113, -1.13104696, -0.00596296, -3.31741775, -0.        ,
        0.37914648,  0.74902885])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, reg_best.predict(X_test))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;3.586249592807347
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-7-cross-validation&#34;&gt;1.7 Cross Validation&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;l_min = 0.05
l_max = 0.2
l_num = 20
lambdas = np.linspace(l_min,l_max, l_num)

train_r_squared = np.zeros(l_num)
test_r_squared = np.zeros(l_num)

pred_num = X.shape[1]
coeff_a = np.zeros((l_num, pred_num))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import cross_val_score

for ind, i in enumerate(lambdas):    
    reg = Lasso(alpha = i)
    reg.fit(X_train, y_train)
    results = cross_val_score(reg, X, y, cv=5, scoring=&amp;quot;r2&amp;quot;)

    train_r_squared[ind] = reg.score(X_train, y_train)    
    test_r_squared[ind] = reg.score(X_test, y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plotting
plt.figure(figsize=(18, 8))
plt.plot(train_r_squared, &#39;bo-&#39;, label=r&#39;$R^2$ Training set&#39;, color=&amp;quot;darkblue&amp;quot;, alpha=0.6, linewidth=3)
plt.plot(test_r_squared, &#39;bo-&#39;, label=r&#39;$R^2$ Test set&#39;, color=&amp;quot;darkred&amp;quot;, alpha=0.6, linewidth=3)
plt.xlabel(&#39;Lamda value&#39;); plt.ylabel(r&#39;$R^2$&#39;)
plt.xlim(0, 19)
plt.title(r&#39;Evaluate 5-fold cv with different lamdas&#39;)
plt.legend(loc=&#39;best&#39;)
plt.grid()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post/2019-08-12-python-lasso-regression-auto/output_35_0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;1-8-best-model&#34;&gt;1.8 Best Model&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_lam = pd.DataFrame(test_r_squared*100, columns=[&#39;R_squared&#39;])
df_lam[&#39;lambda&#39;] = (lambdas)
# returns the index of the row where column has maximum value.
df_lam.loc[df_lam[&#39;R_squared&#39;].idxmax()]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;R_squared    87.897525
lambda        0.050000
Name: 0, dtype: float64
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Best Model
reg_best = Lasso(alpha = 0.144737)
reg_best.fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Lasso(alpha=0.144737, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False, random_state=None,
   selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import mean_squared_error

mean_squared_error(y_test, reg_best.predict(X_test))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;3.635187490993961
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reg_best.coef_
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([-0.34136411, -1.18223273, -0.        , -3.27132984,  0.        ,
        0.33262331,  0.71385488])
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Correlation Tutorial with R</title>
      <link>/project/r-correlation-tutorial/</link>
      <pubDate>Sun, 11 Aug 2019 05:00:00 +0000</pubDate>
      
      <guid>/project/r-correlation-tutorial/</guid>
      <description>&lt;p&gt;Correlation is a way of measuring the extent to which two variables are related. This means we need to analyze whether as one variable increases, the other&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(1) increases,&lt;/li&gt;
&lt;li&gt;(2) decreases or&lt;/li&gt;
&lt;li&gt;(3) stays the same.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This can be done by calculating the covariance or correlation of two variables.&lt;/p&gt;

&lt;p&gt;In this &lt;a href=&#34;https://github.com/kirenz/correlation/blob/master/Correlation.md&#34; target=&#34;_blank&#34;&gt;Correlation Tutorial in R&lt;/a&gt;, we  use a small dataset to illustrate the concepts of covariance and correlation. You may also download the &lt;a href=&#34;https://github.com/kirenz/correlation/blob/master/Correlation.Rmd&#34; target=&#34;_blank&#34;&gt;Rmarkdown file&lt;/a&gt; and open it in RStudio.&lt;/p&gt;

&lt;p&gt;Check your understanding with &lt;a href=&#34;https://github.com/kirenz/correlation/blob/master/Correlation_task.pdf&#34; target=&#34;_blank&#34;&gt;multiple choice tasks&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lasso Regression with Python</title>
      <link>/project/r-lasso-regression/</link>
      <pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/r-lasso-regression/</guid>
      <description>

&lt;h1 id=&#34;lasso-regression&#34;&gt;Lasso Regression&lt;/h1&gt;

&lt;p&gt;In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces (&lt;a href=&#34;https://en.wikipedia.org/wiki/Lasso_(statistics)&#34; target=&#34;_blank&#34;&gt;Wikipedia&lt;/a&gt;).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Lasso Regression with Python (Auto Data): &lt;a href=&#34;https://github.com/kirenz/lasso-regression/blob/master/python-lasso-regression-auto.ipynb&#34; target=&#34;_blank&#34;&gt;Jupyter Notebook&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Time Series Analysis with Python</title>
      <link>/project/python-time-series/</link>
      <pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/python-time-series/</guid>
      <description>

&lt;h1 id=&#34;time-series-analysis-with-python&#34;&gt;Time Series Analysis with Python&lt;/h1&gt;

&lt;p&gt;Time series analysis can be used in a multitude of business applications for forecasting a quantity into the future and explaining its historical patterns. Here are just a few examples of possible use cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Explaining seasonal patterns in sales&lt;/li&gt;
&lt;li&gt;Predicting the expected number of incoming or churning customers&lt;/li&gt;
&lt;li&gt;Estimating the effect of a newly launched product on number of sold units&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Introduction to Time Series Analysis with Python: Fit &lt;strong&gt;ARIMA&lt;/strong&gt; and &lt;strong&gt;SARIMAX-Models&lt;/strong&gt; with &lt;code&gt;Statsmodel&lt;/code&gt;: &lt;a href=&#34;https://github.com/kirenz/time-series-analysis/blob/master/time-series-first-steps.ipynb&#34; target=&#34;_blank&#34;&gt;Jupyter Notebook&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Introduction to Facebook&amp;rsquo;s time series analysis modul &lt;strong&gt;Prophet&lt;/strong&gt;: &lt;a href=&#34;https://github.com/kirenz/time-series-analysis/blob/master/Prophet.ipynb&#34; target=&#34;_blank&#34;&gt;Jupyter Notebook&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Deskriptive Statistik in R</title>
      <link>/post/2019-08-01-r-descriptive-statistics/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-08-01-r-descriptive-statistics/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#deskriptive-statistik-in-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Deskriptive Statistik in R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#datenimport&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1&lt;/span&gt; Datenimport&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deskriptive-statistiken&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2&lt;/span&gt; Deskriptive Statistiken&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mittelwert&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.1&lt;/span&gt; Mittelwert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standardabweichung&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.2&lt;/span&gt; Standardabweichung&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getrimmter-mittelwert&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.3&lt;/span&gt; Getrimmter Mittelwert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#schiefe&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.4&lt;/span&gt; Schiefe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kurtosis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.5&lt;/span&gt; Kurtosis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standardfehler&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2.6&lt;/span&gt; Standardfehler&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;deskriptive-statistik-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Deskriptive Statistik in R&lt;/h1&gt;
&lt;p&gt;In diesem Beitrag wird die Berechnung einfacher deskriptiver Statistiken und die Visualisierung von Verteilungen in R am Beispiel des Datensatzes “Advertising” behandelt.&lt;/p&gt;
&lt;div id=&#34;datenimport&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Datenimport&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Datensatz: Advertising.csv&lt;/li&gt;
&lt;li&gt;Variablen: &lt;em&gt;TV&lt;/em&gt;, &lt;em&gt;radio&lt;/em&gt;, &lt;em&gt;newspaper&lt;/em&gt; = jeweils Werbeausgaben in Dollar; &lt;em&gt;sales&lt;/em&gt; = Produkte in Tausend Einheiten&lt;/li&gt;
&lt;li&gt;Abhängige Variable (dependent variable, response): &lt;em&gt;sales&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Unabhängige Variablen (independent variables, predictors): &lt;em&gt;TV&lt;/em&gt;, &lt;em&gt;radio&lt;/em&gt;, &lt;em&gt;newspaper&lt;/em&gt;, &lt;em&gt;sales&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Zunächts möchten wir uns einen Überblick über die Daten verschaffen. Dafür importieren wir die Daten und prüfen, ob die Skalenniveaus korrekt sind. Für die weiteren Berechnungen wird die Variable X1 nicht benötigt, weshalb wir diese löschen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
# Daten importieren
Advertising &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/kirenz/datasets/master/advertising.csv&amp;quot;)
# Überblick über die Daten verschaffen (Skalenniveaus prüfen)
head(Advertising)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##      X1    TV radio newspaper sales
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 230.   37.8      69.2  22.1
## 2     2  44.5  39.3      45.1  10.4
## 3     3  17.2  45.9      69.3   9.3
## 4     4 152.   41.3      58.5  18.5
## 5     5 181.   10.8      58.4  12.9
## 6     6   8.7  48.9      75     7.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Bereinigung der Daten
Advertising$X1 &amp;lt;- NULL&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;deskriptive-statistiken&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Deskriptive Statistiken&lt;/h2&gt;
&lt;p&gt;Ausgabe unterschiedlicher deskriptiver Statistiken:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)

psych::describe(Advertising) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           vars   n   mean    sd median trimmed    mad min   max range
## TV           1 200 147.04 85.85 149.75  147.20 108.82 0.7 296.4 295.7
## radio        2 200  23.26 14.85  22.90   23.00  19.79 0.0  49.6  49.6
## newspaper    3 200  30.55 21.78  25.75   28.41  23.13 0.3 114.0 113.7
## sales        4 200  14.02  5.22  12.90   13.78   4.82 1.6  27.0  25.4
##            skew kurtosis   se
## TV        -0.07    -1.24 6.07
## radio      0.09    -1.28 1.05
## newspaper  0.88     0.57 1.54
## sales      0.40    -0.45 0.37&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Hinweise zu den Kennzahlen:
&lt;ul&gt;
&lt;li&gt;vars: Nummer der Variable&lt;/li&gt;
&lt;li&gt;n: Anzahl der Beobachtungen&lt;/li&gt;
&lt;li&gt;mean: arithmetischer Mittelwert&lt;/li&gt;
&lt;li&gt;sd: empirische Standardabweichung&lt;/li&gt;
&lt;li&gt;median: Median&lt;/li&gt;
&lt;li&gt;trimmed: getrimmter Mittelwert&lt;/li&gt;
&lt;li&gt;mad: Mittlere absolute Abweichung vom Median&lt;/li&gt;
&lt;li&gt;min: kleinster Beobachtungswert&lt;/li&gt;
&lt;li&gt;max: größter Beobachtungswert&lt;/li&gt;
&lt;li&gt;range: Spannweite&lt;/li&gt;
&lt;li&gt;skew: Schiefe&lt;/li&gt;
&lt;li&gt;kurtosis: Wölbung&lt;/li&gt;
&lt;li&gt;se = Standardfehler&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;mittelwert&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.1&lt;/span&gt; Mittelwert&lt;/h3&gt;
&lt;p&gt;Bei der Berechnung des &lt;em&gt;arithmetischen Mittelwerts&lt;/em&gt; in R sollte immer die Anweisung gegeben werden, fehlende Werte auszuschließen (na.rm = “remove values which are not available”). Ansonsten stoppt R bei fehlenden Werten die Berechnung und gibt eine Fehlermeldung aus.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_sales &amp;lt;- mean(Advertising$sales, na.rm = TRUE)
print(paste0(&amp;quot;Mittelwert der Variable Sales: &amp;quot;, mean_sales))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Mittelwert der Variable Sales: 14.0225&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;standardabweichung&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.2&lt;/span&gt; Standardabweichung&lt;/h3&gt;
&lt;p&gt;Die Standardabweichung ist ein häufig verwendetes Streuungsmaß und beschreibt die mittlere Abweichung der einzelnen Messwerte vom empirischen Mittelwert. Die Standardabweichung ist die positive Wurzel der empirischen Varianz. Die Varianz einer Stichprobe wird wie folgt berechnet:
&lt;span class=&#34;math display&#34;&gt;\[s^{2} = \frac{\sum_{i=1}^{n} \left(x_{i} - \bar{x}\right)^{2}} {n-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Berechnung der Standardabweichung: &lt;span class=&#34;math display&#34;&gt;\[s = \sqrt{\frac{\sum\limits_{i=1}^{n} \left(x_{i} - \bar{x}\right)^{2}} {n-1}}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var_sales &amp;lt;- var(Advertising$sales, na.rm = TRUE)
print(paste0(&amp;quot;Varianz der Variable Sales: &amp;quot;, round(var_sales, 2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Varianz der Variable Sales: 27.22&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd_sales &amp;lt;-  sd(Advertising$sales, na.rm = TRUE)
print(paste0(&amp;quot;Standardabweichung der Variable Sales: &amp;quot;, round(sd_sales,2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Standardabweichung der Variable Sales: 5.22&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;getrimmter-mittelwert&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.3&lt;/span&gt; Getrimmter Mittelwert&lt;/h3&gt;
&lt;p&gt;Bei dem &lt;em&gt;getrimmten Mittelwert&lt;/em&gt; wird ein bestimmer Anteil der größten und kleinsten Beobachtungen - hier oberhalb des 90% Quantils und unterhalb des 10 % Quantils - ignoriert. Damit sollen Ausreißer aus der Berechnung des Mittelwerts ausgeschlossen werden. Der getrimmte Mittelwert kann wie folgt in R berechnet werden:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_trim_sales &amp;lt;- mean(Advertising$sales, trim = 0.1, na.rm = TRUE)
print(paste0(&amp;quot;Getrimmter Mittelwert der Variable Sales: &amp;quot;, round(mean_trim_sales, 2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Getrimmter Mittelwert der Variable Sales: 13.78&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;schiefe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.4&lt;/span&gt; Schiefe&lt;/h3&gt;
&lt;p&gt;Die &lt;em&gt;Schiefe&lt;/em&gt; ist eine statistische Kennzahl, die die Art und Stärke der Asymmetrie einer Wahrscheinlichkeitsverteilung beschreibt. Sie zeigt an, ob und wie stark die Verteilung nach rechts (positive Schiefe) oder nach links (negative Schiefe) geneigt ist. Jede nicht symmetrische Verteilung heißt schief.&lt;/p&gt;
&lt;p&gt;Darstellung der Verteilung in einem Histogramm:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
# Vorlage für die Erstellung von plots in ggplot2 
plot_1 &amp;lt;-  theme_bw() +
        theme(axis.text.x = element_text(angle = 0, size = 8, family=&amp;quot;Arial&amp;quot;, colour=&amp;#39;black&amp;#39;),
        axis.text.y = element_text(angle = 0, size = 8, family=&amp;quot;Arial&amp;quot;, colour=&amp;#39;black&amp;#39;),
        axis.title = element_text(size=8, face=&amp;quot;bold&amp;quot;, family=&amp;quot;Arial&amp;quot;, colour=&amp;#39;black&amp;#39;),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title=element_text(hjust=0, size=10,  family=&amp;quot;Arial&amp;quot;, face=&amp;quot;bold&amp;quot;, colour=&amp;#39;black&amp;#39;))

ggplot(Advertising, aes(sales)) +
  geom_histogram(binwidth = 2, color=&amp;quot;red&amp;quot;, alpha=.2) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  labs(title=&amp;quot;Histogramm für Sales&amp;quot;, x=&amp;quot;Sales&amp;quot;, y=&amp;quot;Anzahl&amp;quot;) +
  plot_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-01-r-descriptive-statistics/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Darstellung der Verteilung in einer Dichtefunktion:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

ggplot(Advertising, aes(sales)) +
  geom_density(fill=&amp;quot;grey&amp;quot;,alpha=.2 ) +
  geom_vline(aes(xintercept=mean(sales, na.rm=TRUE)), color=&amp;quot;red&amp;quot;, linetype=&amp;quot;dotted&amp;quot;, size=0.6) +
  geom_vline(aes(xintercept=median(sales, na.rm=TRUE)), color=&amp;quot;red&amp;quot;, linetype=&amp;quot;dotted&amp;quot;, size=0.6) +
  geom_text(aes(x=median(sales), y=0.02), colour = &amp;quot;grey&amp;quot;, size =3,  
             label=round(mean(Advertising$sales), digits=2), hjust=-1, family=&amp;quot;Arial&amp;quot;) +
  geom_text(aes(x=mean(sales), y=0.02), hjust=-0.7, colour = &amp;quot;grey&amp;quot;, size = 3, label=&amp;quot;Mittelwert&amp;quot;, family=&amp;quot;Arial&amp;quot;) +
  geom_text(aes(x=median(sales), y=0.005), colour = &amp;quot;grey&amp;quot;, size =3, 
             label=round(median(Advertising$sales), digits=2), hjust=1 , family=&amp;quot;Arial&amp;quot;) +
  geom_text(aes(x=median(sales), y=0.01), colour = &amp;quot;grey&amp;quot;, size = 3, label=&amp;quot;Median&amp;quot;, hjust=1, family=&amp;quot;Arial&amp;quot;) +
  labs(x=&amp;quot;Produktabsatz (in Tausend Einheiten)&amp;quot;, y = &amp;quot;Dichte&amp;quot;, title = &amp;quot;Wahrscheinlichkeitsdichtefunktion&amp;quot;) +
  plot_1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-01-r-descriptive-statistics/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In der Abbildung kann man erkennen, dass es sich um eine asymmetrische Verteilung handelt (d.h. es liegt eine Abweichung von der Normalverteilung vor). Konkret handelt es sich um eine rechtsschiefe Verteilung (Mittelwert &amp;gt; Median; Schiefe = + 0.40).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kurtosis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.5&lt;/span&gt; Kurtosis&lt;/h3&gt;
&lt;p&gt;Die Abweichung des Verlaufs einer Verteilung vom Verlauf einer Normalverteilung wird &lt;em&gt;Kurtosis&lt;/em&gt; (Wölbung) genannt. Sie gibt an, wie spitz die Kurve verläuft. Unterschieden wird zwischen positiver, spitz zulaufender (leptokurtische Verteilung) und negativer, flacher (platykurtische Verteilung) Kurtosis. Die Kurtosis zählt zu den zentralen Momenten einer Verteilung, mittels derer der Kurvenverlauf definiert wird. Eine Kurtosis mit Wert 0 ist normalgipflig (mesokurtisch), ein Wert größer 0 ist steilgipflig und ein Wert unter 0 ist flachgipflig.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standardfehler&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.6&lt;/span&gt; Standardfehler&lt;/h3&gt;
&lt;p&gt;Der &lt;em&gt;Standardfehler&lt;/em&gt; ein Maß für die durchschnittliche Abweichung des geschätzten Parameterwertes vom wahren Parameterwert. Je kleiner der Standardfehler ist, desto genauer kann der unbekannte Parameter der Population mit Hilfe der Schätzfunktion geschätzt werden. Der Standardfehler hängt unter anderem von dem Stichprobenumfang und der Varianz ab. Allgemein gilt: Je größer der Stichprobenumfang, desto kleiner der Standardfehler; je kleiner die Varianz, desto kleiner der Standardfehler.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression Tutorial with Python</title>
      <link>/project/python-linear-regression/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/python-linear-regression/</guid>
      <description>

&lt;h1 id=&#34;linear-regression-tutorial-in-python&#34;&gt;Linear Regression Tutorial in Python&lt;/h1&gt;

&lt;p&gt;Linear regression is the fundamental starting point for all regression methods. In this Jupyter Notebook, we fit a regression model in Python and take a closer look at the following topics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Histogramms&lt;/li&gt;
&lt;li&gt;Boxplots&lt;/li&gt;
&lt;li&gt;Mean&lt;/li&gt;
&lt;li&gt;Standard deviation&lt;/li&gt;
&lt;li&gt;Mean squared error&lt;/li&gt;
&lt;li&gt;$R^2$&lt;/li&gt;
&lt;li&gt;Pearson&amp;rsquo;s correlation coefficient&lt;/li&gt;
&lt;li&gt;F-Statistic&lt;/li&gt;
&lt;li&gt;Standard error&lt;/li&gt;
&lt;li&gt;Confidence interval&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Download the linear regression tutorial Jupyter Notebook in &lt;a href=&#34;https://github.com/kirenz/linear-regression/blob/master/python-regression-tutorial.ipynb&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; and open the file in Jupyter Notebook.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Essential books for Data Science &amp; Statistics with R</title>
      <link>/project/r-data-science-stats/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/r-data-science-stats/</guid>
      <description>&lt;p&gt;&amp;ldquo;&lt;strong&gt;R for Data Science&lt;/strong&gt;&amp;rdquo; offers an excellent introduction into data science in R with a focus on the popular package collection &lt;a href=&#34;https://www.tidyverse.org&#34; target=&#34;_blank&#34;&gt;tidyverse&lt;/a&gt;. See how the &lt;em&gt;tidyverse&lt;/em&gt; makes data science faster, easier and more fun:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r4ds.had.co.nz&#34; target=&#34;_blank&#34;&gt;Wickham, H., &amp;amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O&amp;rsquo;Reilly Media, Inc.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;ldquo;&lt;strong&gt;An Introduction to Statistical Learning&lt;/strong&gt;&amp;rdquo; provides an accessible overview of the field of statistical learning with applications in R. This book presents important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.statlearning.com&#34; target=&#34;_blank&#34;&gt;James, G., Witten, D., Hastie, T., &amp;amp; Tibshirani, R. (2013). An introduction to statistical learning with applications in R (Corr. 7th printing 2017). New York: Springer.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;ldquo;&lt;strong&gt;Statistical Thinking for the 21 Century&lt;/strong&gt;&amp;rdquo; and &amp;ldquo;&lt;strong&gt;Modern Dive: Statistical Inference via Data Science&lt;/strong&gt;&amp;rdquo; are both open-source digital textbooks which provide a great introduction into the fundamentals of modern quantitative methods which take advantage of today’s increased computing power to solve statistical problems with R:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://statsthinking21.org&#34; target=&#34;_blank&#34;&gt;Poldrack, R. A. (2019). Statistical Thinking for the 21 Century. http://thinkstats.org.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://moderndive.com/index.html#sec:intro-for-students&#34; target=&#34;_blank&#34;&gt;Ismay, C. &amp;amp; Kim, A. Y. (2019). Modern Dive: Statistical Inference via Data Science. https://moderndive.com&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
