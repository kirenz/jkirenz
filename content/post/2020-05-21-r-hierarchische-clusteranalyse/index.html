---
#aliases: []
#projects: []
title: Hierarchische Clusteranalyse mit Ward in R
authors: [jan]
date: '2020-05-14'
tags: [Unsupervised Learning, R]
categories:
  - R
summary: Hierarchische Clusteranalyse mit Ward in R
description: Grundlagen der hierarchischen Clusteranalyse mit Ward in R. Mit Erläuterungen zur euklidischen Distanz und Manhattan Metrik.
image:
  caption: '[Photo by Markus Spiske on Unsplash](https://unsplash.com/photos/qjnAnF0jIGk)'
  focal_point: ''
output:
  blogdown::html_page:
    toc: true
    number_sections: true
    toc_depth: 3
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#grundlagen"><span class="toc-section-number">1</span> Grundlagen</a></li>
<li><a href="#proximitätsmaß"><span class="toc-section-number">2</span> Proximitätsmaß</a>
<ul>
<li><a href="#euklidische-distanz"><span class="toc-section-number">2.1</span> Euklidische Distanz</a></li>
<li><a href="#quadrierte-euklidische-distanz"><span class="toc-section-number">2.2</span> Quadrierte euklidische Distanz</a></li>
<li><a href="#l_1-distanz"><span class="toc-section-number">2.3</span> <span class="math inline">\(L_1\)</span>-Distanz</a></li>
</ul></li>
<li><a href="#clustering-algorithmus"><span class="toc-section-number">3</span> Clustering-Algorithmus</a></li>
<li><a href="#implementierung-in-r"><span class="toc-section-number">4</span> Implementierung in R</a>
<ul>
<li><a href="#datenvorbereitung"><span class="toc-section-number">4.1</span> Datenvorbereitung</a>
<ul>
<li><a href="#variablenauswahl"><span class="toc-section-number">4.1.1</span> Variablenauswahl</a></li>
<li><a href="#fehlende-werte"><span class="toc-section-number">4.1.2</span> Fehlende Werte</a></li>
<li><a href="#standardisierung"><span class="toc-section-number">4.1.3</span> Standardisierung</a></li>
</ul></li>
<li><a href="#proximitätsmaß-1"><span class="toc-section-number">4.2</span> Proximitätsmaß</a></li>
<li><a href="#hierarchische-clusteranalyse"><span class="toc-section-number">4.3</span> Hierarchische Clusteranalyse</a></li>
<li><a href="#dendrogramm"><span class="toc-section-number">4.4</span> Dendrogramm</a></li>
</ul></li>
</ul>
</div>

<p><em>In diesem Tutorial werden die Grundlagen der Clusteranalyse beschrieben und die hierarchische Clusteranalyse mit der Ward-Methode in R umgesetzt.</em></p>
<div id="grundlagen" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Grundlagen</h1>
<p>Die Clusteranalyse ist ein exploratives Verfahren um Ähnlichkeitsstrukturen in Daten zu erkennen. Bei den Untersuchungsobjekten einer Clusteranalyse kann es sich sowohl um Personen, Produkte oder um beliebige andere Einheiten wie Filme, Länder oder Unternehmen handeln. Durch die Anwendung der Clusteranalyse können diese Objekte anhand ihrer Eigenschaftsausprägungen zu Clustern zusammengefasst werden. Dabei soll jedes Cluster in sich möglichst gleichartig (homogen) sein und sich gleichzeitig von den anderen Clustern möglichst stark unterscheiden (heterogen).
Beispielsweise erfasst der Streaminganbieter Netflix die Sehgewohnheiten seiner Abonnenten und hat auf dieser Grundlage über 2000 Mikro-Cluster, sogenannte “Taste Communities”, gebildet (<a href="https://www.vulture.com/2018/06/how-netflix-swallowed-tv-industry.html">New York Magazine, 2018</a>). Den Mitgliedern der jeweiligen Clustern sollen anhand der jeweiligen Clusterzugehörigkeit möglichst passende Inhalte vorgeschlagen werden. Die Filme können dabei ebenfalls anhand unterschiedlicher Merkmale geclustert und im Anschluss mit aussagekräftigen Bezeichnungen versehen werden:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="netflix.png" alt="Zusammenhänge zwischen verschiedenen Serien, dargestellt in Clustern (Quelle: [Netflix, 2017](https://media.netflix.com/de/press-releases/decoding-the-defenders-netflix-unveils-the-gateway-shows-that-lead-to-a-heroic-binge))" width="70%" />
<p class="caption">
Figure 1.1: Zusammenhänge zwischen verschiedenen Serien, dargestellt in Clustern (Quelle: <a href="https://media.netflix.com/de/press-releases/decoding-the-defenders-netflix-unveils-the-gateway-shows-that-lead-to-a-heroic-binge">Netflix, 2017</a>)
</p>
</div>
<p>Wichtige Voraussetzungen, die bei der Durchführung der Analyse beachtet werden sollten <a href="https://www.methodenberatung.uzh.ch/de/datenanalyse_spss/interdependenz/gruppierung/cluster.html">(Universität Zürich, 2018)</a>:</p>
<ul>
<li>Die Analyse kann für unterschiedliche Datentypen (kategoriale und metrische Daten) genutzt werden.</li>
<li>Fehlende Werte und Ausreißerwerte sollten vorab beseitigt werden.</li>
<li>Weisen die verwendeten Variablen große Unterschiede bezüglich ihres Wertebereichs auf (bspw. wenn eine Variable in cm und die andere in km gemessen wurde), so sollten diese auf ein einheitliches Niveau transformiert werden. Üblicherweise wird dafür die z-Transformation genutzt.</li>
</ul>
<p>Bei der Berechnung der Cluster wird nach bestimmten Regeln entschieden, wie die Objekte zu Clustern zusammengefasst werden. Das Ergebnis dieses Prozesses hängt nicht nur von der Wahl des Clustering-Algorithmus ab, sondern auch davon, wie die Distanz oder Ähnlichkeit zwischen den Objekten bestimmt wird.</p>
<p>Zu Beginn der Clusteranalyse wird daher in Abhängigkeit von den vorliegenden Datentypen ein sogenanntes <em>Proximitätsmaß</em> gewählt.</p>
</div>
<div id="proximitätsmaß" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Proximitätsmaß</h1>
<p>Mit Hilfe des Proximitätsmaßes wird die Distanz zwischen den Objekten berechnet. In Abhängigkeit von dem Skalenniveau der Variablen wird eine Distanzfunktion zur Bestimmung des Abstandes (Distanz) zweier Elemente oder eine Ähnlichkeitsfunktion zur Bestimmung der Ähnlichkeit verwendet:</p>
<ul>
<li><p>Bei kategorialen (nominalen und ordinalen) Variablen werden Ähnlichkeitsmaße benutzt.</p></li>
<li><p>Bei metrischen Variablen werden Distanzmaße genutzt.</p></li>
</ul>
<p>In diesem Tutorial behandeln wir die Distanzmaße “euklidische Distanz” (auch <span class="math inline">\(L_2\)</span> genannt), “quadrierte euklidische Distanz” und die “L1-Distanz” (auch Manhattan-Metrik, Manhattan-Distanz, Mannheimer Metrik, Taxi- oder Cityblock-Metrik geannt).</p>
<div id="euklidische-distanz" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Euklidische Distanz</h2>
<p>Mit Hilfe der euklidischen Distanz kann der Abstand zwischen zwei Punkten als gerade Linie in einem Raum berechnet werden (“Luftliniendistanz”). Anders formuliert ist der euklidische Abstand zweier Punkte die mit einem Lineal gemessene Länge einer Strecke, die diese zwei Punkte verbindet. Ein Distanzwert von Null bedeutet dabei, dass die Objekte einen Abstand von Null aufweisen, also identisch sind.</p>
<p>Die Formel für die Berechnung der euklidischen Distanz für <span class="math inline">\(n\)</span> verschiedenen Variablen lautet:</p>
<p><span class="math display">\[d(A,B) = \sqrt{\sum_{i=1}^{n}(A_i - B_i)^2}\]</span>
Die Formel kann in einem zweidimensionalen Koordinatensystem mit den beiden Variablen <span class="math inline">\(X\)</span> und <span class="math inline">\(Y\)</span> (d.h. n = 2) wie folgt visualisiert werden:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="https://miro.medium.com/max/1524/1*J2bK-UKhrW1Ill5EyAxXOQ.png" alt="Die euklidische Distanz von Punkt A zu Punkt B  (Quelle: [Korstanje, 2019](https://towardsdatascience.com/3-distances-that-every-data-scientist-should-know-59d864e5030a))" width="50%" />
<p class="caption">
Figure 2.1: Die euklidische Distanz von Punkt A zu Punkt B (Quelle: <a href="https://towardsdatascience.com/3-distances-that-every-data-scientist-should-know-59d864e5030a">Korstanje, 2019</a>)
</p>
</div>
<p>Wie aus dem Punktediagramm entnommen werden kann, gelten für die Punkte A und B:</p>
<ul>
<li><span class="math inline">\(x_A\)</span> = 70</li>
<li><span class="math inline">\(x_B\)</span> = 330</li>
<li><span class="math inline">\(y_A\)</span> = 40</li>
<li><span class="math inline">\(y_B\)</span> = 228</li>
</ul>
<p>Da wir in diesem Beispiel nur 2 Variablen vorliegen haben (n = 2), gilt hier ein bekannter Spezialfall der Berechnung des euklidischen Abstandes: der Satz des Pythagoras. Für die Berechnung der euklidischen Distanz werden daher lediglich die (X,Y)-Koordinaten benötigt um mit Hilfe der Formel von Pythagoras die Distanz zu berechnen:</p>
<p><span class="math display">\[d(A,B) = \sqrt{(x_A-x_B)^2 + (y_A-y_B)^2}\]</span></p>
<p><span class="math display">\[d(A,B) = \sqrt{(70-330)^2 + (40-228)^2}\]</span></p>
<p><span class="math display">\[d(A,B) = \sqrt{(-260)^2 + (-188)^2}\]</span></p>
<p><span class="math display">\[d(A,B) = \sqrt{(76600 + 35344) }\]</span></p>
<p><span class="math display">\[d(A,B) = \sqrt{(112225) }\]</span></p>
<p><span class="math display">\[d(A,B) = 335\]</span></p>
</div>
<div id="quadrierte-euklidische-distanz" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Quadrierte euklidische Distanz</h2>
<p>Anstelle der einfachen euklidischen Distanz kann auch die quadrierte euklidische Distanz als Distanzmaß genutzt werden. Dadurch werden größere Abweichungen stärker gewichtet. Die Formel der quadrierten euklidischen Distanz lautet:</p>
<p><span class="math display">\[d^2(A,B) = \sum_{i=1}^{n}(A_i - B_i)^2\]</span></p>
<p>Für unser Datenbeispiel gilt daher:</p>
<p><span class="math display">\[d^2(A,B) = (x_A-x_B)^2 + (y_A-y_B)^2\]</span></p>
<p><span class="math display">\[d^2(A,B) = (70-330)^2 + (40-228)^2\]</span></p>
<p><span class="math display">\[d^2(A,B) = 112225\]</span></p>
</div>
<div id="l_1-distanz" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> <span class="math inline">\(L_1\)</span>-Distanz</h2>
<p>Die <span class="math inline">\(L_1\)</span>-Distanz (auch Manhattan-Metrik, Manhattan-Distanz, Mannheimer Metrik, Taxi- oder Cityblock-Metrik) ist eine Metrik, in der die Distanz <span class="math inline">\(d\)</span> zwischen zwei Punkten <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> als die Summe der absoluten Differenzen ihrer Einzelkoordinaten definiert wird. Dies ist insbesondere bei der Berechnung von geografischen Abständen relevant, bei welchen der Abstand zwischen zwei Punkten über vordefinierte Wege (bspw. Straßen in einer Stadt mit einer blockartigen Struktur wie in Manhattan oder Mannheim) zurückgelegt werden muss.</p>
<div class="figure" style="text-align: center"><span id="fig:manhattan-distance"></span>
<img src="https://miro.medium.com/max/1400/1*88uZae0Utf7kavhQFvMqaw.png" alt="Die L1 Distanz von Punkt A zu Punkt B als Manhatten-Metrik. [(Quelle: Korstanje, 2019](https://towardsdatascience.com/3-distances-that-every-data-scientist-should-know-59d864e5030a))" width="50%" />
<p class="caption">
Figure 2.2: Die L1 Distanz von Punkt A zu Punkt B als Manhatten-Metrik. <a href="https://towardsdatascience.com/3-distances-that-every-data-scientist-should-know-59d864e5030a">(Quelle: Korstanje, 2019</a>)
</p>
</div>
<p>Wie aus der Abbildung ersichtlich wird, existieren mehrere Möglichkeiten, den Abstand zwischen den Punkten A und B zu berechnen. Wichtig ist jedoch, dass die “Straßen” nicht verlassen werden dürfen. D.h. es können bspw. zwei Blöcke nach oben (Norden) und dann drei Blöcke nach rechts (Osten) auf der Fahrbahn zurückgelegt werden, um von Punkt A aus Punkt B zu erreichen. Unabhängig von dem gewählten Pfad ist die Distanz aufgrund der blockartigen Struktur immer die gleiche.</p>
<p>Allgemein lautet die Formel für die Berechnung des L1-Abstands wie folgt:</p>
<p><span class="math display">\[d(A,B) = \sum_{i} |A_i - B_i|\]</span></p>
<p>In unserem Fall gilt:</p>
<p><span class="math display">\[d(A,B) = |x_A - x_B| + |y_A - y_B |\]</span></p>
<p><span class="math display">\[d(A,B) = |70 - 330| + |40 - 228 |\]</span></p>
<p><span class="math display">\[d(A,B) = |-260 | + |-188|\]</span></p>
<p><span class="math display">\[d(A,B) = 260 + 188\]</span></p>
<p><span class="math display">\[d(A,B) = 448 \]</span></p>
</div>
</div>
<div id="clustering-algorithmus" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Clustering-Algorithmus</h1>
<p>Ist das Proximitätsmaß berechnet, so wird anhand eines Clustering-Algorithmus die eigentliche Gruppierung der Daten vorgenommen. In dieser Abbildung sind beispielhaft einige Clustering-Algorithmen aufgeführt:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="https://www.methodenberatung.uzh.ch/dam/jcr:ffffffff-81eb-fc79-0000-000008e2c10d/Clus_Abb_04.jpg" alt="Überblick über Clustering-Algorithmen (Quelle: [Universität Zürich,  2018](https://www.methodenberatung.uzh.ch/de/datenanalyse_spss/interdependenz/gruppierung/cluster.html))" width="80%" />
<p class="caption">
Figure 3.1: Überblick über Clustering-Algorithmen (Quelle: <a href="https://www.methodenberatung.uzh.ch/de/datenanalyse_spss/interdependenz/gruppierung/cluster.html">Universität Zürich, 2018</a>)
</p>
</div>
<p>Bei den hier dargestellten Algorithmen wird zwischen <strong>hierarchischen</strong> und <strong>nicht-hierarchischen</strong> Algorithmen unterschieden. Im Rahmen dieses Tutorials werden ausschließlich hierarchische Algorithmen behandelt. Diese werden weiter in agglomerative und divisive Verfahren unterteilt.</p>
<p>Bei <strong>divisiven</strong> Verfahren wird zunächst ein Cluster gebildet, welches alle Datenpunkte enthält. Dieses wird dann schrittweise in kleinere Cluster zerteilt, bis jeder Fall ein eigenes Cluster bildet. Bei <strong>agglomerativen</strong> Verfahren werden die Datenpunkte zuerst einzeln betrachtet (d.h. jeder Fall ist ein eigenes Cluster) und dann schrittweise zu größeren Clustern zusammengefasst. Die agglomerativen Verfahren werden in Linkage-Methoden und Varianz-Methoden unterteilt.</p>
<p>Bei den <strong>Linkage-Methoden</strong> wird in jedem Schritt nach einer bestimmten Logik geprüft, welche Cluster den geringsten Abstand zueinander aufweisen. Diese Cluster werden dann zu einem neuen Cluster fusioniert. Je nach Linkage-Methode wird diese Distanz zwischen den Clustern unterschiedlich bestimmt <a href="https://www.methodenberatung.uzh.ch/de/datenanalyse_spss/interdependenz/gruppierung/cluster.html">(Universität Zürich, 2018)</a>:</p>
<ul>
<li>Nächstgelegener Nachbar (engl. “<em>single linkage</em>”): Das Minimum aller möglichen Distanzen zwischen den Datenpunkten in Cluster 1 und jenen in Cluster 2 wird betrachtet:</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="SingleLinkage.svg" alt="Single Linkage (Quelle: [Sigbert,  2011](https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#/media/Datei:SingleLinkage.svg))" width="40%" />
<p class="caption">
Figure 3.2: Single Linkage (Quelle: <a href="https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#/media/Datei:SingleLinkage.svg">Sigbert, 2011</a>)
</p>
</div>
<ul>
<li>Entferntester Nachbar (engl. “<em>complete linkage</em>”): Das Maximum aller möglichen Distanzen zwischen den Datenpunkten in Cluster 1 und jenen in Cluster 2 wird betrachtet:</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="CompleteLinkage.svg" alt="Complete Linkage (Quelle: [Sigbert, 2011](https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#/media/Datei:CompleteLinkage.svg))" width="40%" />
<p class="caption">
Figure 3.3: Complete Linkage (Quelle: <a href="https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#/media/Datei:CompleteLinkage.svg">Sigbert, 2011</a>)
</p>
</div>
<ul>
<li>Linkage zwischen Gruppen (engl. “<em>average linkage</em>”): Der Mittelwert aller möglichen Distanzen zwischen den Datenpunkten in Cluster 1 und jenen in Cluster 2 wird betrachtet.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="AverageLinkage.svg" alt="Average Linkage (Quelle: [Sigbert , 2011](https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#/media/Datei:AverageLinkage.svg))" width="40%" />
<p class="caption">
Figure 3.4: Average Linkage (Quelle: <a href="https://de.wikipedia.org/wiki/Hierarchische_Clusteranalyse#/media/Datei:AverageLinkage.svg">Sigbert , 2011</a>)
</p>
</div>
<ul>
<li>Other Linkage: Dies umfasst verschiedene Methoden. Beispielsweise wird die Distanz zwischen dem Median von Cluster 1 und dem Median von Cluster 2 betrachtet (<em>Median-Clustering</em>):</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-7"></span>
<img src="MedianLinkage.svg" alt="Median Linkage (Quelle: [Sigbert,  2011](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/MedianLinkage.svg/300px-MedianLinkage.svg.png))" width="40%" />
<p class="caption">
Figure 3.5: Median Linkage (Quelle: <a href="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/MedianLinkage.svg/300px-MedianLinkage.svg.png">Sigbert, 2011</a>)
</p>
</div>
<p>Neben den Linkage-Methoden exisiteren noch weitere Methoden. Die <strong>Ward-Methode</strong> ist eine beliebte <strong>Varianz-basierte-Methode</strong>. Dabei werden die Cluster, die den kleinsten Zuwachs der totalen Varianz aufweisen, fusioniert. Die Methode ist daher eine Erweiterung der empirischen Varianz einer Variablen auf den multivariaten Fall.</p>
<p>Formel der empirischen Varianz:</p>
<p><span class="math display">\[s^2 = \frac{1}{n} \sum_{i=1}^{n}(x_i - \bar{x})^2\]</span></p>
<p>Formel der totalen Varianz (Streuung eines multivariaten Datensatzes mit <span class="math inline">\(p\)</span> Variablen <span class="math inline">\(X_j\)</span>):</p>
<p><span class="math display">\[T = \frac{1}{n}\sum_{j=1}^{p}  \sum_{i=1}^{n}(x_{ij} - \bar{x_j})^2\]</span></p>
<p>In den Formeln wird ersichtlich, dass <span class="math inline">\((x_i-\bar{x})^2\)</span> mit der bereits bekannten quadrierten euklidischen Distanz <span class="math inline">\(d^2(x_i,\bar{x})\)</span> übereinstimmt. Es wird also für jedes Cluster die Summe der quadrierten Distanzen der Einzelfälle vom jeweiligen Cluster-Mittelwert berechnet. Diese Werte werden dann über alle Variablen <span class="math inline">\(p\)</span> aufsummiert. Im nächsten Schritt werden jeweils jene zwei Cluster fusioniert, deren Zusammenfügen die geringste Erhöhung der Gesamtsumme der quadrierten Distanzen zur Folge hat.</p>
<p>In dieser Abbildung sind die Ergebnisse der verschiedenen Clustering-Algorithmen für unterschiedliche Datensätze exemplarisch dargestellt (Quelle: <a href="https://scikit-learn.org/stable/modules/clustering.html">scikit-learn</a>):</p>
<p><img src="linkage_comparison.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Bei den agglomerativen Verfahren führt das single linkage Verfahren in einigen Fällen zu einer sehr einseitigen Verteilung der Cluster. Die Ward Methode führt dagegen in den meisten Fällen zu einer relativ ausgeglichenen Aufteilung. Im folgenden Beispiel wird ebenfalls die Ward-Methode genutzt.</p>
</div>
<div id="implementierung-in-r" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Implementierung in R</h1>
<p>Für die Durchführung der hierarchischen Clusteranalyse mit der Ward-Methode nutzen wir die Daten des World Happiness Reports aus dem Jahr 2020. Der World Happiness Report ist ein jährlich vom Sustainable Development Solutions Network der Vereinten Nationen veröffentlichter Bericht. Der Bericht enthält Ranglisten zur Lebenszufriedenheit in verschiedenen Ländern der Welt und Datenanalysen aus verschiedenen Perspektiven (siehe <a href="https://worldhappiness.report">Helliwell et al., 2020</a>).</p>
<p>Import der Daten:</p>
<pre class="r"><code>library(tidyverse)

df &lt;- read_csv(&quot;https://raw.githubusercontent.com/kirenz/datasets/master/whr_20.csv&quot;)</code></pre>
<p>In dieser Analyse nutzen wir die landesspezifischen Informationen zu der Lebenserwartung in Jahren (<code>healthy_life_expectancy</code>) und das logarithmierte Bruttoinlandsprodukt pro Einwohner (<code>logged_gdp_per_capita</code>):</p>
<pre class="r"><code>df %&gt;% 
  ggplot(aes(logged_gdp_per_capita, 
             healthy_life_expectancy, 
             label = country_name )) +
  geom_point() +
  geom_text(check_overlap = TRUE,
            vjust = 0, nudge_y = 0.5) +
  theme_classic() +
  ylab(&quot;Lebenserwartung&quot;) +
  xlab(&quot;Bruttoinlandsprodukt pro Einwohner (logarithmiert)&quot;) </code></pre>
<p><img src="/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Damit die Vorgehensweise des hierarchischen Clustering-Algorithmus besser nachvollzogen werden kann, ziehen wir zufällig 20 Länder aus dem Datensatz:</p>
<pre class="r"><code>set.seed(1234)

df &lt;- df %&gt;% 
  sample_n(20)</code></pre>
<p>Darstellung der Länder in einem Punktediagramm:</p>
<pre class="r"><code>df %&gt;% 
  ggplot(aes(logged_gdp_per_capita, 
             healthy_life_expectancy, 
             label = country_name )) +
  geom_point() +
  geom_text(size = 3,
            check_overlap = FALSE,
            vjust = 0, nudge_y = 0.5) +
  theme_classic() +
  ylab(&quot;Lebenserwartung&quot;) +
  xlab(&quot;Bruttoinlandsprodukt pro Einwohner (logarithmiert)&quot;) </code></pre>
<p><img src="/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="datenvorbereitung" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Datenvorbereitung</h2>
<div id="variablenauswahl" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Variablenauswahl</h3>
<p>Wir erzeugen einen neuen Datensatz <code>df_cl</code>, in welchem nur die Variablen enthalten sind, die für die Clusteranalyse genutzt werden sollen. Zusätzlich nutzen wir die Variable <code>country_name</code>, um in einem späteren Schritt die Daten sinnvoll beschriften zu können.</p>
<pre class="r"><code>df_cl &lt;- select(df, c(&quot;country_name&quot;, 
                      &quot;logged_gdp_per_capita&quot;, 
                      &quot;healthy_life_expectancy&quot;))</code></pre>
</div>
<div id="fehlende-werte" class="section level3" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Fehlende Werte</h3>
<p>Wir prüfen, ob in den Daten fehlende Werte vorliegen:</p>
<pre class="r"><code>sum(is.na(df_cl))</code></pre>
<pre><code>## [1] 0</code></pre>
<p>In diesem Datensatz liegen keine fehlenden Werte vor. Falls dies in einem anderen Projekt jedoch der Fall sein sollte, könnten wir diese fehlenden Werte mit dem Befehl <code>drop_na()</code> entfernen:</p>
<pre class="r"><code>df_cl &lt;- drop_na(df_cl)</code></pre>
</div>
<div id="standardisierung" class="section level3" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Standardisierung</h3>
<p>Damit die Werte der Variablen in einem einheitlichen Werteintervall vorliegen, nutzen wir für die Standardisierung der Daten die z-Transformation. Mit Hilfe dieser Standardisierung wird der Mittelwert auf 0 und die Standardabweichung der Variablen auf 1 gesetzt. Die Formel dafür lautet:</p>
<p><span class="math display">\[z = \frac{x - \bar{x}}{s}\]</span></p>
<ul>
<li><span class="math inline">\(\bar{x}\)</span>: Mittelwert der Daten</li>
<li><span class="math inline">\(s\)</span>: Standardabweichung der Daten</li>
</ul>
<p>Wir führen die Standardisierung mit Hilfe des Befehls <code>scale()</code> durch und speichern die neuen Variablen in dem Datensatz ab.</p>
<pre class="r"><code>df_cl$healthy_life_expectancy_sc &lt;-  scale(df_cl$healthy_life_expectancy, 
                                           center = TRUE, 
                                           scale = TRUE)

df_cl$logged_gdp_per_capita_sc &lt;-  scale(df_cl$logged_gdp_per_capita, 
                                         center = TRUE, 
                                         scale = TRUE)</code></pre>
<p>Wie in der Abbildung nachvollzogen werden kann, ändert sich nicht die Position der Länder, sondern lediglich die Einheiten auf der X- und Y-Achse:</p>
<pre class="r"><code>df_cl %&gt;% 
  ggplot(aes(logged_gdp_per_capita_sc, 
             healthy_life_expectancy_sc, 
             label = country_name)) +
  geom_point() +
  geom_text(size = 3,
            check_overlap = FALSE,
            vjust = 0, nudge_y = 0.1) +
  theme_classic() +
  ylab(&quot;Lebenserwartung (z-Werte)&quot;) +
  xlab(&quot;Bruttoinlandsprodukt pro Einwohner (z-Werte)&quot;) </code></pre>
<p><img src="/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="proximitätsmaß-1" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Proximitätsmaß</h2>
<p>Wir nutzen als Proximitätsmaß die euklidische Distanz und speichern das Ergebnis der Funktion <code>dist()</code>, welche die Distanz zwischen allen Ländern berechnet, mit der Bezeichnung <code>d</code> ab. Da wir die Variable <code>country_name</code> nicht mit in die Berechnung einbeziehen möchten, entfernen wir diese in dem <code>select()</code>-Befehl.</p>
<pre class="r"><code>d &lt;- 
  df_cl %&gt;% 
  select(-country_name) %&gt;% 
  dist(method = &quot;euclidean&quot;)</code></pre>
</div>
<div id="hierarchische-clusteranalyse" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Hierarchische Clusteranalyse</h2>
<p>Im nächsten Schritt wird die hierarchische Clusteranalyse mit dem Befehl <code>hclust()</code> angewendet. Dafür übergeben wir der Funktion das Datenobjekt <code>d</code>, welches die euklidischen Distanzen zwischen den Ländern enthält (für weitere Hinweise zu der Funktion, siehe diesen Beitrag auf <a href="https://stats.stackexchange.com/a/109962">stackoverflow</a>).</p>
<pre class="r"><code>hc &lt;- hclust(d, method = &quot;ward.D2&quot;) </code></pre>
<p>Zu Beginn der agglomerativen Cluster-Bildung ist jedes Land in einem eigenen Cluster. Am Ende sind alle Länder in einem gemeinsamen Cluster. Die optimale Clusteranzahl wird dabei nicht von dem Algorithmus bestimmt, sondern muss auf Grundlage weiterer Überlegungen ermittelt werden. Bei der Bestimmung der optimalen Clusteranzahl ist die sogenannte “Cophenetic Distance” und das “Dendogramm” hilfreich.</p>
<p>Zu Beginn der agglomerativen Clusterbildung werden diejenigen Länder fusioniert, welche die geringste Distanz zueinander aufweisen. Diese “geringste Distanz” zwischen zwei Clustern, bei welcher die Zusammeführung stattfindet, kann mit der “Cophenetic Distance” bestimmt werden:</p>
<pre class="r"><code>sort(unique(cophenetic(hc)))</code></pre>
<pre><code>##  [1]  0.4446962  0.4991792  0.6219964  0.8162091  0.9668424  1.2776699
##  [7]  1.5519296  1.8267467  2.2893469  2.3881172  2.7432910  3.0358361
## [13]  3.5850849  4.3343418  4.7705415  4.9156397 14.3947213 14.9659808
## [19] 41.2573679</code></pre>
<p>Die geringste Distanz zwischen zwei Clustern beträgt zu Beginn (wenn jedes Land sein eigenes Cluster darstellt) 0.44. Dies war also der geringste Abstand zwischen zwei Ländern. Danach steigt der Abstand monoton steigend an, da immer unähnlichere Cluster (d.h. mit einem größeren Abstand zueinander) fusioniert werden. Bei der letzten Zusammenführung der Cluster in ein einziges gemeinsames Cluster nimmt die Distanz den Maximalwert von 41 an. Damit die Werte leichter interpretierbar sind, wird der Prozess üblicherweise in einem sogenannten Dendrogramm dargestellt.</p>
</div>
<div id="dendrogramm" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Dendrogramm</h2>
<p>Mit Hilfe des Dendrogramms kann das Ergebnis des Clustering-Algorithmus dargestellt werden. Das Dendrogramm liest sich dabei von unten nach oben und beschreibt in diese Richtung den Prozess des Clusterings. Die vertikale Achse beschreibt die Heterogenität der Cluster mit der bereits erwähnten “Cophenetic Distance” (die in der Abbildung als <code>Height</code> bezeichnet wird). Auf der unteren Seite des Dendrogramms sind alle Fälle einzeln aufgelistet. Zunächst entspricht also jedes Land einem Cluster, was sich daran zeigt, dass jeder Fall eine eigene horizontale Linie aufweist. Diese Cluster werden von unten nach oben sukzessive zu größeren Clustern zusammengefügt. Die vertikalen Linien zeigen an, dass zwei Cluster fusioniert werden.</p>
<p>Darstellung des Dendrogramms:</p>
<pre class="r"><code>plot(hc) </code></pre>
<p><img src="/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-21-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Nutzung der Ländernamen als Labels in dem Dendrogramm:</p>
<pre class="r"><code>hc$labels &lt;- df$country_name

plot(hc)</code></pre>
<p><img src="/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-22-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Die “optimale” Anzahl der Cluster sollte insbesondere anhand inhalticher Interpretationen in Hinblick einer größtmöglichen Plausibilität der gebildeten Cluster geschehen. Zusätzlich kann der größte (bzw. ein großer) Zuwachs der Heterogenität in dem Dendrogramm als Entscheidungskriterium genutzt werden. Bei unseren Daten entsteht der größte Heterogenitätszuwachs zwischen einer 2-Cluster und 1-Cluster-Lösung. Der Heterogenitätszuwachs zwischen einer 4-Cluster und 2-Cluster-Lösung ist ebenfalls relativ groß. Wir entscheiden uns hier für eine Clusteranzahl von 4, hätten jedoch auch die 2-Cluster-Lösung wählen können. Wie bereits erwähnt existiert bei diesem Verfahren oftmals keine eindeutige “optimale” Lösung, da jeweils auch die Interpretiertbarkeit der Cluster auf Grundlage inhaltlicher Überlegungen eine wichtige Rolle spielt.</p>
<p>Darstellung des Dendrogramms mit roten Grenzen bei einer Größe von 4 Clustern:</p>
<pre class="r"><code>hc$labels &lt;- df$country_name

plot(hc)

rect.hclust(hc, k = 4, border = &quot;red&quot;)</code></pre>
<p><img src="/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Ermittlung der Gruppenzugehörigkeit (Cluster 1 bis Cluster 4) der jeweiligen Länder bei einer Clustergröße von k = 4. Dafür nutzen wir die Funktion <code>cutree()</code>, die einen “Schnitt” bei der entsprechenden Clustergröße vornimmt und die Daten in die entsprechenden Gruppen (Nummer des Clusters) einteilt.</p>
<pre class="r"><code>gruppen &lt;- cutree(hc, k = 4) </code></pre>
<p>Hinzufügung der Nummer des Clusters zu dem Datensatz:</p>
<pre class="r"><code>df_cl$cluster &lt;- gruppen</code></pre>
<p>Darstellung der Cluster in einem Punktediagramm:</p>
<pre class="r"><code>df_cl %&gt;% 
  ggplot(aes(logged_gdp_per_capita, 
             healthy_life_expectancy, 
             label = country_name, 
             color = factor(cluster))) +
  geom_point() +
  geom_text(size = 3,
            check_overlap = FALSE,
            vjust = 0, nudge_y = 0.5,
            show.legend = FALSE) +
  theme_classic() +
  ylab(&quot;Lebenserwartung&quot;) +
  xlab(&quot;Bruttoinlandsprodukt pro Einwohner (logarithmiert)&quot;) +
  theme(legend.title=element_blank())</code></pre>
<p><img src="/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-26-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Zum Vergleich, hier noch die Aufteilung der Daten bei einer Wahl von 2 Clustern:</p>
<pre class="r"><code>plot(hc)

rect.hclust(hc, k = 2, border = &quot;red&quot;)</code></pre>
<p><img src="/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-27-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>gruppen_2 &lt;- cutree(hc, k = 2) 

df_cl$cluster_2 &lt;- gruppen_2</code></pre>
<p>Darstellung der 2-Cluster-Lösung in einem Punktediagramm:</p>
<pre class="r"><code>df_cl %&gt;% 
  ggplot(aes(logged_gdp_per_capita, 
             healthy_life_expectancy, 
             label = country_name, 
             color = factor(cluster_2))) +
  geom_point() +
  geom_text(size = 3,
            check_overlap = FALSE,
            vjust = 0, nudge_y = 0.5,
            show.legend = FALSE) +
  theme_classic() +
  ylab(&quot;Lebenserwartung&quot;) +
  xlab(&quot;Bruttoinlandsprodukt pro Einwohner (logarithmiert)&quot;) +
  theme(legend.title=element_blank())</code></pre>
<p><img src="/post/2020-05-21-r-hierarchische-clusteranalyse/index_files/figure-html/unnamed-chunk-28-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
